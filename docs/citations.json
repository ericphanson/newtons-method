{
  "references": {
    "nocedal-wright-2006": {
      "title": "Numerical Optimization",
      "authors": [
        "Jorge Nocedal",
        "Stephen J. Wright"
      ],
      "year": 2006,
      "edition": "2nd",
      "publisher": "Springer",
      "file": "NumericalOptimization2006.pdf"
    },
    "boyd-vandenberghe-2004": {
      "title": "Convex Optimization",
      "authors": [
        "Stephen Boyd",
        "Lieven Vandenberghe"
      ],
      "year": 2004,
      "publisher": "Cambridge University Press",
      "file": "Boyd+Vandenberghe-2004-Convex_optimization.pdf"
    },
    "nesterov-2004": {
      "title": "Introductory Lectures on Convex Optimization: A Basic Course",
      "authors": [
        "Yurii Nesterov"
      ],
      "year": 2004,
      "publisher": "Kluwer Academic Publishers",
      "file": "Introductory-Lectures-on-Convex-Programming-Yurii-Nesterov-2004.pdf"
    },
    "liu-nocedal-1989": {
      "title": "On the Limited Memory BFGS Method for Large Scale Optimization",
      "authors": [
        "Dong C. Liu",
        "Jorge Nocedal"
      ],
      "year": 1989,
      "journal": "Mathematical Programming",
      "volume": "45",
      "pages": "503-528",
      "file": "LiuNocedal1989.pdf"
    },
    "nesterov-2018": {
      "title": "Lectures on Convex Optimization",
      "authors": [
        "Yurii Nesterov"
      ],
      "year": 2018,
      "edition": "2nd",
      "publisher": "Springer",
      "file": "Lectures on Convex Optimization.pdf"
    },
    "rosenbrock-1960": {
      "title": "An Automatic Method for Finding the Greatest or Least Value of a Function",
      "authors": [
        "H. H. Rosenbrock"
      ],
      "year": 1960,
      "journal": "The Computer Journal",
      "volume": "3",
      "issue": "3",
      "pages": "175-184",
      "doi": "10.1093/comjnl/3.3.175",
      "file": "rosenbrock1960.pdf"
    },
    "branin-1972": {
      "title": "Widely Convergent Method for Finding Multiple Solutions of Simultaneous Nonlinear Equations",
      "authors": [
        "F. H. Branin"
      ],
      "year": 1972,
      "journal": "IBM Journal of Research and Development",
      "volume": "16",
      "issue": "5",
      "pages": "504-522",
      "doi": "10.1147/rd.165.0504",
      "file": "branin1972.pdf"
    }
  },
  "citations": {
    "gd-strongly-convex-linear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "101-102",
      "theorem": "Theorem 2.1.15",
      "claim": "Gradient descent with fixed step size achieves linear convergence to the global minimum on strongly convex smooth functions when $0 < \\alpha \\leq 2/(L+\\mu)$",
      "quote": "If $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ and $0 < h \\leq \\frac{2}{\\mu+L}$, then the Gradient Method generates a sequence $\\{x_k\\}$ such that $\\|x_k - x^*\\|^2 \\leq \\left(1 - \\frac{2h\\mu L}{\\mu+L}\\right)^k \\|x_0 - x^*\\|^2$",
      "notes": "Internal: This is the updated 2nd edition (2018) version of the same result from the 2004 edition. The theorem is essentially the same but with slightly refined notation using $\\mathscr{S}_{\\mu,L}^{1,1}$ instead of $S_{\\mu,L}^{1,1}$. The step size bound is $h \\leq 2/(\\mu+L)$ (allowing equality) instead of $h < 2/(L+\\mu)$. Can be used for comparison with the 2004 edition to determine which source to recommend.",
      "readerNotes": "The notation $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ denotes strongly convex functions with strong convexity parameter $\\mu > 0$ and Lipschitz continuous gradient with constant $L$ (see Definition 2.1.3 on page 94). The condition number $Q = L/\\mu$ determines the convergence rate. With optimal step size $h = 2/(L+\\mu)$, the convergence rate is $\\left(\\frac{L-\\mu}{L+\\mu}\\right)^{2k} = \\left(\\frac{Q-1}{Q+1}\\right)^{2k}$, which provides exponentially fast (linear) convergence. Note: Nesterov uses $h$ for step size in the theorem statement; here we use $\\alpha$. This result differs from the merely convex case (Theorem 2.1.14), which has step size bound $2/L$ instead of $2/(L+\\mu)$.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0093.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0094.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0101.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0102.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "INDEPENDENT VERIFICATION (2025-11-12): Verified quote is word-for-word accurate against Theorem 2.1.15 on page 101. Visual inspection confirms: (1) Theorem number is correct, (2) Page numbers 101-102 are accurate, (3) Quote matches source exactly including all mathematical notation. Verified claim matches theorem statement - linear convergence for strongly convex smooth functions with step size bound 0 < h ≤ 2/(μ+L). Cross-referenced usage in GdFixedTab.tsx lines 400 and 511 - both uses correctly state the step size condition and convergence rate. All proofPages are relevant: pages 93-94 provide Definition 2.1.3 of strongly convex functions, pages 101-102 contain Theorem 2.1.15. VERDICT: Citation is complete and accurate. No updates needed.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "nesterov-accelerated-optimal-rate-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "91-92, 102-114",
      "theorem": "Theorem 2.2.2, Theorem 2.1.7 (lower bound), Constant Step Scheme II (2.2.20) for μ=0, and Constant Step Scheme III (2.2.22) for μ>0",
      "claim": "Nesterov's accelerated gradient method achieves the optimal convergence rate $O(1/k^2)$ for smooth convex functions, which is provably optimal among all first-order methods",
      "quote": "Let us take in (2.2.7) $\\gamma_0 = 3L + \\mu$. Then this scheme generates a sequence $\\{x_k\\}_{k=0}^{\\infty}$ such that $f(x_k) - f^* \\leq \\frac{2(4+q_f)L\\|x_0-x^*\\|^2}{3(k+1)^2}$. This means that method (2.2.7) is optimal for solving the unconstrained minimization problem (2.2.1) with $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ and $\\mu \\geq 0$. If $\\mu = 0$, then this method is optimal.",
      "notes": "This is the 2018 edition version with refined calligraphic notation. The general scheme (2.2.7) is developed in pages 102-111, with constant step variants on pages 112-114: Scheme II (equation 2.2.20) for smooth convex (μ=0), and Scheme III (equation 2.2.22) for strongly convex (μ>0). Optimality is proven in Theorem 2.2.2 by comparing with Theorem 2.1.7 lower bound (pages 91-92). For μ=0 (smooth convex), equation (2.2.18) on page 112 gives the explicit rate: $f(x_k) - f^* \\leq \\frac{8L\\|x_0-x^*\\|^2}{3(k+1)^2}$, which is O(1/k²) compared to gradient descent's O(1/k) rate.",
      "readerNotes": "The notation $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ denotes functions with strong convexity parameter $\\mu \\geq 0$ and Lipschitz continuous gradient with constant $L$. When $\\mu = 0$, this reduces to $\\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$, the class of smooth convex functions. The $O(1/k^2)$ convergence rate is provably optimal: Theorem 2.1.7 (pages 91-92) establishes a lower bound showing that no first-order method can achieve better than $O(1/k^2)$ convergence for this function class. Nesterov's accelerated method matches this lower bound up to constant factors, proving optimality. IMPORTANT: There are two different constant step schemes: Scheme II (page 113, equation 2.2.20) for smooth convex ($\\mu=0$) uses momentum coefficient $\\beta_k = \\frac{\\alpha_k(1-\\alpha_k)}{\\alpha_k^2+\\alpha_{k+1}}$ with $\\alpha_k$ from a recurrence relation. Scheme III (page 114, equation 2.2.22) for strongly convex ($\\mu>0$) uses $\\beta = \\frac{1-\\sqrt{q_f}}{1+\\sqrt{q_f}}$ where $q_f = \\mu/L$. The form $x_{k+1} = y_k - \\frac{1}{L}\\nabla f(y_k)$, $y_{k+1} = x_{k+1} + \\beta(x_{k+1} - x_k)$ is the same for both, but the momentum coefficient differs.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0091.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0092.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0102.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0103.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0104.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0105.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0106.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0107.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0108.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0109.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0110.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0111.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0112.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0113.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0114.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "ENHANCED VERIFICATION (adversarial testing): Independently verified all 15 proof pages. Quote is WORD-FOR-WORD accurate from Theorem 2.2.2 (page 110). The O(1/k²) rate is PROVEN (not just stated): equation (2.2.18) on page 112 gives explicit bound. Optimality is PROVEN: Theorem 2.1.7 (pages 91-92) establishes information-theoretic lower bound, and Theorem 2.2.2 proves the method matches this bound. All 15 pages are NECESSARY: pages 91-92 prove lower bound (required for optimality claim), pages 102-109 develop estimating sequence framework (mathematical foundation), pages 110-111 prove Theorem 2.2.2 (main optimality result), pages 112-114 derive concrete algorithmic schemes (Schemes II and III). CORRECTED: Updated theorem field to include both Scheme II (μ=0) and Scheme III (μ>0), as the original only mentioned Scheme III which is incomplete. Updated pages field to explicitly show 91-92, 102-114 structure. REMOVED 'GdFixedTab' from usedIn as grep verification showed the citation is not actually referenced in that file (this was an error in the original citation).",
      "usedIn": []
    },
    "gd-convex-sublinear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "81",
      "theorem": "Theorem 2.1.14 and Corollary 2.1.2",
      "claim": "Gradient descent with fixed step size converges to the global minimum on convex smooth functions (possibly slowly with sublinear rate) when $0 < \\alpha \\leq 2/L$",
      "quote": "Let $f \\in \\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ and $0 < h \\leq 2/L$. Then the Gradient Method generates a sequence $\\{x_k\\}$ such that $\\|x_k - x^*\\|^2 \\leq q^k \\|x_0 - x^*\\|^2$, where $q = 1 - \\frac{h}{2}(2-hL) \\in [0,1)$. [Corollary 2.1.2] If $h = 1/L$ and $f \\in \\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$, then $f(x_k) - f^* \\leq \\frac{L\\|x_0-x^*\\|^2}{2(k+1)}$",
      "notes": "Internal: Used in GdFixedTab to explain convex (non-strongly) convergence. This is the 2018 edition update of the 2004 result, using calligraphic notation $\\mathscr{F}_L^{1,1}$ instead of $F_L^{1,1}$. The step size bound allows equality ($h \\leq 2/L$) instead of strict inequality ($h < 2/L$) in the 2004 edition. This is the $\\mu=0$ case compared to Theorem 2.1.15.",
      "readerNotes": "The notation $\\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ denotes convex functions with Lipschitz continuous gradient with constant $L$ (see Definition 2.1.2 on page 62). With optimal step size $\\alpha = 1/L$, the convergence rate is $O(1/k)$, which is sublinear convergence. Note: Nesterov uses $h$ for step size in the theorem statement; here we use $\\alpha$. This is much slower than the exponentially fast convergence for strongly convex functions (Theorem 2.1.15, with step size bound $2/(L+\\mu)$) - without strong convexity, gradient descent loses the geometric convergence rate and can only guarantee polynomial convergence.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0059.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0062.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0080.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0081.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 2.1.14 and Corollary 2.1.2 on page 81. The quote is extracted from visual verification of the PDF pages. The theorem provides convergence in terms of distance to optimum, while Corollary 2.1.2 provides the $O(1/k)$ sublinear rate in function values with optimal step size $h=1/L$. Page 62 provides Definition 2.1.2 for convex functions. Page 59 contains the beginning of Section 2.1.1 (Smooth Convex Functions) where the function class $\\mathscr{F}_L^{1,1}$ is introduced. Page 80 contains the beginning of Section 2.1.5 (The Gradient Method). The step size bound allows equality ($h \\leq 2/L$) which is a refinement from the 2004 edition's strict inequality.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "gd-smooth-descent-condition-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "81",
      "theorem": "Theorem 2.1.14",
      "claim": "With step size $\\alpha \\leq 2/L$, gradient descent guarantees that $f(w_{k+1}) < f(w_k)$ for smooth functions (Lipschitz continuous gradient with constant $L$).",
      "quote": "Let $f \\in \\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ and $0 < h \\leq 2/L$. Then the Gradient Method generates a sequence $\\{x_k\\}$, which converges to some optimal point $x^*$.",
      "notes": "Internal: Used in GdFixedTab to explain the sufficient condition for descent on smooth functions. This is the 2018 edition version using calligraphic notation $\\mathscr{F}_L^{1,1}$ instead of $F_L^{1,1}$ from the 2004 edition. The step size bound allows equality ($h \\leq 2/L$) instead of strict inequality ($h < 2/L$). The descent property follows from the fundamental inequality for smooth functions (see Lemma 1.2.3 on page 23 which shows $f(y) \\leq f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{L}{2}\\|y-x\\|^2$ for L-smooth functions), which combined with the gradient step yields descent when $h \\leq 2/L$.",
      "readerNotes": "The notation $\\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ denotes convex functions with Lipschitz continuous gradient with constant $L$ (the function class is defined in Section 2.1.1, pages 59-69; see Definition 2.1.2 on page 62 for convex functions). However, the descent property itself follows from the upper bound inequality for smooth functions (Lemma 1.2.3 on page 23), which holds for any function with Lipschitz continuous gradient, not just convex functions. The condition $\\alpha \\leq 2/L$ ensures that each gradient descent step decreases the function value. Note: Nesterov uses $h$ for step size; here we use $\\alpha$. This is a more general result than convergence - it guarantees monotonic decrease at each step. The 2018 edition uses calligraphic script $\\mathscr{F}$ for function classes instead of the regular $F$ used in the 2004 edition, and allows equality in the step size bound ($h \\leq 2/L$) instead of strict inequality ($h < 2/L$).",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0059.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0060.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0062.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0080.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0081.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 2.1.14 on page 81 which states the gradient method convergence for smooth convex functions with step size bound $0 < h \\leq 2/L$. The theorem explicitly states convergence, and the descent property is implied by the proof which uses the fundamental inequality for L-smooth functions (Lemma 1.2.3 on page 23). Pages 59-60 provide context on Chapter 2 (Smooth Convex Optimization) structure. Page 62 contains Definition 2.1.2 for convex functions. Pages 80-81 contain Section 2.1.5 'The Gradient Method' where Theorem 2.1.14 appears. The step size condition $h \\leq 2/L$ (allowing equality) is explicitly stated in the theorem, which is a refinement from the 2004 edition's strict inequality $h < 2/L$.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "gd-descent-lemma-quadratic-upper-bound-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "24-26",
      "lemma": "Lemma 1.2.3",
      "claim": "The quadratic upper bound for L-smooth functions: any function with Lipschitz continuous gradient can be upper-bounded by a quadratic approximation",
      "quote": "Let $f \\in C_L^{1,1}(\\mathbb{R}^n)$. Then, for any $x, y$ from $\\mathbb{R}^n$, we have $|f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle| \\leq \\frac{L}{2} \\|y - x\\|^2$. [...] Geometrically, we have the following picture. Consider a function $f \\in C_L^{1,1}(\\mathbb{R}^n)$. Let us fix a point $x_0 \\in \\mathbb{R}^n$, and define two quadratic functions $\\phi_1(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle - \\frac{L}{2} \\|x - x_0\\|^2$, $\\phi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2} \\|x - x_0\\|^2$. Then the graph of the function $f$ lies between the graphs of $\\phi_1$ and $\\phi_2$: $\\phi_1(x) \\leq f(x) \\leq \\phi_2(x)$, $\\forall x \\in \\mathbb{R}^n$.",
      "notes": "Internal: This is the fundamental descent lemma for L-smooth functions. The upper bound $f(y) \\leq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2}\\|y - x\\|^2$ is crucial for proving convergence of gradient descent. It shows that the function is upper-bounded by its first-order Taylor approximation plus a quadratic term. This result appears in Section 1.2.2 (Classes of Differentiable Functions) and is used throughout Chapter 2 for analyzing first-order methods. Book pages 24-26 correspond to PDF pages 44-46.",
      "readerNotes": "The notation $C_L^{1,1}(\\mathbb{R}^n)$ denotes the class of functions with Lipschitz continuous gradient with constant $L$ (see page 24). This lemma is fundamental for analyzing gradient descent: it shows that any L-smooth function can be upper-bounded by a quadratic function. The upper bound $\\phi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2}\\|x - x_0\\|^2$ is the quadratic upper bound used to prove that gradient descent decreases the function value at each iteration. When we take a gradient step $y = x - \\alpha \\nabla f(x)$, this bound guarantees descent when $\\alpha \\leq 2/L$.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0044.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0045.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0046.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "ENHANCED verification completed following adversarial test protocol. CRITICAL FIX: Corrected pages from '23' to '24-26' to match actual book page numbers visible in proof page headers (PDF pages 44-46). Added ellipsis [...] in quote to properly indicate omitted content (proof and examples) between Lemma 1.2.3 statement and geometric interpretation. The lemma field is correctly populated (Lemma 1.2.3); theorem: null is appropriate since this is a lemma, not a theorem. Quote verified word-for-word against visual proof pages - both the lemma statement (equation 1.2.11) and geometric sandwich inequality are accurate. All three proof pages verified: page 44 (book 24) defines C_L^{1,1} notation, page 45 (book 25) states Lemma 1.2.3 with proof, page 46 (book 26) provides geometric interpretation with φ₁ and φ₂. Claim appropriately captures the quadratic upper bound result without overstatement. Usage in GdFixedTab.tsx verified correct - cites lemma when introducing quadratic upper bound inequality.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "condition-number-definition-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "95-97",
      "theorem": "Definition on page 97 (following Theorem 2.1.12), Theorem 2.1.11, and Example 2.1.2",
      "claim": "The condition number $Q = L/\\mu$ for strongly convex smooth functions bounds the Hessian eigenvalues. For $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$, the Hessian satisfies $\\mu I \\preceq \\nabla^2 f(x) \\preceq LI$, meaning all eigenvalues satisfy $\\mu \\leq \\lambda_i \\leq L$. For quadratic functions where the Hessian eigenvalues exactly equal these bounds ($\\lambda_{\\min} = \\mu$, $\\lambda_{\\max} = L$), we have $Q = \\lambda_{\\max}/\\lambda_{\\min}$.",
      "quote": "One of the most important functional classes is $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ (recall that the corresponding norm is standard Euclidean). This class is described by the following inequalities: $\\langle \\nabla f(x) - \\nabla f(y), x - y \\rangle \\geq \\mu \\| x - y \\|^2$, $\\| \\nabla f(x) - \\nabla f(y) \\| \\leq L \\| x - y \\|$. The value $Q_f = L/\\mu \\geq 1$ is called the condition number of the function $f$.",
      "notes": "Internal: This citation documents the equivalence between the two definitions of condition number that appear in the codebase: (1) Q = L/μ based on function properties, and (2) κ = λ_max/λ_min based on Hessian eigenvalues. Theorem 2.1.11 on pages 95-96 establishes the connection: for twice continuously differentiable functions, f ∈ S^{1,1}_{μ,L} if and only if μI ⪯ ∇²f(x) ⪯ LI for all x, which means the Hessian eigenvalues satisfy μ ≤ λ_i ≤ L, making λ_min = μ and λ_max = L (for quadratic functions or locally near minima). The glossary entry for condition-number mentions both definitions and states they are equivalent.",
      "readerNotes": "For smooth strongly convex functions (class $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$), the condition number can be defined two equivalent ways: (1) $Q = L/\\mu$ where $L$ is the Lipschitz constant of the gradient and $\\mu$ is the strong convexity parameter, or (2) $\\kappa = \\lambda_{\\text{max}}/\\lambda_{\\text{min}}$ where these are the largest and smallest eigenvalues of the Hessian. These are equivalent because Theorem 2.1.11 (pages 95-96) shows that $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ if and only if $\\mu I \\preceq \\nabla^2 f(x) \\preceq L I$ for all $x$, meaning all Hessian eigenvalues satisfy $\\mu \\leq \\lambda_i \\leq L$. For quadratic functions $f(x) = \\frac{1}{2}x^T A x$, the Hessian is constant ($\\nabla^2 f = A$), so $\\mu = \\lambda_{\\text{min}}(A)$ and $L = \\lambda_{\\text{max}}(A)$ exactly. For general strongly convex functions, these bounds hold throughout the domain, establishing the equivalence.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0095.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0096.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0097.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "CORRECTED (Batch 4 adversarial flag, fixed in Batch 5): The original claim overstated the relationship as 'equivalent to' λ_max/λ_min, but the source (pages 95-97) actually shows Q = L/μ BOUNDS the Hessian eigenvalues, not that they are always equal. Theorem 2.1.11 (equation 2.1.28, page 96) establishes μI ⪯ ∇²f(x) ⪯ LI for f ∈ S^{1,1}_{μ,L}. Example 2.1.2 (page 96) shows that for quadratic functions f(x) = α + ⟨a,x⟩ + (1/2)⟨Ax,x⟩ where μI ⪯ A ⪯ LI, we have f ∈ S^{1,1}_{μ,L}. The definition Q_f = L/μ appears on page 97 after Theorem 2.1.12. For quadratic functions with tight eigenvalue bounds (λ_min = μ, λ_max = L), Q equals κ = λ_max/λ_min. For general smooth strongly convex functions, Q bounds the condition number of the Hessian but they may not be exactly equal. Updated claim, theorem field, and notes to reflect this more nuanced relationship.",
      "usedIn": [
        "glossary"
      ]
    },
    "gd-linesearch-convex-sublinear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "48-50, 81",
      "theorem": "Equation (1.2.20) with Corollary 2.1.2",
      "claim": "For convex, L-smooth functions, gradient descent with Armijo line search achieves sublinear convergence: $f(w_k) - f^* \\leq O(L\\|w_0 - w^*\\|^2/k)$. Line search automatically adapts the step size to achieve near-optimal constants without requiring knowledge of $L$",
      "quote": "The Armijo rule: Find $x_{k+1} = x_k - h\\nabla f(x_k)$ with $h > 0$ such that $\\alpha\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle \\leq f(x_k) - f(x_{k+1})$, $\\beta\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle \\geq f(x_k) - f(x_{k+1})$, where $0 < \\alpha < \\beta < 1$ are some fixed parameters. [...] Thus, we have proved that in all cases we have $f(x_k) - f(x_{k+1}) \\geq \\frac{\\omega}{L} \\|\\nabla f(x_k)\\|^2$, where $\\omega$ is some positive constant. [...] For the Armijo rule, $\\omega = \\frac{2\\alpha(1-\\beta)}{L}$",
      "notes": "Internal: Used in GdLineSearchTab to show that line search achieves the same O(1/k) convergence rate as fixed step size for convex smooth functions, but without requiring knowledge of L. The key insight from equation (1.2.20) is that all step size strategies (constant, full relaxation, Armijo) satisfy the same type of descent inequality: $f(x_k) - f(x_{k+1}) \\geq \\omega \\|\\nabla f(x_k)\\|^2$ for some positive constant $\\omega$. For Armijo rule with parameters $\\alpha, \\beta \\in (0,1)$, we get $h_k \\geq \\frac{2}{L}(1-\\beta)$ and thus $\\omega = \\frac{2\\alpha(1-\\beta)}{L}$. Applying the same argument as Corollary 2.1.2 (which uses the descent inequality for convex functions), this gives the O(1/k) rate. The advantage of line search is that it automatically finds a good step size without knowing L in advance, while achieving comparable convergence constants to the optimal fixed step size $h = 1/L$.",
      "readerNotes": "The Armijo line search rule (also called backtracking line search) finds a step size $h_k$ at each iteration that satisfies two conditions: sufficient decrease $f(x_k) - f(x_{k+1}) \\geq \\alpha h_k \\|\\nabla f(x_k)\\|^2$ and an upper bound $f(x_k) - f(x_{k+1}) \\leq \\beta h_k \\|\\nabla f(x_k)\\|^2$, where $0 < \\alpha < \\beta < 1$ are parameters (typically $\\alpha \\approx 0.3$, $\\beta \\approx 0.7$). Nesterov shows (pages 48-50) that for smooth functions ($f \\in C_L^{1,1}$), the Armijo rule guarantees a step size of at least $h_k \\geq \\frac{2}{L}(1-\\beta)$, yielding the descent inequality $f(x_k) - f(x_{k+1}) \\geq \\frac{2\\alpha(1-\\beta)}{L} \\|\\nabla f(x_k)\\|^2$. This is the same type of inequality as with fixed step size $h = \\frac{2\\alpha}{L}$ (equation on page 50), showing that line search achieves comparable per-iteration progress. For convex smooth functions, this descent inequality leads to $O(1/k)$ convergence by the same argument as Corollary 2.1.2: summing over iterations gives $f(x_k) - f^* \\leq \\frac{L\\|x_0-x^*\\|^2}{2\\omega(k+1)}$. The key advantage of line search is **automatic step size selection**: it adapts to the local smoothness without requiring prior knowledge of $L$, achieving near-optimal convergence constants in practice.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0048.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0049.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0050.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0081.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified the Armijo rule definition (equation 1.2.16-1.2.17) on page 48-49, the unified descent inequality (equation 1.2.20) on page 50 showing $f(x_k) - f(x_{k+1}) \\geq \\frac{\\omega}{L}\\|\\nabla f(x_k)\\|^2$ for all step size strategies including Armijo, and the derivation showing Armijo achieves $\\omega = \\frac{2\\alpha(1-\\beta)}{L}$. Also verified Corollary 2.1.2 on page 81 which converts the descent inequality to the O(1/k) convergence rate for convex smooth functions. The Armijo rule analysis shows that $h_k \\geq \\frac{2}{L}(1-\\beta)$ (bottom of page 50), and combining with the sufficient decrease condition (equation 1.2.16) yields the stated bound. The quote is extracted from visual verification of pages 48 and 50. Note: Nesterov uses $h$ for step size; the notation $\\alpha$ is used here to denote step size in our implementation.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "wolfe-conditions-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "33-36",
      "theorem": "Equations (3.6) and (3.7)",
      "claim": "The Wolfe conditions combine Armijo's sufficient decrease $f(x_k + \\alpha p_k) \\leq f(x_k) + c_1\\alpha\\nabla f_k^T p_k$ with a curvature condition $\\nabla f(x_k + \\alpha p_k)^T p_k \\geq c_2\\nabla f_k^T p_k$ (where $0 < c_1 < c_2 < 1$) to ensure steps are neither too small nor too large",
      "quote": "The sufficient decrease and curvature conditions are known collectively as the Wolfe conditions. We illustrate them in Figure 3.5 and restate them here for future reference: $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$, $\\nabla f(x_k + \\alpha_k p_k)^T p_k \\geq c_2 \\nabla f_k^T p_k$, with $0 < c_1 < c_2 < 1$. [...] The strong Wolfe conditions require $\\alpha_k$ to satisfy $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$, $|\\nabla f(x_k + \\alpha_k p_k)^T p_k| \\leq c_2 |\\nabla f_k^T p_k|$, with $0 < c_1 < c_2 < 1$.",
      "notes": "Internal: This is for background context only in GdLineSearchTab. The key insight is that Armijo (sufficient decrease) alone is satisfied by arbitrarily small steps (see Figure 3.3 on page 53), so the curvature condition is needed to prevent tiny steps. The curvature condition ensures the slope at the accepted point is not too negative (not much room for further decrease). Strong Wolfe adds an absolute value to also exclude points far from stationary points. Typical values: c1=1e-4, c2=0.9 (Newton/quasi-Newton) or 0.1 (conjugate gradient). We implement Armijo backtracking only, but mention Wolfe briefly to explain why pure Armijo can be inefficient without backtracking from a reasonable initial step.",
      "readerNotes": "The Wolfe conditions consist of two parts: (1) The sufficient decrease (Armijo) condition $f(x_k + \\alpha p_k) \\leq f(x_k) + c_1\\alpha\\nabla f_k^T p_k$ ensures the step reduces the function value proportionally to the step size and directional derivative. However, this condition alone is satisfied by all sufficiently small steps (see Figure 3.3), which could lead to inefficiently tiny steps. (2) The curvature condition $\\nabla f(x_k + \\alpha p_k)^T p_k \\geq c_2\\nabla f_k^T p_k$ prevents arbitrarily small steps by requiring the slope at the accepted point to be at least $c_2$ times the initial slope. If the slope is still strongly negative ($\\ll c_2\\nabla f_k^T p_k$), we can reduce $f$ significantly by moving further, so the search continues. The strong Wolfe conditions use $|\\nabla f(x_k + \\alpha p_k)^T p_k| \\leq c_2 |\\nabla f_k^T p_k|$ to also exclude points with excessively positive slope, forcing steps to lie near stationary points of the line search function. Common parameter values are $c_1 = 10^{-4}$ and $c_2 = 0.9$ for Newton/quasi-Newton methods.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0052.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0053.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0054.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0055.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0056.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "Independent verification completed 2025-11-12. Quote is word-for-word accurate from book page 34 (PDF page 54). Equations (3.6a)/(3.6b) for standard Wolfe conditions and (3.7a)/(3.7b) for strong Wolfe conditions verified. Page numbers: citation uses book pages 33-36, proof pages use PDF pages 52-56 (book pages 32-36). Core content: Wolfe conditions (3.6) and (3.7) appear on book page 34. Context pages verified: page 33 defines Armijo condition (3.4) and curvature condition (3.5); Figure 3.3 illustrates why Armijo alone accepts arbitrarily small steps; Figure 3.4 illustrates curvature condition; Figure 3.5 (page 35) shows combined Wolfe conditions; Lemma 3.1 (pages 35-36) proves existence of step lengths satisfying Wolfe conditions for smooth functions bounded below. Usage in GdLineSearchTab.tsx (line 402-404) verified: citation correctly explains Wolfe conditions as alternative to Armijo backtracking. Claim accurately summarizes the conditions and their purpose. Mathematical notation correctly transcribed. All quality standards met.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "armijo-backtracking-termination-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "37, 56-57",
      "theorem": "Algorithm 3.1 (Backtracking Line Search)",
      "claim": "For L-smooth functions, Armijo backtracking with geometric step reduction $\\alpha \\leftarrow \\tau\\alpha$ (where $\\tau \\in (0,1)$) terminates in finite steps. The backtracking procedure will find an acceptable step length after a finite number of trials.",
      "quote": "Algorithm 3.1 (Backtracking Line Search). Choose $\\bar{\\alpha} > 0$, $\\rho \\in (0, 1)$, $c \\in (0, 1)$; Set $\\alpha \\leftarrow \\bar{\\alpha}$; repeat until $f(x_k + \\alpha p_k) \\leq f(x_k) + c\\alpha\\nabla f_k^T p_k$, $\\alpha \\leftarrow \\rho\\alpha$; end (repeat). Terminate with $\\alpha_k = \\alpha$. [...] An acceptable step length will be found after a finite number of trials, because $\\alpha_k$ will eventually become small enough that the sufficient decrease condition holds (see Figure 3.3).",
      "notes": "Internal: Used in GdLineSearchTab to explain backtracking line search termination. The book guarantees finite termination but does not provide an explicit bound on the number of backtracking iterations in terms of problem parameters. Page 37 contains the algorithm (PDF page 57), and page 37 contains the statement about finite termination. The sufficient decrease condition is also called the Armijo condition (page 33, PDF page 53). The notation: $\\bar{\\alpha}$ is initial step length (typically 1 for Newton/quasi-Newton), $\\rho$ is the contraction factor (reduction ratio), $c$ is the Armijo parameter (typically $10^{-4}$), $p_k$ is the search direction. The book uses $\\rho$ for the contraction factor; in the codebase this may be denoted $\\tau$.",
      "readerNotes": "The Armijo backtracking algorithm guarantees that a step length satisfying the sufficient decrease condition $f(x_k + \\alpha p_k) \\leq f(x_k) + c\\alpha\\nabla f_k^T p_k$ will be found in finitely many iterations. At each iteration, the step length is reduced by the factor $\\rho$ (i.e., $\\alpha \\leftarrow \\rho\\alpha$). The algorithm terminates because the step length eventually becomes small enough that the sufficient decrease condition is satisfied. For L-smooth functions (Lipschitz continuous gradient with constant $L$), this is guaranteed by Lemma 1.2.3 in Nesterov 2018 (page 45/25), which shows that $f(y) \\leq f(x) + \\langle\\nabla f(x), y-x\\rangle + \\frac{L}{2}\\|y-x\\|^2$. However, the standard references do not provide an explicit bound on the number of backtracking iterations in terms of $L$, $c$, or $\\rho$; they only guarantee finite termination. The typical choice is $c = 10^{-4}$ (page 33) and $\\rho \\in [\\rho_{lo}, \\rho_{hi}]$ for some fixed $0 < \\rho_{lo} < \\rho_{hi} < 1$ (page 37).",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0037.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0052.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0053.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0056.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0057.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "INDEPENDENT VERIFICATION: Quote verified word-for-word against OCR text and PDF images. Algorithm 3.1 on page 37 (PDF page 57) accurately transcribed. The [...] ellipsis correctly omits explanatory prose between the algorithm statement and the termination guarantee. The termination statement \"An acceptable step length will be found after a finite number of trials, because αk will eventually become small enough that the sufficient decrease condition holds (see Figure 3.3)\" is word-for-word accurate. Page numbers verified: book page 37 = PDF page 57 (algorithm), book page 33 = PDF page 53 (Armijo condition definition), book page 32 = PDF page 52 (Figure 3.3). Claim is accurate: finite termination is guaranteed but no explicit bound is provided in terms of problem parameters. The book uses ρ for contraction factor; code may use τ (correctly documented in notes). Usage in GdLineSearchTab.tsx verified at lines 373, 383, 605, 624 - all uses are appropriate and consistent with the source material.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "gd-linesearch-strongly-convex-linear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "53-55",
      "theorem": "Theorem 1.2.4",
      "claim": "For $\\mu$-strongly convex, $L$-smooth functions (locally, near a strict local minimum), gradient descent with optimal step size $h^* = 2/(L+\\mu)$ achieves linear convergence: $\\|x_k - x^*\\| \\leq C(1 - 2\\mu/(L+3\\mu))^k$ where the convergence rate $\\rho = 1 - 2\\mu/(L+3\\mu) < 1$ depends on the condition number $Q = L/\\mu$. The constant $C = \\bar{r}r_0/(\\bar{r}-r_0)$ depends on how close the initial point is to the local minimum.",
      "quote": "Theorem 1.2.4 Let the function $f(\\cdot)$ satisfy our assumptions and let the starting point $x_0$ be close enough to a strict local minimum $x^*$: $r_0 = \\|x_0 - x^*\\| < \\bar{r} = 2\\mu/M$. Then the Gradient Method with step size $h^*_k = 2/(L+\\mu)$ converges as follows: $\\|x_k - x^*\\| \\leq \\frac{\\bar{r}r_0}{\\bar{r}-r_0}\\left(1 - \\frac{2\\mu}{L+3\\mu}\\right)^k$. This type of rate of convergence is called linear.",
      "notes": "Internal: This is a LOCAL convergence result for gradient descent near a strict local minimum, not a global result. The assumptions are: (1) $f \\in C_M^{2,2}(\\mathbb{R}^n)$ (twice differentiable with Lipschitz continuous Hessian), (2) $x^*$ is a local minimum with positive definite Hessian bounded by $\\mu I \\preceq \\nabla^2 f(x^*) \\preceq LI$, and (3) starting point $x_0$ is close enough to $x^*$. The theorem shows linear convergence with the optimal step size $h^* = 2/(L+\\mu)$. The Armijo rule (pages 48-50, equations 1.2.16-1.2.17) is discussed as a practical line search strategy that guarantees sufficient decrease and is shown to give $f(x_k) - f(x_{k+1}) \\geq \\frac{\\alpha(1-\\beta)}{2L}\\|\\nabla f(x_k)\\|^2$ for parameters $0 < \\alpha < \\beta < 1$. This differs from the global results in Chapter 2 (Theorems 2.1.14 and 2.1.15) which apply to globally convex/strongly convex functions. Used in GdLineSearchTab.",
      "readerNotes": "This theorem establishes linear convergence for gradient descent with line search in a LOCAL neighborhood of a strict minimum, not globally. The assumptions require: (1) twice differentiable function with Lipschitz continuous Hessian (constant $M$), (2) a strict local minimum $x^*$ where the Hessian satisfies $\\mu I \\preceq \\nabla^2 f(x^*) \\preceq LI$, and (3) initial point sufficiently close to $x^*$ (within radius $\\bar{r} = 2\\mu/M$). The convergence rate $\\rho = 1 - 2\\mu/(L+3\\mu)$ depends on the condition number $Q = L/\\mu$. The Armijo rule (equations 1.2.16-1.2.17, pages 48-50) is a practical line search that finds step size $h > 0$ satisfying $\\alpha\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle \\leq f(x_k) - f(x_{k+1}) \\leq \\beta\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle$ for parameters $0 < \\alpha < \\beta < 1$, ensuring sufficient decrease. Note: Nesterov uses $h$ for step size; here we use $\\alpha$. This LOCAL result complements the GLOBAL results in Chapter 2: Theorem 2.1.15 (pages 101-102) for globally strongly convex functions requires the function to be strongly convex everywhere, not just near a minimum.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0048.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0049.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0050.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0053.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0054.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0055.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent-enhanced",
      "verificationNotes": "ADVERSARIAL TEST RESULT: Found critical error in claim field. The original claim stated ρ = (1 - 2μ/(L+3μ))² (squared), but Theorem 1.2.4 on page 55 clearly shows the correct formula is ρ = 1 - 2μ/(L+3μ) (NOT squared). The convergence bound is ‖xₖ - x*‖ ≤ C(1 - 2μ/(L+3μ))^k where C = r̄r₀/(r̄-r₀). The quote field was actually correct all along. Also corrected the claim to show ‖xₖ - x*‖ (distance) not ‖xₖ - x*‖² (squared distance). Verified all 6 proof pages (48-50 for Armijo rule, 53-55 for Theorem 1.2.4). Step size formula h* = 2/(L+μ) is correct per equation 1.2.27 on page 54. Pages 53-55 are correct. Usage in GdLineSearchTab.tsx line 671 shows the correct formula (not squared).",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "gd-linesearch-strongly-convex-linear-convergence-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "62-64",
      "theorem": "Theorem 3.3 and Theorem 3.4",
      "claim": "For strongly convex quadratic functions and general smooth strongly convex functions, steepest descent with exact line search achieves linear convergence with rate determined by the condition number: $\\|x_{k+1} - x^*\\|_Q^2 \\leq \\left(\\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\right)^2 \\|x_k - x^*\\|_Q^2$ where $\\kappa(Q) = \\lambda_n/\\lambda_1$ is the condition number",
      "quote": "Theorem 3.3. When the steepest descent method with exact line searches is applied to the strongly convex quadratic function $f(x) = \\frac{1}{2}x^T Q x - b^T x$, the error norm satisfies $\\|x_{k+1} - x^*\\|_Q^2 \\leq \\left(\\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\right)^2 \\|x_k - x^*\\|_Q^2$, where $0 < \\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_n$ are the eigenvalues of $Q$. [...] Theorem 3.4. Suppose that $f : \\mathbb{R}^n \\to \\mathbb{R}$ is twice continuously differentiable, and that the iterates generated by the steepest-descent method with exact line searches converge to a point $x^*$ at which the Hessian matrix $\\nabla^2 f(x^*)$ is positive definite. Let $r$ be any scalar satisfying $r \\in [(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1), 1]$, where $\\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_n$ are the eigenvalues of $\\nabla^2 f(x^*)$. Then for all $k$ sufficiently large, we have $f(x_{k+1}) - f(x^*) \\leq r^2[f(x_k) - f(x^*)]$.",
      "notes": "Internal: These theorems establish linear convergence for steepest descent (gradient descent) with exact line search on strongly convex functions. Theorem 3.3 gives the exact rate for quadratic functions, while Theorem 3.4 extends to general nonlinear functions (asymptotically, near the solution). The convergence rate depends on the condition number $\\kappa = \\lambda_n/\\lambda_1 = L/\\mu$. The rate $(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1) = (L-\\mu)/(L+\\mu) = (Q-1)/(Q+1)$ where $Q = L/\\mu$ is the condition number. This matches the result in Nesterov 2018 Theorem 2.1.15 for globally strongly convex functions. The exact line search minimizes $f(x_k - \\alpha \\nabla f(x_k))$ over $\\alpha > 0$. Used in GdLineSearchTab.",
      "readerNotes": "These results show that steepest descent with exact line search achieves linear convergence on strongly convex functions, with the convergence rate determined by the condition number $\\kappa = \\lambda_n/\\lambda_1$. For quadratic functions $f(x) = \\frac{1}{2}x^T Q x - b^T x$, Theorem 3.3 gives the exact rate $(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1)$. For general smooth functions, Theorem 3.4 shows that near a solution $x^*$ where $\\nabla^2 f(x^*)$ is positive definite, the method achieves the same asymptotic rate based on the eigenvalues of the Hessian at $x^*$. The exact line search finds the step length $\\alpha_k$ that minimizes $f(x_k - \\alpha \\nabla f(x_k))$, given by $\\alpha_k = \\nabla f_k^T \\nabla f_k / (\\nabla f_k^T Q \\nabla f_k)$ for quadratic functions (equation 3.25, page 62). The convergence rate degrades as the condition number increases: when $\\kappa$ is large, the rate approaches 1 and convergence becomes very slow. The weighted norm $\\|x\\|_Q^2 = x^T Q x$ measures optimality gap: $\\frac{1}{2}\\|x - x^*\\|_Q^2 = f(x) - f(x^*)$ (equation 3.27).",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0062.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0063.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0064.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 3.3 on pages 62-63 for strongly convex quadratic functions and Theorem 3.4 on page 64 for general smooth functions. Both theorems establish linear convergence with rate determined by condition number. The exact line search formula (equation 3.25) and the weighted norm definition (equation 3.27) are on page 62. Theorem 3.3 provides the exact convergence rate for quadratics, while Theorem 3.4 extends to nonlinear functions asymptotically. The rate $(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1)$ can be rewritten as $(Q-1)/(Q+1)$ where $Q = \\lambda_n/\\lambda_1 = L/\\mu$ is the condition number, matching Nesterov's results.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "bfgs-superlinear-convergence-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "157-160, 176",
      "theorem": "Theorem 6.6",
      "claim": "BFGS (not L-BFGS) achieves superlinear convergence on strongly convex functions. L-BFGS achieves only linear convergence due to limited memory preventing full Hessian approximation",
      "quote": "Theorem 6.6. Suppose that $f$ is twice continuously differentiable and that the iterates generated by the BFGS algorithm converge to a minimizer $x^*$ at which Assumption 6.2 holds. Suppose also that (6.52) holds. Then $x_k$ converges to $x^*$ at a superlinear rate.",
      "notes": "Internal: CRITICAL DISTINCTION - This theorem applies to full BFGS, NOT L-BFGS. Book page 176 (Section 7.2) explicitly states that L-BFGS yields 'an acceptable (albeit linear) rate of convergence.' The limited memory in L-BFGS prevents the Hessian approximation from fully converging to the true Hessian, degrading convergence from superlinear to linear. Used in LbfgsTab with this important caveat. Assumption 6.2 requires Lipschitz continuous Hessian at x*. Condition (6.52) requires sum of distances to converge: Σ||x_k - x*|| < ∞.",
      "readerNotes": "**Important:** This theorem establishes superlinear convergence for **full BFGS**, not L-BFGS. Nocedal & Wright (book page 176, Section 7.2) explicitly state that L-BFGS yields 'an acceptable (albeit **linear**) rate of convergence.' The distinction arises because L-BFGS uses limited memory (only the $M$ most recent correction pairs), preventing the Hessian approximation $B_k$ from fully converging to the true Hessian $\\nabla^2 f(w^*)$. For L-BFGS in practice, the linear convergence rate is still effective, and the method's low memory requirements make it practical for large-scale problems where full BFGS would be infeasible.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0173.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0174.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0175.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0176.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0177.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0178.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0179.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0180.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0196.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "adversarial-verification-agent-batch5-agent3",
      "verificationNotes": "ADVERSARIAL VERIFICATION COMPLETED. Fixed critical page numbering errors: Changed pages from '177-180' (PDF pages) to '157-160, 176' (book pages). Added proof page for book page 176 (PDF page 196) containing the L-BFGS linear convergence statement. Verified quote is word-for-word accurate. The claim's distinction '(not L-BFGS)' is a valid inference from page 176 but not explicitly stated in Theorem 6.6 itself. Verified Assumption 6.2 (Lipschitz continuous Hessian) and condition (6.52) (Σ||x_k - x*|| < ∞). Usage in LbfgsTab is consistent with corrected citation.",
      "usedIn": [
        "LbfgsTab"
      ]
    },
    "bfgs-update-formula-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "136-140",
      "theorem": "Equation (6.19)",
      "claim": "The BFGS update formula maintains positive definiteness and satisfies the secant equation $B_{k+1} s_k = y_k$",
      "quote": "The update formula for $B_k$ is obtained by simply applying the Sherman–Morrison–Woodbury formula (A.28) to (6.17) to obtain $B_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$ where $s_k = x_{k+1} - x_k = \\alpha_k p_k$ and $y_k = \\nabla f_{k+1} - \\nabla f_k$.",
      "notes": "Internal: This is the BFGS update formula for the Hessian approximation $B_k$. The formula can be rewritten in the symmetric form: $B_{k+1} = (I - \\rho_k s_k y_k^T) B_k (I - \\rho_k y_k s_k^T) + \\rho_k s_k s_k^T$ where $\\rho_k = 1/(s_k^T y_k)$ (equation 6.14). Used in LbfgsTab to explain the BFGS/L-BFGS update mechanism.",
      "readerNotes": "The BFGS formula updates the Hessian approximation $B_k$ using information from the most recent step. The notation: $s_k = x_{k+1} - x_k$ is the parameter change (step), $y_k = \\nabla f_{k+1} - \\nabla f_k$ is the gradient change, and $\\rho_k = 1/(s_k^T y_k)$ is the curvature scaling factor (equation 6.14). The secant equation $B_{k+1} s_k = y_k$ (equation 6.6, page 138) requires that the updated approximation maps the step to the gradient change, mimicking the property of the true Hessian. The formula maintains positive definiteness when the curvature condition $s_k^T y_k > 0$ holds (equation 6.7, page 138).",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0136.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0137.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0138.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0139.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0140.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "VERIFIED: Quote is word-for-word accurate from page 140, lines 468-474 (equation 6.19). Visual verification of all proof pages (136-140) confirms correct content. Page numbers are correct (book page numbers, not PDF page numbers). The quote accurately captures the BFGS update formula for B_k, including the definitions of s_k and y_k. Cross-referenced equation (6.17) on page 140 (inverse Hessian form H_k), equation (6.6) on page 138 (secant equation), equation (6.7) on page 138 (curvature condition s_k^T y_k > 0), and equation (6.14) on page 139 (definition of ρ_k). Usage in LbfgsTab.tsx line 850 is correct - cited in explanation of BFGS update mechanism. Claim accurately states that the formula maintains positive definiteness (when curvature condition holds) and satisfies the secant equation. All context is properly documented in notes and readerNotes fields.",
      "usedIn": [
        "LbfgsTab"
      ]
    },
    "lbfgs-computational-complexity-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "177-178, 197-198",
      "algorithm": "Algorithm 7.4 (L-BFGS two-loop recursion)",
      "claim": "L-BFGS requires $O(Md)$ memory and $O(Md)$ time per iteration, where $M$ is the memory size (number of stored curvature pairs) and $d$ is the dimension",
      "quote": "Without considering the multiplication $H_k^0 q$, the two-loop recursion scheme requires $4mn$ multiplications; if $H_k^0$ is diagonal, then $n$ additional multiplications are needed. [...] To circumvent this problem, we store a modified version of $H_k$ implicitly, by storing a certain number (say, $m$) of the vector pairs $\\{s_i, y_i\\}$ used in the formulas (7.16)–(7.18). The product $H_k \\nabla f_k$ can be obtained by performing a sequence of inner products and vector summations involving $\\nabla f_k$ and the pairs $\\{s_i, y_i\\}$.",
      "notes": "Internal: The complexity analysis appears on page 178 following Algorithm 7.4. The key observation is that L-BFGS stores $m$ vector pairs $\\{s_i, y_i\\}$, each of length $n$ (dimension $d$ in our notation), requiring $O(mn) = O(Md)$ storage. The two-loop recursion performs $4mn$ scalar multiplications plus $n$ for the initial Hessian approximation, giving $O(mn) = O(Md)$ time complexity per iteration.",
      "readerNotes": "L-BFGS achieves its efficiency through compact storage: instead of storing a dense $d \\times d$ approximate Hessian (requiring $O(d^2)$ memory), it stores only $M$ vector pairs $\\{s_k, y_k\\}$ of dimension $d$, requiring $O(Md)$ memory. The two-loop recursion (Algorithm 7.4) computes the search direction efficiently using these stored pairs. The algorithm performs two passes through the $M$ stored pairs, with each loop iteration performing $O(d)$ operations (inner products and vector additions), yielding $O(Md)$ total time per optimization iteration. The quote states that the recursion requires $4mn$ scalar multiplications (where $m = M$ and $n = d$), plus $n$ more if the initial approximation $H_k^0$ is diagonal.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0177.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0178.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0197.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0198.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "Independent verification completed 2025-11-12: (1) Both quotes verified word-for-word against OCR text from pages 177-178; (2) Algorithm 7.4 correctly identified on page 178; (3) Claim accurately reflects source material - O(Md) memory for storing M vector pairs of dimension d, and O(Md) time from 4mn multiplications in two-loop recursion; (4) Usage in LbfgsTab.tsx (line 1311) is correct and appropriate; (5) Page numbers correct (printed pages 177-178, which are PDF physical pages 197-198 due to ~20 page offset from frontmatter). Citation is complete, accurate, and properly contextualized.",
      "usedIn": [
        "LbfgsTab"
      ]
    },
    "bfgs-positive-definiteness-preservation-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "136-140, 156-161",
      "theorem": "Section 6.1, equations (6.7), (6.8), positive definiteness argument (p. 161)",
      "claim": "BFGS/L-BFGS maintains positive definiteness of the approximate Hessian by only accepting curvature pairs where $s_k^T y_k > 0$ (positive curvature condition). If $H_k$ is positive definite and $s_k^T y_k > 0$, then $H_{k+1}$ computed by the BFGS update is positive definite. This makes BFGS more robust than Newton's method in non-convex regions where the true Hessian may have negative eigenvalues",
      "quote": "Note that the minimization problem (6.16) that gives rise to the BFGS update formula does not explicitly require the updated Hessian approximation to be positive definite. It is easy to show, however, that $H_{k+1}$ will be positive definite whenever $H_k$ is positive definite, by using the following argument. First, note from (6.8) that $y_k^T s_k$ is positive, so that the updating formula (6.17), (6.14) is well-defined. For any nonzero vector $z$, we have $z^T H_{k+1} z = w^T H_k w + \\rho_k (z^T s_k)^2 \\geq 0$, where we have defined $w = z - \\rho_k y_k (s_k^T z)$. The right hand side can be zero only if $s_k^T z = 0$, but in this case $w = z \\neq 0$, which implies that the first term is greater than zero. Therefore, $H_{k+1}$ is positive definite.",
      "notes": "Internal: This citation establishes the key robustness property of BFGS - it maintains positive definiteness through curvature filtering. The curvature condition $s_k^T y_k > 0$ (equation 6.7, page 157) is guaranteed by the Wolfe line search conditions as shown in equation (6.8, page 157). The positive definiteness preservation argument on page 161 shows that if $H_k$ is positive definite, then $H_{k+1}$ from BFGS update is also positive definite. NOTE: Pages 136-140 are from Chapter 5 (Conjugate Gradient Methods) discussing preliminary quasi-Newton concepts. Pages 156-161 are from Chapter 6 (Quasi-Newton Methods) which is dedicated to BFGS. The 16-page gap (141-155) contains unrelated material: end of Chapter 5 (Fletcher-Reeves, nonlinear CG), exercises, and Chapter 6 title page. The non-contiguous page range is intentional and correct.",
      "readerNotes": "The BFGS method maintains positive definiteness of its Hessian approximation through a curvature filtering mechanism. The key is the **curvature condition** $s_k^T y_k > 0$ (equation 6.7, page 157), where $s_k = x_{k+1} - x_k$ is the step and $y_k = \\nabla f_{k+1} - \\nabla f_k$ is the change in gradients. This condition is automatically satisfied when using the Wolfe line search conditions (equation 6.8, page 157). The BFGS update formula has the remarkable property that **if $H_k$ is positive definite and $s_k^T y_k > 0$, then $H_{k+1}$ is also positive definite** (argument on page 161). This property makes BFGS significantly more robust than Newton's method in non-convex regions: Newton's method uses the true Hessian $\\nabla^2 f(x_k)$, which can have negative eigenvalues when the function is non-convex locally.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0136.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0137.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0138.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0139.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0140.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0156.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0157.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0158.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0159.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0160.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0161.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent-adversarial",
      "verificationNotes": "ADVERSARIAL VERIFICATION COMPLETE. Examined all 11 proof pages plus the 16-page gap (141-155). FINDINGS: (1) The 16-page gap is CORRECT and INTENTIONAL - pages 141-155 contain Chapter 5 material on Conjugate Gradient methods (Fletcher-Reeves, nonlinear CG, global convergence theorems, exercises) and Chapter 6 title page, all unrelated to BFGS positive definiteness. Pages 136-140 are end of Chapter 5 discussing preliminary quasi-Newton concepts. Pages 156-161 are from Chapter 6 (Quasi-Newton Methods) dedicated to BFGS theory. (2) Equations (6.7) and (6.8) CORRECTLY located on page 157. Eq (6.7) defines curvature condition $s_k^T y_k > 0$. Eq (6.8) proves this holds under Wolfe conditions: $y_k^T s_k \\geq (c_2 - 1)\\nabla f_k^T s_k > 0$. (3) The positive definiteness preservation is NOT a formal numbered theorem but an ARGUMENT on page 161. Citation correctly refers to 'Section 6.1' rather than claiming a theorem number. (4) Quote is WORD-FOR-WORD accurate from page 161, matching exactly including punctuation and mathematical notation. (5) Claim accurately reflects source without overstatement. (6) Usage in LbfgsTab.tsx correctly cites this for curvature filtering ($s^T y > 0$ acceptance criterion). VERDICT: Citation is accurate, appropriately scoped, and the non-contiguous page range is necessary and correct. The gap exists because pages 141-155 are irrelevant to BFGS positive definiteness.",
      "usedIn": [
        "LbfgsTab"
      ]
    },
    "newton-quadratic-convergence": {
      "reference": "nocedal-wright-2006",
      "pages": "64-65",
      "theorem": "Theorem 3.5",
      "claim": "Newton's method achieves quadratic convergence on strongly convex functions with Lipschitz continuous Hessian, when starting close enough to the optimum",
      "quote": "Suppose that $f$ is twice differentiable and that the Hessian $\\nabla^2 f(x)$ is Lipschitz continuous in a neighborhood of a solution $x^*$ at which the sufficient conditions (Theorem 2.4) are satisfied. Consider the iteration $x_{k+1} = x_k + p_k$, where $p_k$ is given by (3.30). Then (i) if the starting point $x_0$ is sufficiently close to $x^*$, the sequence of iterates converges to $x^*$; (ii) the rate of convergence of $\\{x_k\\}$ is quadratic; and (iii) the sequence of gradient norms $\\{\\|\\nabla f_k\\|\\}$ converges quadratically to zero.",
      "notes": "Internal: Theorem 3.5 establishes quadratic convergence for Newton's method under the key conditions: (1) twice differentiable function, (2) Lipschitz continuous Hessian near the solution, (3) second-order sufficient conditions satisfied at $x^*$ (i.e., $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*)$ positive definite), and (4) starting point sufficiently close to the solution. The proof shows $\\|x_{k+1} - x^*\\| \\leq \\tilde{L}\\|x_k - x^*\\|^2$ where $\\tilde{L} = L\\|\\nabla^2 f(x^*)^{-1}\\|$ and $L$ is the Lipschitz constant of the Hessian. Equation (3.30) refers to the Newton step $p_k^N = -(\\nabla^2 f_k)^{-1}\\nabla f_k$. Theorem 2.4 refers to second-order sufficient conditions for a local minimizer. Used in NewtonTab to explain the theoretical convergence rate.",
      "readerNotes": "Newton's method achieves quadratic convergence under three key conditions: (1) The Hessian $\\nabla^2 f(x)$ must be Lipschitz continuous near the solution $x^*$, meaning $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$ for some constant $L > 0$. This is stronger than just having a continuous Hessian. (2) The solution $x^*$ must satisfy second-order sufficient conditions: $\\nabla f(x^*) = 0$ (first-order optimality) and $\\nabla^2 f(x^*)$ is positive definite (second-order optimality). This means $x^*$ is a strict local minimum where the Hessian has all positive eigenvalues. (3) The starting point $x_0$ must be sufficiently close to $x^*$. The convergence rate is $\\|x_{k+1} - x^*\\| \\leq \\tilde{L}\\|x_k - x^*\\|^2$ where $\\tilde{L} = L\\|\\nabla^2 f(x^*)^{-1}\\|$, demonstrating that the error is squared at each iteration. This quadratic convergence means the number of correct digits roughly doubles at each step once the method is close enough to the solution. The Lipschitz continuous Hessian condition is crucial: it ensures the Hessian doesn't change too rapidly, allowing the quadratic model to be accurate. For strongly convex functions (where $\\mu I \\preceq \\nabla^2 f(x) \\preceq LI$ globally), these conditions are satisfied automatically once we're in the basin of attraction.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0064.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0065.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "INDEPENDENTLY VERIFIED following ENHANCED workflow (citation-verification.md): (1) QUOTE ACCURACY: Verified word-for-word against OCR (pages 61-70) and visual inspection of proof pages 64-65. Quote matches Theorem 3.5 exactly, including punctuation and mathematical notation. (2) PAGE NUMBERS: Confirmed pages 64-65 are correct - page 64 contains the theorem statement (lines 195-202 of OCR), page 65 contains the proof (lines 210-269). PDF pages match printed book pages. (3) THEOREM REFERENCE: Theorem 3.5 correctly identified. Equation (3.30) referenced in quote is defined on page 64 line 180 as $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$. Theorem 2.4 (second-order sufficient conditions) referenced in quote. (4) CLAIM CORRECTNESS: The claim accurately reflects the theorem. The claim says 'strongly convex functions with Lipschitz continuous Hessian' which is slightly stronger than what the theorem requires (theorem only requires Lipschitz continuous Hessian and second-order sufficient conditions at $x^*$), but this is acceptable as strongly convex functions satisfy these conditions. (5) USAGE IN CODE: Checked NewtonTab.tsx lines 327, 526, 642, 662 - all uses are contextually appropriate, explaining quadratic convergence rate near the solution. (6) PROOF VERIFICATION: The proof on page 65 shows the quadratic bound in equation (3.33): $\\|x_k + p_k^N - x^*\\| \\leq L\\|\\nabla^2 f(x^*)^{-1}\\|\\|x_k - x^*\\|^2$ where $\\tilde{L} = L\\|\\nabla^2 f(x^*)^{-1}\\|$ and $L$ is the Lipschitz constant. This establishes $\\|e_{k+1}\\| \\leq C\\|e_k\\|^2$ (quadratic convergence). The three parts of the theorem are: (i) convergence when starting close enough, (ii) quadratic rate, (iii) gradient norms converge quadratically to zero. All verified correct.",
      "usedIn": [
        "NewtonTab"
      ]
    },
    "newton-convex-convergence": {
      "reference": "nesterov-2018",
      "pages": "243-258",
      "theorem": "Theorem 4.1.2, Theorem 4.1.6, and Example 4.1.1",
      "claim": "For convex functions with Lipschitz continuous Hessian, Newton's method (with cubic regularization) converges to the global minimum. Specifically, the method finds stationary points satisfying $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*) \\succeq 0$, which for convex functions are global minima.",
      "quote": "Theorem 4.1.2: Let the sequence $\\{x_i\\}$ be generated by method (4.1.16). Let us assume that for some $i \\geq 0$ the set $\\mathscr{L}(f(x_i))$ is bounded. Then there exists a limit $\\lim_{i\\to\\infty} f(x_i) = f^*$. The set $X^*$ of limit points of this sequence is non-empty. Moreover, this is a connected set such that for any $x^* \\in X^*$ we have $f(x^*) = f^*$, $\\nabla f(x^*) = 0$, $\\nabla^2 f(x^*) \\succeq 0$. [...] Example 4.1.1 (Convex Functions): Let $f$ be convex on $\\mathbb{R}^n$. Assume it achieves its minimum at point $x^*$. Then, for any $x \\in \\mathbb{R}^n$ with $\\|x - x^*\\| < R$, we have $f(x) - f(x^*) \\leq \\langle\\nabla f(x), x - x^*\\rangle \\leq \\|\\nabla f(x)\\| \\cdot R$. Thus, the function $f$ is a gradient dominated function of degree one. [...] Theorem 4.1.6: If $f(x_0) - f(x^*) \\leq \\gamma^2 \\hat{\\omega}$, then $f(x_k) - f(x^*) \\leq \\hat{\\omega} \\cdot \\frac{\\gamma^2}{(2 + k + \\frac{3}{2\\gamma})^2}$.",
      "notes": "Internal: This citation addresses Newton's method convergence on convex functions with Lipschitz continuous Hessian. The key insight is that convex functions are gradient-dominated of degree 1 (Example 4.1.1), and the Cubic Regularization of Newton's Method converges for such functions. Assumption 4.1.1 (page 262) requires Lipschitz continuous Hessian: $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$. For convex functions, stationary points with positive semidefinite Hessian are global minima (Theorem 1.2.2 on page 33, combined with convexity). The convergence rate for gradient-dominated functions of degree 1 is $O(1/k^2)$ (Theorem 4.1.6, equation 4.1.36). This is the cubic regularized Newton method (4.1.16), not the classical Newton method, but it handles the case where Hessian may not be positive definite everywhere. Used in NewtonTab.",
      "readerNotes": "Newton's method convergence on convex functions requires careful treatment because the Hessian may not be positive definite everywhere (only at the minimum). Nesterov's Cubic Regularization of Newton's Method (Algorithm 4.1.16) addresses this by using cubic regularization: at each iteration, minimize $\\nabla f(x_k)^T(y - x_k) + \\frac{1}{2}(y - x_k)^T\\nabla^2 f(x_k)(y - x_k) + \\frac{M}{6}\\|y - x_k\\|^3$ where $M \\geq L$ is the Lipschitz constant of the Hessian. This regularization ensures the subproblem is always well-defined. For convex functions achieving their minimum at $x^*$, the convergence guarantee follows from two results: (1) Example 4.1.1 shows that convex functions are gradient-dominated of degree 1, meaning $f(x) - f(x^*) \\leq \\|\\nabla f(x)\\| \\cdot R$ for $\\|x - x^*\\| < R$. (2) Theorem 4.1.6 establishes that for gradient-dominated functions of degree 1, the method achieves $f(x_k) - f(x^*) \\leq O(1/k^2)$ convergence. (3) Theorem 4.1.2 guarantees that limit points satisfy $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*) \\succeq 0$, which for convex functions are precisely the global minima. The requirement of Lipschitz continuous Hessian (Assumption 4.1.1: $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$) is essential for the analysis and is satisfied by many practical functions including twice-differentiable convex functions with bounded Hessians.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0262.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0263.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0264.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0265.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0266.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0267.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0268.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0269.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0270.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0271.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0272.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0273.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0274.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0275.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0276.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0277.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "VERIFIED (Batch 5 - Adversarial): FIXED page numbering from PDF pages to book pages (262-277 → 243-258). Added missing proof pages (268-270 for Theorem 4.1.2 continuation, 272-273 for connecting theorems). All 16 pages verified necessary: pages 262-267 (Assumption 4.1.1, Algorithm 4.1.16, supporting lemmas), pages 268-270 (Theorem 4.1.2 complete statement), pages 271-273 (connection to gradient dominated functions), pages 274-275 (Example 4.1.1 showing convex functions are gradient dominated), pages 276-277 (Theorem 4.1.6 convergence rate). Quote is accurate, claim correctly describes cubic regularized Newton's method converging to global minimum for convex functions with Lipschitz continuous Hessian. The 16-page range is justified.",
      "usedIn": [
        "NewtonTab"
      ]
    },
    "newton-computational-complexity": {
      "reference": "nocedal-wright-2006",
      "pages": "64, 627-628",
      "theorem": "Section 3.3 (Newton's Method) and Appendix A.1 (Cholesky/LU Factorization)",
      "claim": "Solving the Newton system $H \\cdot p = -\\nabla f$ requires $O(d^3)$ operations using direct methods (Cholesky or LU decomposition)",
      "quote": "Newton's method: $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$ [...] The factorization (A.20) can be found by using Gaussian elimination with row partial pivoting, an algorithm that requires approximately $2n^3/3$ floating-point operations when $A$ is dense. [...] When $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, it is possible to compute a similar but more specialized factorization at about half the cost—about $n^3/3$ operations. This factorization, known as the Cholesky factorization, produces a matrix $L$ such that $A = LL^T$.",
      "notes": "Internal: Used in NewtonTab to explain computational cost of Newton's method. Page 64 defines Newton's method as computing the search direction $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$, which requires solving the linear system $\\nabla^2 f_k \\cdot p_k = -\\nabla f_k$. Pages 627-628 (Appendix A.1) document the computational complexity of solving dense linear systems: LU decomposition via Gaussian elimination requires approximately $2n^3/3$ operations (equation A.20, Algorithm A.1), while Cholesky factorization for symmetric positive definite matrices requires about $n^3/3$ operations (equation A.23, Algorithm A.2). Both are $O(n^3)$ methods. The Newton system has the Hessian matrix as the coefficient matrix, so solving it requires $O(d^3)$ operations where $d$ is the problem dimension (number of variables). In the book, $n$ is used for dimension; we use $d$ in the codebase.",
      "readerNotes": "Newton's method computes the search direction by solving the linear system $H \\cdot p = -\\nabla f$ where $H = \\nabla^2 f$ is the Hessian matrix (see equation 3.30 on page 64: $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$). This requires solving a $d \\times d$ linear system where $d$ is the number of optimization variables. For dense matrices, direct solution methods require $O(d^3)$ operations: **Gaussian elimination with LU decomposition** requires approximately $2d^3/3$ floating-point operations (page 627, Algorithm A.1), while **Cholesky factorization** (applicable when the Hessian is positive definite) requires about $d^3/3$ operations (page 628, Algorithm A.2). Both methods scale cubically with the problem dimension. In practice, once the factorization is computed, solving the system via forward- and back-substitution requires only $O(d^2)$ operations, so the dominant cost is the $O(d^3)$ factorization step. For large-scale problems, this cubic cost motivates the use of quasi-Newton methods (like BFGS or L-BFGS) which avoid computing and factorizing the full Hessian.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0064.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0627.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0628.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "ENHANCED VERIFICATION (563-page gap scrutiny): ✅ VERIFIED - The 563-page gap between page 64 and pages 627-628 is LEGITIMATE and intentional. This citation correctly combines two essential components: (1) Newton's method definition from Chapter 3 (page 64, equation 3.30), and (2) computational complexity analysis from Appendix A.1 (pages 627-628, Algorithms A.1 and A.2). VERIFICATION DETAILS: Page 64 shows equation 3.30: p_k^N = -∇²f_k^(-1)∇f_k, which defines Newton's search direction requiring solution of the linear system ∇²f_k·p_k = -∇f_k. Page 627 presents Algorithm A.1 (Gaussian Elimination with Partial Pivoting) and equation (A.20) stating LU factorization requires approximately 2n³/3 floating-point operations for dense n×n matrices. Page 628 presents Algorithm A.2 (Cholesky Factorization) stating it requires about n³/3 operations for symmetric positive definite matrices. Both algorithms are O(n³). Quote accuracy: VERIFIED word-for-word against source pages. Claim consistency: VERIFIED - claim correctly states O(d³) complexity (using d for dimension as per codebase convention, source uses n). The claim accurately reflects that solving the Newton system Hessian requires O(d³) operations. Cross-references VERIFIED: Used in NewtonTab (line 325) and DiagonalPrecondTab (lines 228, 504, 742, 768, 777) - all usage consistent with citation claim. GAP JUSTIFICATION: This is NOT a split citation error. The gap exists because Newton's method (Chapter 3, page 64) defines WHAT needs to be solved (the linear system), while Appendix A.1 (pages 627-628) provides the computational complexity analysis of HOW to solve dense linear systems. Both are essential to support the computational cost claim. This follows the workflow guideline: 'Appendix references: Generally DON'T include appendix pages in proofPages... Exception: If the appendix formula is obscure and critical, consider including' - here the appendix algorithms ARE critical as they provide the O(n³) complexity proof.",
      "usedIn": [
        "NewtonTab",
        "DiagonalPrecondTab"
      ]
    },
    "lbfgs-linear-convergence-liu-nocedal-1989": {
      "reference": "liu-nocedal-1989",
      "pages": "21-24",
      "theorem": "Theorem 6.1",
      "claim": "L-BFGS converges R-linearly (not superlinearly) on uniformly convex problems. The rate of convergence is linear, not superlinear. With sufficient memory M, L-BFGS approaches BFGS behavior but does not achieve superlinear convergence.",
      "quote": "In this section we show that the limited memory BFGS method is globally convergent on uniformly convex problems, and that its rate of convergence is R-linear... Then for any positive definite $B_0$, Algorithm 6.1 generates a sequence $\\{x_k\\}$ which converges to $x^*$. Moreover there is a constant $0 \\leq r < 1$ such that $f_k - f^* \\leq r^k[f_0 - f^*]$ which implies that $\\{x_k\\}$ converges R-linearly... R-linear convergence is the best we can expect.",
      "notes": "Internal: CRITICAL CORRECTION - This citation establishes that L-BFGS has LINEAR convergence, NOT superlinear. The paper explicitly states 'R-linear convergence is the best we can expect' (p. 23). This is in contrast to full BFGS, which can achieve superlinear convergence. Nocedal & Wright (2006) p. 196 also confirms L-BFGS has 'acceptable (albeit linear) rate of convergence.' The memory parameter M affects the constant in the linear convergence rate, not the order of convergence. Used in AlgorithmExplainer to correct the incorrect superlinear convergence claim.",
      "readerNotes": "**IMPORTANT:** L-BFGS achieves only **linear convergence**, not superlinear. Liu & Nocedal (1989) Theorem 6.1 proves that L-BFGS has R-linear convergence, and the paper explicitly states 'R-linear convergence is the best we can expect' (p. 23). This is fundamentally different from full BFGS, which can achieve superlinear convergence (Nocedal & Wright 2006, Theorem 6.6). The limited memory in L-BFGS (storing only $M$ recent curvature pairs) prevents the Hessian approximation from fully converging to the true Hessian, which limits the convergence rate to linear. The memory parameter $M$ affects the constant in the linear convergence rate but does NOT change the order of convergence to superlinear. Nocedal & Wright (2006) p. 196 confirms: L-BFGS yields 'an acceptable (albeit linear) rate of convergence.' Despite being 'only' linear, L-BFGS's convergence is still very effective in practice due to its low memory requirements ($O(Md)$ vs $O(d^2)$ for full BFGS) which enable its use on large-scale problems where full BFGS would be infeasible.",
      "proofPages": [
        "docs/references/extracted-pages/liunocedal1989_page_0021.png",
        "docs/references/extracted-pages/liunocedal1989_page_0022.png",
        "docs/references/extracted-pages/liunocedal1989_page_0023.png",
        "docs/references/extracted-pages/liunocedal1989_page_0024.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified by reading Liu & Nocedal (1989) Section 7 'Convergence Analysis' which contains Theorem 6.1 proving R-linear convergence. Cross-referenced with Nocedal & Wright (2006) p. 196 which states L-BFGS yields 'an acceptable (albeit linear) rate of convergence.' The claim of superlinear convergence is false and contradicts the theoretical results. This citation corrects a common misconception about L-BFGS convergence rate.",
      "usedIn": [
        "AlgorithmExplainer"
      ]
    },
    "inexact-newton-superlinear-convergence": {
      "reference": "nocedal-wright-2006",
      "pages": "165-169",
      "theorem": "Theorem 7.2",
      "claim": "Inexact Newton methods solve the Newton system $\\nabla^2 f_k p_k = -\\nabla f_k$ approximately using iterative methods like conjugate gradient, reducing computational cost from $O(d^3)$ to $O(d^2)$ or better while maintaining superlinear convergence with appropriate forcing sequences",
      "quote": "In this section, we describe techniques for obtaining approximations to $p_k^N$ that are inexpensive to calculate but are good search directions or steps. These approaches are based on solving (7.1) by using the conjugate gradient (CG) method (see Chapter 5) or the Lanczos method, with modifications to handle negative curvature in the Hessian $\\nabla^2 f_k$... The use of iterative methods for (7.1) spares us from concerns about the expense of a direct factorization of the Hessian $\\nabla^2 f_k$ and the fill-in that may occur during this process... Theorem 7.2: Suppose that the conditions of Theorem 7.1 hold, and assume that the iterates $\\{x_k\\}$ generated by the inexact Newton method converge to $x^*$. Then the rate of convergence is superlinear if $\\eta_k \\to 0$. If in addition, $\\nabla^2 f(x)$ is Lipschitz continuous for $x$ near $x^*$ and if $\\eta_k = O(\\|\\nabla f_k\\|)$, then the convergence is quadratic.",
      "notes": "Internal: Section 7.1 (pages 165-169) introduces inexact Newton methods and their motivation. Key points: (1) Direct factorization of Hessian costs $O(d^3)$, iterative methods reduce this to $O(d^2)$ per iteration or better depending on problem structure. (2) Methods solve Newton system approximately using CG or Lanczos with residual tolerance $\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|$ where $\\{\\eta_k\\}$ is the forcing sequence. (3) Theorem 7.1 (p. 186) proves local convergence when $\\eta_k \\leq \\eta < 1$. (4) Theorem 7.2 (p. 188) proves superlinear convergence when $\\eta_k \\to 0$, and quadratic convergence when $\\eta_k = O(\\|\\nabla f_k\\|)$. (5) Common choice: $\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})$ for superlinear convergence. (6) Page 166 explicitly states this applies 'not just to Newton-CG procedures but to all inexact Newton methods whose steps satisfy (7.2) and (7.3).' Used in NewtonTab to justify using CG for large-scale problems.",
      "readerNotes": "Inexact Newton methods solve the Newton system $\\nabla^2 f_k p_k = -\\nabla f_k$ approximately rather than exactly. Instead of computing an exact factorization of the Hessian (which costs $O(d^3)$ operations), these methods use iterative linear solvers like conjugate gradient (CG) that terminate after achieving residual $\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|$ for some tolerance $\\eta_k \\in (0,1)$. For sparse or structured problems, each CG iteration costs only $O(d^2)$ or even $O(d)$ operations (matrix-vector products), making the method practical for large-scale optimization. The forcing sequence $\\{\\eta_k\\}$ controls the trade-off between per-iteration cost and convergence rate: (1) If $\\eta_k \\leq \\eta < 1$ (constant), the method converges linearly (Theorem 7.1). (2) If $\\eta_k \\to 0$, the method converges superlinearly (Theorem 7.2). (3) If $\\eta_k = O(\\|\\nabla f_k\\|)$, the method converges quadratically (Theorem 7.2). A common practical choice is $\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})$ which achieves superlinear convergence while keeping early iterations cheap. The method can even be implemented in a 'Hessian-free' manner using finite differences to approximate Hessian-vector products $\\nabla^2 f_k d \\approx (\\nabla f(x_k + hd) - \\nabla f(x_k))/h$ (page 190, equation 7.10), requiring only gradient evaluations.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0185.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0186.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0187.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0188.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0189.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "verification-agent",
      "verificationNotes": "VERIFIED (Batch 5 - Adversarial): FIXED critical page numbering error. Original proofPages had PDF pages 0165-0169 which contained WRONG content (SR1 Method from Chapter 6). Corrected to PDF pages 0185-0189 (book pages 165-169) which contain Section 7.1 'Inexact Newton Methods'. The 20-page offset was not accounted for in original citation. Quote is word-for-word accurate. Theorem 7.1 (page 186) and Theorem 7.2 (page 188) verified. NOTE: The claim's 'O(d³) to O(d²)' complexity reduction is an INTERPRETATION - the source says 'spares us from concerns about the expense of a direct factorization' but doesn't explicitly state Big-O notation. This interpretation is correct and well-justified (direct factorization = O(d³), CG matrix-vector products = O(d²) for dense or O(d) for sparse).",
      "usedIn": [
        "NewtonTab"
      ]
    },
    "rosenbrock-function-benchmark": {
      "reference": "rosenbrock-1960",
      "pages": "175-184",
      "claim": "",
      "notes": "BENCHMARK FUNCTION ATTRIBUTION: This citation provides historical attribution for the Rosenbrock function (also known as Rosenbrock's valley or banana function), a classic non-convex test problem for optimization algorithms. The function $f(x,y) = (1-x)^2 + b(y-x^2)^2$ was introduced in this 1960 paper on automatic optimization methods. This citation is for attribution only - no specific theorem or claim is being cited. The 'claim' field is intentionally empty for benchmark function attributions.",
      "readerNotes": "The Rosenbrock function, introduced by H.H. Rosenbrock in 1960, is one of the most famous benchmark functions for testing optimization algorithms. The function creates a narrow, curved valley (resembling a banana) where the global minimum is easy to find but difficult to converge to, making it ideal for demonstrating the challenges of non-convex optimization.",
      "usedIn": [
        "rosenbrockProblem",
        "ProblemExplainer"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Original paper extracted and verified. The paper introduces an automatic method for optimization and presents the test function that became known as the Rosenbrock function. This is a benchmark function attribution citation (no specific claim)."
    },
    "three-hump-camel-function-benchmark": {
      "reference": "branin-1972",
      "pages": "504-522",
      "claim": "",
      "notes": "BENCHMARK FUNCTION ATTRIBUTION: This citation provides historical attribution for the Three-Hump Camel function, a multi-modal test problem introduced by F.H. Branin in this 1972 paper on finding multiple solutions of nonlinear equations. The function $f(x,y) = 2x^2 - 1.05x^4 + \\frac{x^6}{6} + xy + y^2$ has three local minima, making it useful for testing global optimization algorithms. This citation is for attribution only - no specific theorem or claim is being cited. The 'claim' field is intentionally empty for benchmark function attributions.",
      "readerNotes": "The Three-Hump Camel function was introduced by F.H. Branin in 1972 as a test problem for algorithms designed to find multiple solutions. It features three local minima (one global at the origin and two symmetric local minima), making it particularly useful for demonstrating basin of convergence and the challenges of multi-modal optimization.",
      "usedIn": [
        "threeHumpCamelProblem",
        "ProblemExplainer"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Original paper extracted and verified. The paper presents methods for finding multiple solutions and includes various test functions including what became known as the Three-Hump Camel function. This is a benchmark function attribution citation (no specific claim)."
    }
  }
}