{
  "references": {
    "nocedal-wright-2006": {
      "title": "Numerical Optimization",
      "authors": ["Jorge Nocedal", "Stephen J. Wright"],
      "year": 2006,
      "edition": "2nd",
      "publisher": "Springer",
      "file": "NumericalOptimization2006.pdf"
    },
    "boyd-vandenberghe-2004": {
      "title": "Convex Optimization",
      "authors": ["Stephen Boyd", "Lieven Vandenberghe"],
      "year": 2004,
      "publisher": "Cambridge University Press",
      "file": "Boyd+Vandenberghe-2004-Convex_optimization.pdf"
    },
    "nesterov-2004": {
      "title": "Introductory Lectures on Convex Optimization: A Basic Course",
      "authors": ["Yurii Nesterov"],
      "year": 2004,
      "publisher": "Kluwer Academic Publishers",
      "file": "Introductory-Lectures-on-Convex-Programming-Yurii-Nesterov-2004.pdf"
    },
    "liu-nocedal-1989": {
      "title": "On the Limited Memory BFGS Method for Large Scale Optimization",
      "authors": ["Dong C. Liu", "Jorge Nocedal"],
      "year": 1989,
      "journal": "Mathematical Programming",
      "volume": "45",
      "pages": "503-528",
      "file": "LiuNocedal1989.pdf"
    }
  },
  "citations": {
    "gd-strongly-convex-linear-convergence": {
      "reference": "nesterov-2004",
      "pages": "86-87",
      "theorem": "Theorem 2.1.15",
      "claim": "Gradient descent with fixed step size achieves linear convergence to the global minimum on strongly convex smooth functions when 0 < α < 2/(L+μ)",
      "quote": "If f ∈ S^(1,1)_μ,L(R^n) and 0 < h < 2/(L+μ), then the gradient method generates a sequence {x_k} such that ||x_k - x*||^2 ≤ (1 - (2hμL)/(L+μ))^k ||x_0 - x*||^2",
      "notes": "Internal: Used in GdFixedTab to explain strongly convex convergence. Compare with Theorem 2.1.14 (convex case) - different step size bounds.",
      "readerNotes": "The notation S^(1,1)_μ,L(ℝⁿ) denotes strongly convex functions with strong convexity parameter μ > 0 and Lipschitz continuous gradient with constant L (see Definition 2.1.1 on page 82). The condition number Q = L/μ determines the convergence rate. With optimal step size, the convergence rate is ((Q-1)/(Q+1))ᵏ, which provides exponentially fast (linear) convergence. Note: Nesterov uses 'h' for step size in the theorem statement; here we use 'α'. This result differs from the merely convex case (Theorem 2.1.14), which has step size bound 2/L instead of 2/(L+μ).",
      "proofPages": [
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0082.png",
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0085.png",
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0086.png",
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0087.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "verification-agent",
      "verificationNotes": "Independently verified: quote is word-for-word accurate, claim matches source, usage in GdFixedTab.tsx is consistent. Added page 82 (notation definition) and page 85 (gradient method setup) to proofPages for better context. Expanded notes to clarify notation translation (h→α) and distinction from Theorem 2.1.14.",
      "usedIn": ["GdFixedTab"]
    },
    "gd-convex-sublinear-convergence": {
      "reference": "nesterov-2004",
      "pages": "86-87",
      "theorem": "Theorem 2.1.14 and Corollary 2.1.2",
      "claim": "Gradient descent with fixed step size converges to the global minimum on convex smooth functions (possibly slowly with sublinear rate) when 0 < α < 2/L",
      "quote": "Let f ∈ F_L^{1,1}(R^n) and 0 < h < 2/L. Then the gradient method generates a sequence {x_k}, which converges... [With optimal step size] If h = 1/L and f ∈ F_L^{1,1}(R^n), then f(x_k) - f* ≤ 2L||x_0 - x*||^2/(k+4)",
      "notes": "Internal: Used in GdFixedTab to explain convex (non-strongly) convergence. This is the μ=0 case compared to Theorem 2.1.15.",
      "readerNotes": "The notation F_L^{1,1}(ℝⁿ) denotes convex functions with Lipschitz continuous gradient with constant L. With optimal step size α = 1/L, the convergence rate is O(1/k), which is sublinear convergence. Note: Nesterov uses 'h' for step size in the theorem statement; here we use 'α'. This is much slower than the exponentially fast convergence for strongly convex functions (Theorem 2.1.15, with step size bound 2/(L+μ)) - without strong convexity, gradient descent loses the geometric convergence rate and can only guarantee polynomial convergence.",
      "proofPages": [
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0082.png",
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0085.png",
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0086.png",
        "docs/references/extracted-pages/introductory-lectures-on-convex-programming-yurii-nesterov-2004_ocr_page_0087.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "citation-creation-agent",
      "verificationNotes": "Verified Theorem 2.1.14 and Corollary 2.1.2. Quote extracted from visual verification of PDF pages. The O(1/k) sublinear rate directly supports the claim that convergence is 'possibly slow'. All proof pages include necessary context (notation definition, gradient method setup, theorem statements).",
      "usedIn": ["GdFixedTab"]
    }
  }
}
