{
  "references": {
    "nocedal-wright-2006": {
      "title": "Numerical Optimization",
      "authors": [
        "Jorge Nocedal",
        "Stephen J. Wright"
      ],
      "year": 2006,
      "edition": "2nd",
      "publisher": "Springer",
      "file": "NumericalOptimization2006.pdf"
    },
    "boyd-vandenberghe-2004": {
      "title": "Convex Optimization",
      "authors": [
        "Stephen Boyd",
        "Lieven Vandenberghe"
      ],
      "year": 2004,
      "publisher": "Cambridge University Press",
      "file": "Boyd+Vandenberghe-2004-Convex_optimization.pdf"
    },
    "nesterov-2004": {
      "title": "Introductory Lectures on Convex Optimization: A Basic Course",
      "authors": [
        "Yurii Nesterov"
      ],
      "year": 2004,
      "publisher": "Kluwer Academic Publishers",
      "file": "Introductory-Lectures-on-Convex-Programming-Yurii-Nesterov-2004.pdf"
    },
    "liu-nocedal-1989": {
      "title": "On the Limited Memory BFGS Method for Large Scale Optimization",
      "authors": [
        "Dong C. Liu",
        "Jorge Nocedal"
      ],
      "year": 1989,
      "journal": "Mathematical Programming",
      "volume": "45",
      "pages": "503-528",
      "file": "LiuNocedal1989.pdf"
    },
    "nesterov-2018": {
      "title": "Lectures on Convex Optimization",
      "authors": [
        "Yurii Nesterov"
      ],
      "year": 2018,
      "edition": "2nd",
      "publisher": "Springer",
      "file": "Lectures on Convex Optimization.pdf"
    }
  },
  "citations": {
    "gd-strongly-convex-linear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "101-102",
      "theorem": "Theorem 2.1.15",
      "claim": "Gradient descent with fixed step size achieves linear convergence to the global minimum on strongly convex smooth functions when $0 < \\alpha \\leq 2/(L+\\mu)$",
      "quote": "If $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ and $0 < h \\leq \\frac{2}{\\mu+L}$, then the Gradient Method generates a sequence $\\{x_k\\}$ such that $\\|x_k - x^*\\|^2 \\leq \\left(1 - \\frac{2h\\mu L}{\\mu+L}\\right)^k \\|x_0 - x^*\\|^2$",
      "notes": "Internal: This is the updated 2nd edition (2018) version of the same result from the 2004 edition. The theorem is essentially the same but with slightly refined notation using $\\mathscr{S}_{\\mu,L}^{1,1}$ instead of $S_{\\mu,L}^{1,1}$. The step size bound is $h \\leq 2/(\\mu+L)$ (allowing equality) instead of $h < 2/(L+\\mu)$. Can be used for comparison with the 2004 edition to determine which source to recommend.",
      "readerNotes": "The notation $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ denotes strongly convex functions with strong convexity parameter $\\mu > 0$ and Lipschitz continuous gradient with constant $L$ (see Definition 2.1.3 on page 94). The condition number $Q = L/\\mu$ determines the convergence rate. With optimal step size $h = 2/(L+\\mu)$, the convergence rate is $\\left(\\frac{L-\\mu}{L+\\mu}\\right)^{2k} = \\left(\\frac{Q-1}{Q+1}\\right)^{2k}$, which provides exponentially fast (linear) convergence. Note: Nesterov uses $h$ for step size in the theorem statement; here we use $\\alpha$. This result differs from the merely convex case (Theorem 2.1.14), which has step size bound $2/L$ instead of $2/(L+\\mu)$.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0093.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0094.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0101.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0102.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 2.1.15 (pages 101-102) and Definition 2.1.3 (page 94) for strongly convex functions. The quote is word-for-word accurate from the PDF. The theorem appears on page 101 (starting at the bottom) and continues to page 102. The step size condition allows equality ($h \\leq 2/(\\mu+L)$) unlike the 2004 edition which used strict inequality. Pages 93-94 provide the definition and characterization of strongly convex functions including Theorem 2.1.9 which gives equivalent conditions.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "nesterov-accelerated-optimal-rate-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "102-114",
      "theorem": "Theorem 2.2.2 and Constant Step Scheme III (2.2.22)",
      "claim": "Nesterov's accelerated gradient method achieves the optimal convergence rate $O(1/k^2)$ for smooth convex functions, which is provably optimal among all first-order methods",
      "quote": "Let us take in (2.2.7) $\\gamma_0 = 3L + \\mu$. Then this scheme generates a sequence $\\{x_k\\}_{k=0}^{\\infty}$ such that $f(x_k) - f^* \\leq \\frac{2(4+q_f)L\\|x_0-x^*\\|^2}{3(k+1)^2}$. This means that method (2.2.7) is optimal for solving the unconstrained minimization problem (2.2.1) with $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ and $\\mu \\geq 0$. If $\\mu = 0$, then this method is optimal.",
      "notes": "Internal: Used in GdFixedTab to explain Nesterov acceleration. This is the 2018 edition version with refined calligraphic notation. The general scheme (2.2.7) is developed in pages 102-111, with the constant step variant (Scheme III, equation 2.2.22) on page 114. Optimality is proven in Theorem 2.2.2 by comparing with Theorem 2.1.7 lower bound (pages 91-92). For \u03bc=0 (smooth convex), equation (2.2.18) on page 112 gives the explicit rate: $f(x_k) - f^* \\leq \\frac{8L\\|x_0-x^*\\|^2}{3(k+1)^2}$, which is O(1/k\u00b2) compared to gradient descent's O(1/k) rate.",
      "readerNotes": "The notation $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ denotes functions with strong convexity parameter $\\mu \\geq 0$ and Lipschitz continuous gradient with constant $L$. When $\\mu = 0$, this reduces to $\\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$, the class of smooth convex functions. The $O(1/k^2)$ convergence rate is provably optimal: Theorem 2.1.7 (pages 91-92) establishes a lower bound showing that no first-order method can achieve better than $O(1/k^2)$ convergence for this function class. Nesterov's accelerated method matches this lower bound up to constant factors, proving optimality. Constant Step Scheme III (page 114, equation 2.2.22) shows the classic momentum form: $x_{k+1} = y_k - \\frac{1}{L}\\nabla f(y_k)$, $y_{k+1} = x_{k+1} + \\frac{1-\\sqrt{q_f}}{1+\\sqrt{q_f}}(x_{k+1} - x_k)$ where $q_f = \\mu/L$. For smooth convex functions ($\\mu=0$), the method requires a different parameter choice to avoid division by zero, as shown in Constant Step Scheme II (page 113).",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0091.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0092.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0102.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0103.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0104.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0105.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0106.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0107.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0108.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0109.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0110.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0111.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0112.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0113.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0114.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 2.2.2 (pages 110-111, optimality claim), Constant Step Scheme III (2.2.22) on page 114 which is the classic Nesterov acceleration with momentum, and Theorem 2.1.7 (pages 91-92, lower complexity bound). The quote is from page 110 (Theorem 2.2.2). Visual verification confirmed the O(1/k\u00b2) rate for \u03bc=0 (equation 2.2.18 on page 112) and the optimality proof which shows the method matches the information-theoretic lower bound. Pages 102-109 develop the estimating sequence framework (Definition 2.2.1, Lemmas 2.2.1-2.2.4) and general optimal method scheme (2.2.7). Pages 110-111 prove optimality. Pages 112-114 derive the simplified constant step schemes with explicit momentum coefficients.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "gd-convex-sublinear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "81",
      "theorem": "Theorem 2.1.14 and Corollary 2.1.2",
      "claim": "Gradient descent with fixed step size converges to the global minimum on convex smooth functions (possibly slowly with sublinear rate) when $0 < \\alpha \\leq 2/L$",
      "quote": "Let $f \\in \\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ and $0 < h \\leq 2/L$. Then the Gradient Method generates a sequence $\\{x_k\\}$ such that $\\|x_k - x^*\\|^2 \\leq q^k \\|x_0 - x^*\\|^2$, where $q = 1 - \\frac{h}{2}(2-hL) \\in [0,1)$. [Corollary 2.1.2] If $h = 1/L$ and $f \\in \\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$, then $f(x_k) - f^* \\leq \\frac{L\\|x_0-x^*\\|^2}{2(k+1)}$",
      "notes": "Internal: Used in GdFixedTab to explain convex (non-strongly) convergence. This is the 2018 edition update of the 2004 result, using calligraphic notation $\\mathscr{F}_L^{1,1}$ instead of $F_L^{1,1}$. The step size bound allows equality ($h \\leq 2/L$) instead of strict inequality ($h < 2/L$) in the 2004 edition. This is the $\\mu=0$ case compared to Theorem 2.1.15.",
      "readerNotes": "The notation $\\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ denotes convex functions with Lipschitz continuous gradient with constant $L$ (see Definition 2.1.2 on page 62). With optimal step size $\\alpha = 1/L$, the convergence rate is $O(1/k)$, which is sublinear convergence. Note: Nesterov uses $h$ for step size in the theorem statement; here we use $\\alpha$. This is much slower than the exponentially fast convergence for strongly convex functions (Theorem 2.1.15, with step size bound $2/(L+\\mu)$) - without strong convexity, gradient descent loses the geometric convergence rate and can only guarantee polynomial convergence.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0059.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0062.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0080.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0081.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 2.1.14 and Corollary 2.1.2 on page 81. The quote is extracted from visual verification of the PDF pages. The theorem provides convergence in terms of distance to optimum, while Corollary 2.1.2 provides the $O(1/k)$ sublinear rate in function values with optimal step size $h=1/L$. Page 62 provides Definition 2.1.2 for convex functions. Page 59 contains the beginning of Section 2.1.1 (Smooth Convex Functions) where the function class $\\mathscr{F}_L^{1,1}$ is introduced. Page 80 contains the beginning of Section 2.1.5 (The Gradient Method). The step size bound allows equality ($h \\leq 2/L$) which is a refinement from the 2004 edition's strict inequality.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "gd-smooth-descent-condition-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "81",
      "theorem": "Theorem 2.1.14",
      "claim": "With step size $\\alpha \\leq 2/L$, gradient descent guarantees that $f(w_{k+1}) < f(w_k)$ for smooth functions (Lipschitz continuous gradient with constant $L$).",
      "quote": "Let $f \\in \\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ and $0 < h \\leq 2/L$. Then the Gradient Method generates a sequence $\\{x_k\\}$, which converges to some optimal point $x^*$.",
      "notes": "Internal: Used in GdFixedTab to explain the sufficient condition for descent on smooth functions. This is the 2018 edition version using calligraphic notation $\\mathscr{F}_L^{1,1}$ instead of $F_L^{1,1}$ from the 2004 edition. The step size bound allows equality ($h \\leq 2/L$) instead of strict inequality ($h < 2/L$). The descent property follows from the fundamental inequality for smooth functions (see Lemma 1.2.3 on page 23 which shows $f(y) \\leq f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{L}{2}\\|y-x\\|^2$ for L-smooth functions), which combined with the gradient step yields descent when $h \\leq 2/L$.",
      "readerNotes": "The notation $\\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$ denotes convex functions with Lipschitz continuous gradient with constant $L$ (the function class is defined in Section 2.1.1, pages 59-69; see Definition 2.1.2 on page 62 for convex functions). However, the descent property itself follows from the upper bound inequality for smooth functions (Lemma 1.2.3 on page 23), which holds for any function with Lipschitz continuous gradient, not just convex functions. The condition $\\alpha \\leq 2/L$ ensures that each gradient descent step decreases the function value. Note: Nesterov uses $h$ for step size; here we use $\\alpha$. This is a more general result than convergence - it guarantees monotonic decrease at each step. The 2018 edition uses calligraphic script $\\mathscr{F}$ for function classes instead of the regular $F$ used in the 2004 edition, and allows equality in the step size bound ($h \\leq 2/L$) instead of strict inequality ($h < 2/L$).",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0059.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0060.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0062.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0080.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0081.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 2.1.14 on page 81 which states the gradient method convergence for smooth convex functions with step size bound $0 < h \\leq 2/L$. The theorem explicitly states convergence, and the descent property is implied by the proof which uses the fundamental inequality for L-smooth functions (Lemma 1.2.3 on page 23). Pages 59-60 provide context on Chapter 2 (Smooth Convex Optimization) structure. Page 62 contains Definition 2.1.2 for convex functions. Pages 80-81 contain Section 2.1.5 'The Gradient Method' where Theorem 2.1.14 appears. The step size condition $h \\leq 2/L$ (allowing equality) is explicitly stated in the theorem, which is a refinement from the 2004 edition's strict inequality $h < 2/L$.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "gd-descent-lemma-quadratic-upper-bound-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "23",
      "lemma": "Lemma 1.2.3",
      "claim": "The quadratic upper bound for L-smooth functions: any function with Lipschitz continuous gradient can be upper-bounded by a quadratic approximation",
      "quote": "Let $f \\in C_L^{1,1}(\\mathbb{R}^n)$. Then, for any $x, y$ from $\\mathbb{R}^n$, we have $|f(y) - f(x) - \\langle \\nabla f(x), y - x \\rangle| \\leq \\frac{L}{2} \\|y - x\\|^2$. Geometrically, we have the following picture. Consider a function $f \\in C_L^{1,1}(\\mathbb{R}^n)$. Let us fix a point $x_0 \\in \\mathbb{R}^n$, and define two quadratic functions $\\phi_1(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle - \\frac{L}{2} \\|x - x_0\\|^2$, $\\phi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2} \\|x - x_0\\|^2$. Then the graph of the function $f$ lies between the graphs of $\\phi_1$ and $\\phi_2$: $\\phi_1(x) \\leq f(x) \\leq \\phi_2(x)$, $\\forall x \\in \\mathbb{R}^n$.",
      "notes": "Internal: This is the fundamental descent lemma for L-smooth functions. The upper bound $f(y) \\leq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2}\\|y - x\\|^2$ is crucial for proving convergence of gradient descent. It shows that the function is upper-bounded by its first-order Taylor approximation plus a quadratic term. This result appears in Section 1.2.2 (Classes of Differentiable Functions) and is used throughout Chapter 2 for analyzing first-order methods. The book page number is 25, but the PDF page is 45.",
      "readerNotes": "The notation $C_L^{1,1}(\\mathbb{R}^n)$ denotes the class of functions with Lipschitz continuous gradient with constant $L$ (see page 24). This lemma is fundamental for analyzing gradient descent: it shows that any L-smooth function can be upper-bounded by a quadratic function. The upper bound $\\phi_2(x) = f(x_0) + \\langle \\nabla f(x_0), x - x_0 \\rangle + \\frac{L}{2}\\|x - x_0\\|^2$ is the quadratic upper bound used to prove that gradient descent decreases the function value at each iteration. When we take a gradient step $y = x - \\alpha \\nabla f(x)$, this bound guarantees descent when $\\alpha \\leq 2/L$.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0044.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0045.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0046.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Lemma 1.2.3 on pages 45-46 (book page 25-26). The lemma provides both a bound on the error of the linear approximation and the geometric interpretation showing the quadratic upper bound. The quote is extracted from visual verification of the PDF pages. Page 44 provides the definition of the function class $C_L^{1,1}(\\mathbb{R}^n)$ (functions with Lipschitz continuous gradient). The upper bound inequality $f(y) \\leq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2}\\|y - x\\|^2$ follows directly from the lemma by rearranging the absolute value inequality.",
      "usedIn": [
        "GdFixedTab"
      ]
    },
    "condition-number-definition-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "95-97",
      "theorem": "Definition following Theorem 2.1.12, and Theorem 2.1.11",
      "claim": "The condition number $Q = L/\\mu$ for strongly convex smooth functions is equivalent to the ratio of largest to smallest Hessian eigenvalues $\\lambda_{\\text{max}}/\\lambda_{\\text{min}}$",
      "quote": "One of the most important functional classes is $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ (recall that the corresponding norm is standard Euclidean). This class is described by the following inequalities: $\\langle \\nabla f(x) - \\nabla f(y), x - y \\rangle \\geq \\mu \\| x - y \\|^2$, $\\| \\nabla f(x) - \\nabla f(y) \\| \\leq L \\| x - y \\|$. The value $Q_f = L/\\mu \\geq 1$ is called the condition number of the function $f$.",
      "notes": "Internal: This citation documents the equivalence between the two definitions of condition number that appear in the codebase: (1) Q = L/\u03bc based on function properties, and (2) \u03ba = \u03bb_max/\u03bb_min based on Hessian eigenvalues. Theorem 2.1.11 on pages 95-96 establishes the connection: for twice continuously differentiable functions, f \u2208 S^{1,1}_{\u03bc,L} if and only if \u03bcI \u2aaf \u2207\u00b2f(x) \u2aaf LI for all x, which means the Hessian eigenvalues satisfy \u03bc \u2264 \u03bb_i \u2264 L, making \u03bb_min = \u03bc and \u03bb_max = L (for quadratic functions or locally near minima). The glossary entry for condition-number mentions both definitions and states they are equivalent.",
      "readerNotes": "For smooth strongly convex functions (class $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$), the condition number can be defined two equivalent ways: (1) $Q = L/\\mu$ where $L$ is the Lipschitz constant of the gradient and $\\mu$ is the strong convexity parameter, or (2) $\\kappa = \\lambda_{\\text{max}}/\\lambda_{\\text{min}}$ where these are the largest and smallest eigenvalues of the Hessian. These are equivalent because Theorem 2.1.11 (pages 95-96) shows that $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ if and only if $\\mu I \\preceq \\nabla^2 f(x) \\preceq L I$ for all $x$, meaning all Hessian eigenvalues satisfy $\\mu \\leq \\lambda_i \\leq L$. For quadratic functions $f(x) = \\frac{1}{2}x^T A x$, the Hessian is constant ($\\nabla^2 f = A$), so $\\mu = \\lambda_{\\text{min}}(A)$ and $L = \\lambda_{\\text{max}}(A)$ exactly. For general strongly convex functions, these bounds hold throughout the domain, establishing the equivalence.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0095.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0096.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0097.png"
      ],
      "verified": "2025-11-11",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified the definition of condition number Q_f = L/\u03bc on page 97 (following Theorem 2.1.12). Verified Theorem 2.1.11 on pages 95-96 which establishes the second-order characterization showing f \u2208 S^{2}_{\u03bc,L}(Q, \u2016\u00b7\u2016) if and only if \u2207\u00b2f(x)h, h \u2265 \u03bc\u2016h\u2016\u00b2 for all x \u2208 intQ and h \u2208 R^n. In the Euclidean norm case, this is equivalent to the matrix inequality \u2207\u00b2f(x) \u2ab0 \u03bcI (equation 2.1.28 on page 96). Combined with inequality (2.1.31) which gives \u2016\u2207f(x) - \u2207f(y)\u2016 \u2264 L\u2016x - y\u2016 (implying \u2207\u00b2f(x) \u2aaf LI), this establishes that \u03bc and L bound the Hessian eigenvalues, making Q_f = L/\u03bc equivalent to \u03bb_max/\u03bb_min for the Hessian.",
      "usedIn": [
        "glossary"
      ]
    },
    "gd-linesearch-convex-sublinear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "48-50, 81",
      "theorem": "Equation (1.2.20) with Corollary 2.1.2",
      "claim": "For convex, L-smooth functions, gradient descent with Armijo line search achieves sublinear convergence: $f(w_k) - f^* \\leq O(L\\|w_0 - w^*\\|^2/k)$. Line search automatically adapts the step size to achieve near-optimal constants without requiring knowledge of $L$",
      "quote": "The Armijo rule: Find $x_{k+1} = x_k - h\\nabla f(x_k)$ with $h > 0$ such that $\\alpha\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle \\leq f(x_k) - f(x_{k+1})$, $\\beta\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle \\geq f(x_k) - f(x_{k+1})$, where $0 < \\alpha < \\beta < 1$ are some fixed parameters. [...] Thus, we have proved that in all cases we have $f(x_k) - f(x_{k+1}) \\geq \\frac{\\omega}{L} \\|\\nabla f(x_k)\\|^2$, where $\\omega$ is some positive constant. [...] For the Armijo rule, $\\omega = \\frac{2\\alpha(1-\\beta)}{L}$",
      "notes": "Internal: Used in GdLineSearchTab to show that line search achieves the same O(1/k) convergence rate as fixed step size for convex smooth functions, but without requiring knowledge of L. The key insight from equation (1.2.20) is that all step size strategies (constant, full relaxation, Armijo) satisfy the same type of descent inequality: $f(x_k) - f(x_{k+1}) \\geq \\omega \\|\\nabla f(x_k)\\|^2$ for some positive constant $\\omega$. For Armijo rule with parameters $\\alpha, \\beta \\in (0,1)$, we get $h_k \\geq \\frac{2}{L}(1-\\beta)$ and thus $\\omega = \\frac{2\\alpha(1-\\beta)}{L}$. Applying the same argument as Corollary 2.1.2 (which uses the descent inequality for convex functions), this gives the O(1/k) rate. The advantage of line search is that it automatically finds a good step size without knowing L in advance, while achieving comparable convergence constants to the optimal fixed step size $h = 1/L$.",
      "readerNotes": "The Armijo line search rule (also called backtracking line search) finds a step size $h_k$ at each iteration that satisfies two conditions: sufficient decrease $f(x_k) - f(x_{k+1}) \\geq \\alpha h_k \\|\\nabla f(x_k)\\|^2$ and an upper bound $f(x_k) - f(x_{k+1}) \\leq \\beta h_k \\|\\nabla f(x_k)\\|^2$, where $0 < \\alpha < \\beta < 1$ are parameters (typically $\\alpha \\approx 0.3$, $\\beta \\approx 0.7$). Nesterov shows (pages 48-50) that for smooth functions ($f \\in C_L^{1,1}$), the Armijo rule guarantees a step size of at least $h_k \\geq \\frac{2}{L}(1-\\beta)$, yielding the descent inequality $f(x_k) - f(x_{k+1}) \\geq \\frac{2\\alpha(1-\\beta)}{L} \\|\\nabla f(x_k)\\|^2$. This is the same type of inequality as with fixed step size $h = \\frac{2\\alpha}{L}$ (equation on page 50), showing that line search achieves comparable per-iteration progress. For convex smooth functions, this descent inequality leads to $O(1/k)$ convergence by the same argument as Corollary 2.1.2: summing over iterations gives $f(x_k) - f^* \\leq \\frac{L\\|x_0-x^*\\|^2}{2\\omega(k+1)}$. The key advantage of line search is **automatic step size selection**: it adapts to the local smoothness without requiring prior knowledge of $L$, achieving near-optimal convergence constants in practice.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0048.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0049.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0050.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0081.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified the Armijo rule definition (equation 1.2.16-1.2.17) on page 48-49, the unified descent inequality (equation 1.2.20) on page 50 showing $f(x_k) - f(x_{k+1}) \\geq \\frac{\\omega}{L}\\|\\nabla f(x_k)\\|^2$ for all step size strategies including Armijo, and the derivation showing Armijo achieves $\\omega = \\frac{2\\alpha(1-\\beta)}{L}$. Also verified Corollary 2.1.2 on page 81 which converts the descent inequality to the O(1/k) convergence rate for convex smooth functions. The Armijo rule analysis shows that $h_k \\geq \\frac{2}{L}(1-\\beta)$ (bottom of page 50), and combining with the sufficient decrease condition (equation 1.2.16) yields the stated bound. The quote is extracted from visual verification of pages 48 and 50. Note: Nesterov uses $h$ for step size; the notation $\\alpha$ is used here to denote step size in our implementation.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "wolfe-conditions-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "33-36",
      "theorem": "Equations (3.6) and (3.7)",
      "claim": "The Wolfe conditions combine Armijo's sufficient decrease $f(x_k + \\alpha p_k) \\leq f(x_k) + c_1\\alpha\\nabla f_k^T p_k$ with a curvature condition $\\nabla f(x_k + \\alpha p_k)^T p_k \\geq c_2\\nabla f_k^T p_k$ (where $0 < c_1 < c_2 < 1$) to ensure steps are neither too small nor too large",
      "quote": "The sufficient decrease and curvature conditions are known collectively as the Wolfe conditions. We illustrate them in Figure 3.5 and restate them here for future reference: $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$, $\\nabla f(x_k + \\alpha_k p_k)^T p_k \\geq c_2 \\nabla f_k^T p_k$, with $0 < c_1 < c_2 < 1$. [...] The strong Wolfe conditions require $\\alpha_k$ to satisfy $f(x_k + \\alpha_k p_k) \\leq f(x_k) + c_1 \\alpha_k \\nabla f_k^T p_k$, $|\\nabla f(x_k + \\alpha_k p_k)^T p_k| \\leq c_2 |\\nabla f_k^T p_k|$, with $0 < c_1 < c_2 < 1$.",
      "notes": "Internal: This is for background context only in GdLineSearchTab. The key insight is that Armijo (sufficient decrease) alone is satisfied by arbitrarily small steps (see Figure 3.3 on page 53), so the curvature condition is needed to prevent tiny steps. The curvature condition ensures the slope at the accepted point is not too negative (not much room for further decrease). Strong Wolfe adds an absolute value to also exclude points far from stationary points. Typical values: c1=1e-4, c2=0.9 (Newton/quasi-Newton) or 0.1 (conjugate gradient). We implement Armijo backtracking only, but mention Wolfe briefly to explain why pure Armijo can be inefficient without backtracking from a reasonable initial step.",
      "readerNotes": "The Wolfe conditions consist of two parts: (1) The sufficient decrease (Armijo) condition $f(x_k + \\alpha p_k) \\leq f(x_k) + c_1\\alpha\\nabla f_k^T p_k$ ensures the step reduces the function value proportionally to the step size and directional derivative. However, this condition alone is satisfied by all sufficiently small steps (see Figure 3.3), which could lead to inefficiently tiny steps. (2) The curvature condition $\\nabla f(x_k + \\alpha p_k)^T p_k \\geq c_2\\nabla f_k^T p_k$ prevents arbitrarily small steps by requiring the slope at the accepted point to be at least $c_2$ times the initial slope. If the slope is still strongly negative ($\\ll c_2\\nabla f_k^T p_k$), we can reduce $f$ significantly by moving further, so the search continues. The strong Wolfe conditions use $|\\nabla f(x_k + \\alpha p_k)^T p_k| \\leq c_2 |\\nabla f_k^T p_k|$ to also exclude points with excessively positive slope, forcing steps to lie near stationary points of the line search function. Common parameter values are $c_1 = 10^{-4}$ and $c_2 = 0.9$ for Newton/quasi-Newton methods.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0052.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0053.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0054.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0055.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0056.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Wolfe conditions (3.6) on page 54 and strong Wolfe conditions (3.7) on page 54. The Armijo (sufficient decrease) condition is defined as equation (3.4) on page 53. The curvature condition is equation (3.5) on page 53. Figure 3.3 on page 53 illustrates how Armijo alone accepts arbitrarily small steps. Figure 3.4 on page 54 illustrates the curvature condition. Figure 3.5 on page 55 shows the combined Wolfe conditions. Page 52 provides context on why simple decrease is insufficient (Figure 3.2). The text explicitly states 'The sufficient decrease condition is not enough by itself to ensure that the algorithm makes reasonable progress because, as we see from Figure 3.3, it is satisfied for all sufficiently small values of \u03b1.' Lemma 3.1 (pages 55-56) proves that step lengths satisfying Wolfe conditions always exist for smooth functions bounded below.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "armijo-backtracking-termination-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "37, 56-57",
      "theorem": "Algorithm 3.1 (Backtracking Line Search)",
      "claim": "For L-smooth functions, Armijo backtracking with geometric step reduction $\\alpha \\leftarrow \\tau\\alpha$ (where $\\tau \\in (0,1)$) terminates in finite steps. The backtracking procedure will find an acceptable step length after a finite number of trials.",
      "quote": "Algorithm 3.1 (Backtracking Line Search). Choose $\\bar{\\alpha} > 0$, $\\rho \\in (0, 1)$, $c \\in (0, 1)$; Set $\\alpha \\leftarrow \\bar{\\alpha}$; repeat until $f(x_k + \\alpha p_k) \\leq f(x_k) + c\\alpha\\nabla f_k^T p_k$, $\\alpha \\leftarrow \\rho\\alpha$; end (repeat). Terminate with $\\alpha_k = \\alpha$. [...] An acceptable step length will be found after a finite number of trials, because $\\alpha_k$ will eventually become small enough that the sufficient decrease condition holds (see Figure 3.3).",
      "notes": "Internal: Used in GdLineSearchTab to explain backtracking line search termination. The book guarantees finite termination but does not provide an explicit bound on the number of backtracking iterations in terms of problem parameters. Page 37 contains the algorithm (PDF page 57), and page 37 contains the statement about finite termination. The sufficient decrease condition is also called the Armijo condition (page 33, PDF page 53). The notation: $\\bar{\\alpha}$ is initial step length (typically 1 for Newton/quasi-Newton), $\\rho$ is the contraction factor (reduction ratio), $c$ is the Armijo parameter (typically $10^{-4}$), $p_k$ is the search direction. The book uses $\\rho$ for the contraction factor; in the codebase this may be denoted $\\tau$.",
      "readerNotes": "The Armijo backtracking algorithm guarantees that a step length satisfying the sufficient decrease condition $f(x_k + \\alpha p_k) \\leq f(x_k) + c\\alpha\\nabla f_k^T p_k$ will be found in finitely many iterations. At each iteration, the step length is reduced by the factor $\\rho$ (i.e., $\\alpha \\leftarrow \\rho\\alpha$). The algorithm terminates because the step length eventually becomes small enough that the sufficient decrease condition is satisfied. For L-smooth functions (Lipschitz continuous gradient with constant $L$), this is guaranteed by Lemma 1.2.3 in Nesterov 2018 (page 45/25), which shows that $f(y) \\leq f(x) + \\langle\\nabla f(x), y-x\\rangle + \\frac{L}{2}\\|y-x\\|^2$. However, the standard references do not provide an explicit bound on the number of backtracking iterations in terms of $L$, $c$, or $\\rho$; they only guarantee finite termination. The typical choice is $c = 10^{-4}$ (page 33) and $\\rho \\in [\\rho_{lo}, \\rho_{hi}]$ for some fixed $0 < \\rho_{lo} < \\rho_{hi} < 1$ (page 37).",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0037.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0052.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0053.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0056.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0057.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Algorithm 3.1 (Backtracking Line Search) on page 37 (PDF page 57). Verified the statement about finite termination on page 37 (PDF page 57): 'An acceptable step length will be found after a finite number of trials, because \u03b1k will eventually become small enough that the sufficient decrease condition holds.' Also verified the Armijo condition definition on page 33 (PDF page 53): the sufficient decrease condition is referred to as 'the Armijo condition.' Page 32 (PDF page 52) shows Figure 3.3 illustrating the sufficient decrease condition. The book does not provide an explicit O(log(1/c)) bound on the number of backtracking iterations; it only guarantees finite termination. The contraction factor is denoted $\\rho$ in the algorithm statement.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "gd-linesearch-strongly-convex-linear-convergence-nesterov-2018": {
      "reference": "nesterov-2018",
      "pages": "53-55",
      "theorem": "Theorem 1.2.4",
      "claim": "For $\\mu$-strongly convex, $L$-smooth functions (locally, near a strict local minimum), gradient descent with Armijo line search or optimal step size $h^* = 2/(L+\\mu)$ achieves linear convergence: $\\|x_k - x^*\\|^2 \\leq \\rho^k \\|x_0 - x^*\\|^2$ where $\\rho = (1 - 2\\mu/(L+3\\mu))^2 < 1$ depends on the condition number $Q = L/\\mu$",
      "quote": "Theorem 1.2.4 Let the function $f(\\cdot)$ satisfy our assumptions and let the starting point $x_0$ be close enough to a strict local minimum $x^*$: $r_0 = \\|x_0 - x^*\\| < \\bar{r} = 2\\mu/M$. Then the Gradient Method with step size $h^*_k = 2/(L+\\mu)$ converges as follows: $\\|x_k - x^*\\| \\leq \\frac{\\bar{r}r_0}{\\bar{r}-r_0}\\left(1 - \\frac{2\\mu}{L+3\\mu}\\right)^k$. This type of rate of convergence is called linear.",
      "notes": "Internal: This is a LOCAL convergence result for gradient descent near a strict local minimum, not a global result. The assumptions are: (1) $f \\in C_M^{2,2}(\\mathbb{R}^n)$ (twice differentiable with Lipschitz continuous Hessian), (2) $x^*$ is a local minimum with positive definite Hessian bounded by $\\mu I \\preceq \\nabla^2 f(x^*) \\preceq LI$, and (3) starting point $x_0$ is close enough to $x^*$. The theorem shows linear convergence with the optimal step size $h^* = 2/(L+\\mu)$. The Armijo rule (pages 48-50, equations 1.2.16-1.2.17) is discussed as a practical line search strategy that guarantees sufficient decrease and is shown to give $f(x_k) - f(x_{k+1}) \\geq \\frac{\\alpha(1-\\beta)}{2L}\\|\\nabla f(x_k)\\|^2$ for parameters $0 < \\alpha < \\beta < 1$. This differs from the global results in Chapter 2 (Theorems 2.1.14 and 2.1.15) which apply to globally convex/strongly convex functions. Used in GdLineSearchTab.",
      "readerNotes": "This theorem establishes linear convergence for gradient descent with line search in a LOCAL neighborhood of a strict minimum, not globally. The assumptions require: (1) twice differentiable function with Lipschitz continuous Hessian (constant $M$), (2) a strict local minimum $x^*$ where the Hessian satisfies $\\mu I \\preceq \\nabla^2 f(x^*) \\preceq LI$, and (3) initial point sufficiently close to $x^*$ (within radius $\\bar{r} = 2\\mu/M$). The convergence rate $\\rho = 1 - 2\\mu/(L+3\\mu)$ depends on the condition number $Q = L/\\mu$. The Armijo rule (equations 1.2.16-1.2.17, pages 48-50) is a practical line search that finds step size $h > 0$ satisfying $\\alpha\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle \\leq f(x_k) - f(x_{k+1}) \\leq \\beta\\langle\\nabla f(x_k), x_k - x_{k+1}\\rangle$ for parameters $0 < \\alpha < \\beta < 1$, ensuring sufficient decrease. Note: Nesterov uses $h$ for step size; here we use $\\alpha$. This LOCAL result complements the GLOBAL results in Chapter 2: Theorem 2.1.15 (pages 101-102) for globally strongly convex functions requires the function to be strongly convex everywhere, not just near a minimum.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0048.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0049.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0050.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0053.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0054.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0055.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 1.2.4 on pages 53-55 showing local linear convergence for gradient descent near a strict local minimum with step size $h^* = 2/(L+\\mu)$. The convergence rate is $\\|x_k - x^*\\| \\leq C(1 - 2\\mu/(L+3\\mu))^k$ where $C = \\bar{r}r_0/(\\bar{r}-r_0)$. Also verified the Armijo rule definition on pages 48-50 (equations 1.2.16-1.2.17) and the analysis showing that Armijo line search ensures decrease $f(x_k) - f(x_{k+1}) \\geq \\omega/L \\|\\nabla f(x_k)\\|^2$ for constant $\\omega = \\alpha(1-\\beta)/2$ (page 50, equation 1.2.20). This is a LOCAL convergence result requiring the initial point to be close to a local minimum where the Hessian is positive definite, distinct from the GLOBAL strongly convex results in Chapter 2.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "gd-linesearch-strongly-convex-linear-convergence-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "62-64",
      "theorem": "Theorem 3.3 and Theorem 3.4",
      "claim": "For strongly convex quadratic functions and general smooth strongly convex functions, steepest descent with exact line search achieves linear convergence with rate determined by the condition number: $\\|x_{k+1} - x^*\\|_Q^2 \\leq \\left(\\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\right)^2 \\|x_k - x^*\\|_Q^2$ where $\\kappa(Q) = \\lambda_n/\\lambda_1$ is the condition number",
      "quote": "Theorem 3.3. When the steepest descent method with exact line searches is applied to the strongly convex quadratic function $f(x) = \\frac{1}{2}x^T Q x - b^T x$, the error norm satisfies $\\|x_{k+1} - x^*\\|_Q^2 \\leq \\left(\\frac{\\lambda_n - \\lambda_1}{\\lambda_n + \\lambda_1}\\right)^2 \\|x_k - x^*\\|_Q^2$, where $0 < \\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_n$ are the eigenvalues of $Q$. [...] Theorem 3.4. Suppose that $f : \\mathbb{R}^n \\to \\mathbb{R}$ is twice continuously differentiable, and that the iterates generated by the steepest-descent method with exact line searches converge to a point $x^*$ at which the Hessian matrix $\\nabla^2 f(x^*)$ is positive definite. Let $r$ be any scalar satisfying $r \\in [(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1), 1]$, where $\\lambda_1 \\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_n$ are the eigenvalues of $\\nabla^2 f(x^*)$. Then for all $k$ sufficiently large, we have $f(x_{k+1}) - f(x^*) \\leq r^2[f(x_k) - f(x^*)]$.",
      "notes": "Internal: These theorems establish linear convergence for steepest descent (gradient descent) with exact line search on strongly convex functions. Theorem 3.3 gives the exact rate for quadratic functions, while Theorem 3.4 extends to general nonlinear functions (asymptotically, near the solution). The convergence rate depends on the condition number $\\kappa = \\lambda_n/\\lambda_1 = L/\\mu$. The rate $(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1) = (L-\\mu)/(L+\\mu) = (Q-1)/(Q+1)$ where $Q = L/\\mu$ is the condition number. This matches the result in Nesterov 2018 Theorem 2.1.15 for globally strongly convex functions. The exact line search minimizes $f(x_k - \\alpha \\nabla f(x_k))$ over $\\alpha > 0$. Used in GdLineSearchTab.",
      "readerNotes": "These results show that steepest descent with exact line search achieves linear convergence on strongly convex functions, with the convergence rate determined by the condition number $\\kappa = \\lambda_n/\\lambda_1$. For quadratic functions $f(x) = \\frac{1}{2}x^T Q x - b^T x$, Theorem 3.3 gives the exact rate $(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1)$. For general smooth functions, Theorem 3.4 shows that near a solution $x^*$ where $\\nabla^2 f(x^*)$ is positive definite, the method achieves the same asymptotic rate based on the eigenvalues of the Hessian at $x^*$. The exact line search finds the step length $\\alpha_k$ that minimizes $f(x_k - \\alpha \\nabla f(x_k))$, given by $\\alpha_k = \\nabla f_k^T \\nabla f_k / (\\nabla f_k^T Q \\nabla f_k)$ for quadratic functions (equation 3.25, page 62). The convergence rate degrades as the condition number increases: when $\\kappa$ is large, the rate approaches 1 and convergence becomes very slow. The weighted norm $\\|x\\|_Q^2 = x^T Q x$ measures optimality gap: $\\frac{1}{2}\\|x - x^*\\|_Q^2 = f(x) - f(x^*)$ (equation 3.27).",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0062.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0063.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0064.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 3.3 on pages 62-63 for strongly convex quadratic functions and Theorem 3.4 on page 64 for general smooth functions. Both theorems establish linear convergence with rate determined by condition number. The exact line search formula (equation 3.25) and the weighted norm definition (equation 3.27) are on page 62. Theorem 3.3 provides the exact convergence rate for quadratics, while Theorem 3.4 extends to nonlinear functions asymptotically. The rate $(\\lambda_n - \\lambda_1)/(\\lambda_n + \\lambda_1)$ can be rewritten as $(Q-1)/(Q+1)$ where $Q = \\lambda_n/\\lambda_1 = L/\\mu$ is the condition number, matching Nesterov's results.",
      "usedIn": [
        "GdLineSearchTab"
      ]
    },
    "bfgs-superlinear-convergence-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "177-180",
      "theorem": "Theorem 6.6",
      "claim": "BFGS (not L-BFGS) achieves superlinear convergence on strongly convex functions. L-BFGS achieves only linear convergence due to limited memory preventing full Hessian approximation",
      "quote": "Theorem 6.6. Suppose that $f$ is twice continuously differentiable and that the iterates generated by the BFGS algorithm converge to a minimizer $x^*$ at which Assumption 6.2 holds. Suppose also that (6.52) holds. Then $x_k$ converges to $x^*$ at a superlinear rate.",
      "notes": "Internal: CRITICAL DISTINCTION - This theorem applies to full BFGS, NOT L-BFGS. Page 196 explicitly states that L-BFGS yields 'an acceptable (albeit linear) rate of convergence.' The limited memory in L-BFGS prevents the Hessian approximation from fully converging to the true Hessian, degrading convergence from superlinear to linear. Used in LbfgsTab with this important caveat.",
      "readerNotes": "**Important:** This theorem establishes superlinear convergence for **full BFGS**, not L-BFGS. Nocedal & Wright (page 196) explicitly state that L-BFGS yields 'an acceptable (albeit **linear**) rate of convergence.' The distinction arises because L-BFGS uses limited memory (only the $M$ most recent correction pairs), preventing the Hessian approximation $B_k$ from fully converging to the true Hessian $\\nabla^2 f(w^*)$. For L-BFGS in practice, the linear convergence rate is still effective, and the method's low memory requirements make it practical for large-scale problems where full BFGS would be infeasible.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0173.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0174.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0175.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0176.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0177.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0178.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0179.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0180.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 6.6 on pages 177-180 which establishes superlinear convergence for BFGS (not L-BFGS). Verified page 196 which explicitly states that L-BFGS yields 'an acceptable (albeit linear) rate of convergence' - this is NOT superlinear. The limited memory in L-BFGS prevents the Hessian approximation from converging to the true Hessian, degrading convergence from superlinear to linear.",
      "usedIn": [
        "LbfgsTab"
      ]
    },
    "bfgs-update-formula-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "136-140",
      "theorem": "Equation (6.19)",
      "claim": "The BFGS update formula maintains positive definiteness and satisfies the secant equation $B_{k+1} s_k = y_k$",
      "quote": "The update formula for $B_k$ is obtained by simply applying the Sherman–Morrison–Woodbury formula (A.28) to (6.17) to obtain $B_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$ where $s_k = x_{k+1} - x_k = \\alpha_k p_k$ and $y_k = \\nabla f_{k+1} - \\nabla f_k$.",
      "notes": "Internal: This is the BFGS update formula for the Hessian approximation $B_k$. The formula can be rewritten in the symmetric form: $B_{k+1} = (I - \\rho_k s_k y_k^T) B_k (I - \\rho_k y_k s_k^T) + \\rho_k s_k s_k^T$ where $\\rho_k = 1/(s_k^T y_k)$ (equation 6.14). Used in LbfgsTab to explain the BFGS/L-BFGS update mechanism.",
      "readerNotes": "The BFGS formula updates the Hessian approximation $B_k$ using information from the most recent step. The notation: $s_k = x_{k+1} - x_k$ is the parameter change (step), $y_k = \\nabla f_{k+1} - \\nabla f_k$ is the gradient change, and $\\rho_k = 1/(s_k^T y_k)$ is the curvature scaling factor (equation 6.14). The secant equation $B_{k+1} s_k = y_k$ (equation 6.6, page 138) requires that the updated approximation maps the step to the gradient change, mimicking the property of the true Hessian. The formula maintains positive definiteness when the curvature condition $s_k^T y_k > 0$ holds (equation 6.7, page 138).",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0136.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0137.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0138.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0139.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0140.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified the BFGS update formula (equation 6.19) on page 140 and the equivalent inverse Hessian form (equation 6.17) on page 140. The secant equation $B_{k+1} s_k = y_k$ is defined on page 138 (equation 6.6), and the curvature condition $s_k^T y_k > 0$ is on page 138 (equation 6.7).",
      "usedIn": ["LbfgsTab"]
    },
    "lbfgs-computational-complexity-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "177-178, 197-198",
      "algorithm": "Algorithm 7.4 (L-BFGS two-loop recursion)",
      "claim": "L-BFGS requires $O(Md)$ memory and $O(Md)$ time per iteration, where $M$ is the memory size (number of stored curvature pairs) and $d$ is the dimension",
      "quote": "Without considering the multiplication $H_k^0 q$, the two-loop recursion scheme requires $4mn$ multiplications; if $H_k^0$ is diagonal, then $n$ additional multiplications are needed. [...] To circumvent this problem, we store a modified version of $H_k$ implicitly, by storing a certain number (say, $m$) of the vector pairs $\\{s_i, y_i\\}$ used in the formulas (7.16)–(7.18). The product $H_k \\nabla f_k$ can be obtained by performing a sequence of inner products and vector summations involving $\\nabla f_k$ and the pairs $\\{s_i, y_i\\}$.",
      "notes": "Internal: The complexity analysis appears on page 178 following Algorithm 7.4. The key observation is that L-BFGS stores $m$ vector pairs $\\{s_i, y_i\\}$, each of length $n$ (dimension $d$ in our notation), requiring $O(mn) = O(Md)$ storage. The two-loop recursion performs $4mn$ scalar multiplications plus $n$ for the initial Hessian approximation, giving $O(mn) = O(Md)$ time complexity per iteration.",
      "readerNotes": "L-BFGS achieves its efficiency through compact storage: instead of storing a dense $d \\times d$ approximate Hessian (requiring $O(d^2)$ memory), it stores only $M$ vector pairs $\\{s_k, y_k\\}$ of dimension $d$, requiring $O(Md)$ memory. The two-loop recursion (Algorithm 7.4) computes the search direction efficiently using these stored pairs. The algorithm performs two passes through the $M$ stored pairs, with each loop iteration performing $O(d)$ operations (inner products and vector additions), yielding $O(Md)$ total time per optimization iteration. The quote states that the recursion requires $4mn$ scalar multiplications (where $m = M$ and $n = d$), plus $n$ more if the initial approximation $H_k^0$ is diagonal.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0177.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0178.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0197.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0198.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Algorithm 7.4 (L-BFGS two-loop recursion) on page 178 which explicitly states '4mn multiplications' for the two-loop recursion. Verified the introduction to L-BFGS on pages 177-178 explaining the storage of $m$ vector pairs $\\{s_i, y_i\\}$.",
      "usedIn": [
        "LbfgsTab"
      ]
    },
    "bfgs-positive-definiteness-preservation-nocedal-wright-2006": {
      "reference": "nocedal-wright-2006",
      "pages": "136-140, 156-161",
      "theorem": "Section 6.1, equations (6.7), (6.8)",
      "claim": "BFGS/L-BFGS maintains positive definiteness of the approximate Hessian by only accepting curvature pairs where $s_k^T y_k > 0$ (positive curvature condition). If $H_k$ is positive definite and $s_k^T y_k > 0$, then $H_{k+1}$ computed by the BFGS update is positive definite. This makes BFGS more robust than Newton's method in non-convex regions where the true Hessian may have negative eigenvalues",
      "quote": "Note that the minimization problem (6.16) that gives rise to the BFGS update formula does not explicitly require the updated Hessian approximation to be positive definite. It is easy to show, however, that $H_{k+1}$ will be positive definite whenever $H_k$ is positive definite, by using the following argument. First, note from (6.8) that $y_k^T s_k$ is positive, so that the updating formula (6.17), (6.14) is well-defined. For any nonzero vector $z$, we have $z^T H_{k+1} z = w^T H_k w + \\rho_k (z^T s_k)^2 \\geq 0$, where we have defined $w = z - \\rho_k y_k (s_k^T z)$. The right hand side can be zero only if $s_k^T z = 0$, but in this case $w = z \\neq 0$, which implies that the first term is greater than zero. Therefore, $H_{k+1}$ is positive definite.",
      "notes": "Internal: This citation establishes the key robustness property of BFGS - it maintains positive definiteness through curvature filtering. The curvature condition $s_k^T y_k > 0$ (equation 6.7) is guaranteed by the Wolfe line search conditions as shown in equation (6.8). The proof on page 161 shows that if $H_k$ is positive definite, then $H_{k+1}$ from BFGS update is also positive definite. Used in LbfgsTab to explain why BFGS is robust in non-convex settings.",
      "readerNotes": "The BFGS method maintains positive definiteness of its Hessian approximation through a curvature filtering mechanism. The key is the **curvature condition** $s_k^T y_k > 0$ (equation 6.7, page 157), where $s_k = x_{k+1} - x_k$ is the step and $y_k = \\nabla f_{k+1} - \\nabla f_k$ is the change in gradients. This condition is automatically satisfied when using the Wolfe line search conditions. The BFGS update formula has the remarkable property that **if $H_k$ is positive definite and $s_k^T y_k > 0$, then $H_{k+1}$ is also positive definite** (proof on page 161). This property makes BFGS significantly more robust than Newton's method in non-convex regions: Newton's method uses the true Hessian $\\nabla^2 f(x_k)$, which can have negative eigenvalues when the function is non-convex locally.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0136.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0137.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0138.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0139.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0140.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0156.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0157.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0158.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0159.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0160.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0161.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified the BFGS positive definiteness preservation property across pages 156-161 of Nocedal & Wright 2006. Page 161 contains the key proof that $H_{k+1}$ is positive definite whenever $H_k$ is positive definite and $y_k^T s_k > 0$. This establishes that BFGS maintains positive definiteness through curvature filtering.",
      "usedIn": ["LbfgsTab"]
    },
    "newton-quadratic-convergence": {
      "reference": "nocedal-wright-2006",
      "pages": "64-65",
      "theorem": "Theorem 3.5",
      "claim": "Newton's method achieves quadratic convergence on strongly convex functions with Lipschitz continuous Hessian, when starting close enough to the optimum",
      "quote": "Suppose that $f$ is twice differentiable and that the Hessian $\\nabla^2 f(x)$ is Lipschitz continuous in a neighborhood of a solution $x^*$ at which the sufficient conditions (Theorem 2.4) are satisfied. Consider the iteration $x_{k+1} = x_k + p_k$, where $p_k$ is given by (3.30). Then (i) if the starting point $x_0$ is sufficiently close to $x^*$, the sequence of iterates converges to $x^*$; (ii) the rate of convergence of $\\{x_k\\}$ is quadratic; and (iii) the sequence of gradient norms $\\{\\|\\nabla f_k\\|\\}$ converges quadratically to zero.",
      "notes": "Internal: Theorem 3.5 establishes quadratic convergence for Newton's method under the key conditions: (1) twice differentiable function, (2) Lipschitz continuous Hessian near the solution, (3) second-order sufficient conditions satisfied at $x^*$ (i.e., $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*)$ positive definite), and (4) starting point sufficiently close to the solution. The proof shows $\\|x_{k+1} - x^*\\| \\leq \\tilde{L}\\|x_k - x^*\\|^2$ where $\\tilde{L} = L\\|\\nabla^2 f(x^*)^{-1}\\|$ and $L$ is the Lipschitz constant of the Hessian. Equation (3.30) refers to the Newton step $p_k^N = -(\\nabla^2 f_k)^{-1}\\nabla f_k$. Theorem 2.4 refers to second-order sufficient conditions for a local minimizer. Used in NewtonTab to explain the theoretical convergence rate.",
      "readerNotes": "Newton's method achieves quadratic convergence under three key conditions: (1) The Hessian $\\nabla^2 f(x)$ must be Lipschitz continuous near the solution $x^*$, meaning $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$ for some constant $L > 0$. This is stronger than just having a continuous Hessian. (2) The solution $x^*$ must satisfy second-order sufficient conditions: $\\nabla f(x^*) = 0$ (first-order optimality) and $\\nabla^2 f(x^*)$ is positive definite (second-order optimality). This means $x^*$ is a strict local minimum where the Hessian has all positive eigenvalues. (3) The starting point $x_0$ must be sufficiently close to $x^*$. The convergence rate is $\\|x_{k+1} - x^*\\| \\leq \\tilde{L}\\|x_k - x^*\\|^2$ where $\\tilde{L} = L\\|\\nabla^2 f(x^*)^{-1}\\|$, demonstrating that the error is squared at each iteration. This quadratic convergence means the number of correct digits roughly doubles at each step once the method is close enough to the solution. The Lipschitz continuous Hessian condition is crucial: it ensures the Hessian doesn't change too rapidly, allowing the quadratic model to be accurate. For strongly convex functions (where $\\mu I \\preceq \\nabla^2 f(x) \\preceq LI$ globally), these conditions are satisfied automatically once we're in the basin of attraction.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0064.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0065.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Theorem 3.5 on pages 64-65 of Nocedal & Wright (2006). The theorem states that Newton's method $x_{k+1} = x_k + p_k$ where $p_k = -(\\nabla^2 f_k)^{-1}\\nabla f_k$ achieves quadratic convergence when: (1) $f$ is twice differentiable, (2) Hessian $\\nabla^2 f(x)$ is Lipschitz continuous near $x^*$, (3) $x^*$ satisfies second-order sufficient conditions (Theorem 2.4), and (4) starting point $x_0$ is sufficiently close to $x^*$. The proof (equation 3.33) shows $\\|x_k + p_k^N - x^*\\| \\leq L\\|\\nabla^2 f(x^*)^{-1}\\|\\|x_k - x^*\\|^2$, establishing quadratic convergence. The Lipschitz continuous Hessian condition means $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$ for some $L > 0$ (see Appendix A.42). Extracted and visually verified pages 64-65 to confirm exact mathematical notation and conditions.",
      "usedIn": ["NewtonTab"]
    },
    "newton-convex-convergence": {
      "reference": "nesterov-2018",
      "pages": "262-277",
      "theorem": "Theorem 4.1.2, Theorem 4.1.6, and Example 4.1.1",
      "claim": "For convex functions with Lipschitz continuous Hessian, Newton's method (with cubic regularization) converges to the global minimum. Specifically, the method finds stationary points satisfying $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*) \\succeq 0$, which for convex functions are global minima.",
      "quote": "Theorem 4.1.2: Let the sequence $\\{x_i\\}$ be generated by method (4.1.16). Let us assume that for some $i \\geq 0$ the set $\\mathscr{L}(f(x_i))$ is bounded. Then there exists a limit $\\lim_{i\\to\\infty} f(x_i) = f^*$. The set $X^*$ of limit points of this sequence is non-empty. Moreover, this is a connected set such that for any $x^* \\in X^*$ we have $f(x^*) = f^*$, $\\nabla f(x^*) = 0$, $\\nabla^2 f(x^*) \\succeq 0$. [...] Example 4.1.1 (Convex Functions): Let $f$ be convex on $\\mathbb{R}^n$. Assume it achieves its minimum at point $x^*$. Then, for any $x \\in \\mathbb{R}^n$ with $\\|x - x^*\\| < R$, we have $f(x) - f(x^*) \\leq \\langle\\nabla f(x), x - x^*\\rangle \\leq \\|\\nabla f(x)\\| \\cdot R$. Thus, the function $f$ is a gradient dominated function of degree one. [...] Theorem 4.1.6: If $f(x_0) - f(x^*) \\leq \\gamma^2 \\hat{\\omega}$, then $f(x_k) - f(x^*) \\leq \\hat{\\omega} \\cdot \\frac{\\gamma^2}{(2 + k + \\frac{3}{2\\gamma})^2}$.",
      "notes": "Internal: This citation addresses Newton's method convergence on convex functions with Lipschitz continuous Hessian. The key insight is that convex functions are gradient-dominated of degree 1 (Example 4.1.1), and the Cubic Regularization of Newton's Method converges for such functions. Assumption 4.1.1 (page 262) requires Lipschitz continuous Hessian: $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$. For convex functions, stationary points with positive semidefinite Hessian are global minima (Theorem 1.2.2 on page 33, combined with convexity). The convergence rate for gradient-dominated functions of degree 1 is $O(1/k^2)$ (Theorem 4.1.6, equation 4.1.36). This is the cubic regularized Newton method (4.1.16), not the classical Newton method, but it handles the case where Hessian may not be positive definite everywhere. Used in NewtonTab.",
      "readerNotes": "Newton's method convergence on convex functions requires careful treatment because the Hessian may not be positive definite everywhere (only at the minimum). Nesterov's Cubic Regularization of Newton's Method (Algorithm 4.1.16) addresses this by using cubic regularization: at each iteration, minimize $\\nabla f(x_k)^T(y - x_k) + \\frac{1}{2}(y - x_k)^T\\nabla^2 f(x_k)(y - x_k) + \\frac{M}{6}\\|y - x_k\\|^3$ where $M \\geq L$ is the Lipschitz constant of the Hessian. This regularization ensures the subproblem is always well-defined. For convex functions achieving their minimum at $x^*$, the convergence guarantee follows from two results: (1) Example 4.1.1 shows that convex functions are gradient-dominated of degree 1, meaning $f(x) - f(x^*) \\leq \\|\\nabla f(x)\\| \\cdot R$ for $\\|x - x^*\\| < R$. (2) Theorem 4.1.6 establishes that for gradient-dominated functions of degree 1, the method achieves $f(x_k) - f(x^*) \\leq O(1/k^2)$ convergence. (3) Theorem 4.1.2 guarantees that limit points satisfy $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*) \\succeq 0$, which for convex functions are precisely the global minima. The requirement of Lipschitz continuous Hessian (Assumption 4.1.1: $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$) is essential for the analysis and is satisfied by many practical functions including twice-differentiable convex functions with bounded Hessians.",
      "proofPages": [
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0262.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0263.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0264.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0265.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0266.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0267.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0271.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0274.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0275.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0276.png",
        "docs/references/extracted-pages/lectures_on_convex_optimization_page_0277.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified the complete convergence theory for Newton's method on convex functions with Lipschitz continuous Hessian in Nesterov 2018, Chapter 4. Assumption 4.1.1 (page 262) establishes the Lipschitz continuity requirement for the Hessian. Theorem 4.1.2 (pages 266-267) proves convergence to stationary points with positive semidefinite Hessian. Example 4.1.1 (pages 274-275) shows convex functions are gradient-dominated of degree 1. Theorem 4.1.6 (pages 276-277) establishes $O(1/k^2)$ convergence rate for gradient-dominated functions of degree 1. Combined with convexity (which ensures stationary points with $\\nabla^2 f(x^*) \\succeq 0$ are global minima), these results establish convergence to the global minimum. The method uses cubic regularization (equation 4.1.5 on page 262) to handle potentially indefinite Hessians during iteration. All quotes verified visually from the extracted PDF pages.",
      "usedIn": ["NewtonTab"]
    },
    "newton-computational-complexity": {
      "reference": "nocedal-wright-2006",
      "pages": "64, 627-628",
      "theorem": "Section 3.3 (Newton's Method) and Appendix A.1 (Cholesky/LU Factorization)",
      "claim": "Solving the Newton system $H \\cdot p = -\\nabla f$ requires $O(d^3)$ operations using direct methods (Cholesky or LU decomposition)",
      "quote": "Newton's method: $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$ [...] The factorization (A.20) can be found by using Gaussian elimination with row partial pivoting, an algorithm that requires approximately $2n^3/3$ floating-point operations when $A$ is dense. [...] When $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, it is possible to compute a similar but more specialized factorization at about half the cost—about $n^3/3$ operations. This factorization, known as the Cholesky factorization, produces a matrix $L$ such that $A = LL^T$.",
      "notes": "Internal: Used in NewtonTab to explain computational cost of Newton's method. Page 64 defines Newton's method as computing the search direction $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$, which requires solving the linear system $\\nabla^2 f_k \\cdot p_k = -\\nabla f_k$. Pages 627-628 (Appendix A.1) document the computational complexity of solving dense linear systems: LU decomposition via Gaussian elimination requires approximately $2n^3/3$ operations (equation A.20, Algorithm A.1), while Cholesky factorization for symmetric positive definite matrices requires about $n^3/3$ operations (equation A.23, Algorithm A.2). Both are $O(n^3)$ methods. The Newton system has the Hessian matrix as the coefficient matrix, so solving it requires $O(d^3)$ operations where $d$ is the problem dimension (number of variables). In the book, $n$ is used for dimension; we use $d$ in the codebase.",
      "readerNotes": "Newton's method computes the search direction by solving the linear system $H \\cdot p = -\\nabla f$ where $H = \\nabla^2 f$ is the Hessian matrix (see equation 3.30 on page 64: $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$). This requires solving a $d \\times d$ linear system where $d$ is the number of optimization variables. For dense matrices, direct solution methods require $O(d^3)$ operations: **Gaussian elimination with LU decomposition** requires approximately $2d^3/3$ floating-point operations (page 627, Algorithm A.1), while **Cholesky factorization** (applicable when the Hessian is positive definite) requires about $d^3/3$ operations (page 628, Algorithm A.2). Both methods scale cubically with the problem dimension. In practice, once the factorization is computed, solving the system via forward- and back-substitution requires only $O(d^2)$ operations, so the dominant cost is the $O(d^3)$ factorization step. For large-scale problems, this cubic cost motivates the use of quasi-Newton methods (like BFGS or L-BFGS) which avoid computing and factorizing the full Hessian.",
      "proofPages": [
        "docs/references/extracted-pages/numericaloptimization2006_page_0064.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0627.png",
        "docs/references/extracted-pages/numericaloptimization2006_page_0628.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified Newton's method definition on page 64 (equation 3.30) showing $p_k^N = -\\nabla^2 f_k^{-1} \\nabla f_k$, which is equivalent to solving $\\nabla^2 f_k \\cdot p_k = -\\nabla f_k$. Verified computational complexity on pages 627-628: Gaussian elimination with LU decomposition (Algorithm A.1) requires approximately $2n^3/3$ operations for dense $n \\times n$ matrices, and Cholesky factorization (Algorithm A.2) requires about $n^3/3$ operations for symmetric positive definite matrices. Both are $O(n^3)$ complexity. The quote combines the Newton method definition with the factorization complexity statements. Note: The book uses $n$ for dimension; in our codebase we use $d$ for the dimension (number of variables).",
      "usedIn": ["NewtonTab"]
    },
    "lbfgs-linear-convergence-liu-nocedal-1989": {
      "reference": "liu-nocedal-1989",
      "pages": "21-24",
      "theorem": "Theorem 6.1",
      "claim": "L-BFGS converges R-linearly (not superlinearly) on uniformly convex problems. The rate of convergence is linear, not superlinear. With sufficient memory M, L-BFGS approaches BFGS behavior but does not achieve superlinear convergence.",
      "quote": "In this section we show that the limited memory BFGS method is globally convergent on uniformly convex problems, and that its rate of convergence is R-linear... Then for any positive definite $B_0$, Algorithm 6.1 generates a sequence $\\{x_k\\}$ which converges to $x^*$. Moreover there is a constant $0 \\leq r < 1$ such that $f_k - f^* \\leq r^k[f_0 - f^*]$ which implies that $\\{x_k\\}$ converges R-linearly... R-linear convergence is the best we can expect.",
      "notes": "Internal: CRITICAL CORRECTION - This citation establishes that L-BFGS has LINEAR convergence, NOT superlinear. The paper explicitly states 'R-linear convergence is the best we can expect' (p. 23). This is in contrast to full BFGS, which can achieve superlinear convergence. Nocedal & Wright (2006) p. 196 also confirms L-BFGS has 'acceptable (albeit linear) rate of convergence.' The memory parameter M affects the constant in the linear convergence rate, not the order of convergence. Used in AlgorithmExplainer to correct the incorrect superlinear convergence claim.",
      "readerNotes": "**IMPORTANT:** L-BFGS achieves only **linear convergence**, not superlinear. Liu & Nocedal (1989) Theorem 6.1 proves that L-BFGS has R-linear convergence, and the paper explicitly states 'R-linear convergence is the best we can expect' (p. 23). This is fundamentally different from full BFGS, which can achieve superlinear convergence (Nocedal & Wright 2006, Theorem 6.6). The limited memory in L-BFGS (storing only $M$ recent curvature pairs) prevents the Hessian approximation from fully converging to the true Hessian, which limits the convergence rate to linear. The memory parameter $M$ affects the constant in the linear convergence rate but does NOT change the order of convergence to superlinear. Nocedal & Wright (2006) p. 196 confirms: L-BFGS yields 'an acceptable (albeit linear) rate of convergence.' Despite being 'only' linear, L-BFGS's convergence is still very effective in practice due to its low memory requirements ($O(Md)$ vs $O(d^2)$ for full BFGS) which enable its use on large-scale problems where full BFGS would be infeasible.",
      "proofPages": [
        "docs/references/extracted-pages/liunocedal1989_page_0021.png",
        "docs/references/extracted-pages/liunocedal1989_page_0022.png",
        "docs/references/extracted-pages/liunocedal1989_page_0023.png",
        "docs/references/extracted-pages/liunocedal1989_page_0024.png"
      ],
      "verified": "2025-11-12",
      "verifiedBy": "claude-code-agent",
      "verificationNotes": "Verified by reading Liu & Nocedal (1989) Section 7 'Convergence Analysis' which contains Theorem 6.1 proving R-linear convergence. Cross-referenced with Nocedal & Wright (2006) p. 196 which states L-BFGS yields 'an acceptable (albeit linear) rate of convergence.' The claim of superlinear convergence is false and contradicts the theoretical results. This citation corrects a common misconception about L-BFGS convergence rate.",
      "usedIn": ["AlgorithmExplainer"]
    }
  }
}