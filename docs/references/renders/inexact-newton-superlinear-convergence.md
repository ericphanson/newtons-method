# inexact-newton-superlinear-convergence

## Reference

Jorge Nocedal and Stephen J. Wright. *Numerical Optimization* (2nd edition). Springer, 2006.

**File:** `NumericalOptimization2006.pdf`

## Claim

Inexact Newton methods solve the Newton system $\nabla^2 f_k p_k = -\nabla f_k$ approximately using iterative methods like conjugate gradient, reducing computational cost from $O(d^3)$ to $O(d^2)$ or better while maintaining superlinear convergence with appropriate forcing sequences

## Quote

> In this section, we describe techniques for obtaining approximations to $p_k^N$ that are inexpensive to calculate but are good search directions or steps. These approaches are based on solving (7.1) by using the conjugate gradient (CG) method (see Chapter 5) or the Lanczos method, with modifications to handle negative curvature in the Hessian $\nabla^2 f_k$. [...] The use of iterative methods for (7.1) spares us from concerns about the expense of a direct factorization of the Hessian $\nabla^2 f_k$ and the fill-in that may occur during this process. [...] Theorem 7.2: Suppose that the conditions of Theorem 7.1 hold, and assume that the iterates $\{x_k\}$ generated by the inexact Newton method converge to $x^*$. Then the rate of convergence is superlinear if $\eta_k \to 0$. If in addition, $\nabla^2 f(x)$ is Lipschitz continuous for $x$ near $x^*$ and if $\eta_k = O(\|\nabla f_k\|)$, then the convergence is quadratic.

**Pages:** 165-169

**Theorem/Result:** Theorem 7.2

## Extracted Formulas

*These formulas were extracted using the cropping workflow (see [agent-formula-extraction.md](../workflows/agent-formula-extraction.md)) for verification.*

### Formula 1

**Extracted LaTeX:**

$$
\eta_k \to 0
$$

<details>
<summary>LaTeX Source</summary>

```latex
\eta_k \to 0
```

</details>

**Verification:** ❌ Not Verified

---

### Formula 2

**Extracted LaTeX:**

$$
\eta_k = O(\|\nabla f_k\|)
$$

<details>
<summary>LaTeX Source</summary>

```latex
\eta_k = O(\|\nabla f_k\|)
```

</details>

**Verification:** ❌ Not Verified

---

### Formula 3

**Extracted LaTeX:**

$$
\|r_k\| \leq \eta_k \|\nabla f_k\|
$$

<details>
<summary>LaTeX Source</summary>

```latex
\|r_k\| \leq \eta_k \|\nabla f_k\|
```

</details>

**Verification:** ❌ Not Verified

---

### Formula 4

**Extracted LaTeX:**

$$
\eta_k = \min(0.5, \sqrt{\|\nabla f_k\|})
$$

<details>
<summary>LaTeX Source</summary>

```latex
\eta_k = \min(0.5, \sqrt{\|\nabla f_k\|})
```

</details>

**Verification:** ❌ Not Verified

---

## Reader Notes

Inexact Newton methods solve the Newton system $\nabla^2 f_k p_k = -\nabla f_k$ approximately rather than exactly. Instead of computing an exact factorization of the Hessian (which costs $O(d^3)$ operations), these methods use iterative linear solvers like conjugate gradient (CG) that terminate after achieving residual $\|r_k\| \leq \eta_k \|\nabla f_k\|$ for some tolerance $\eta_k \in (0,1)$. For sparse or structured problems, each CG iteration costs only $O(d^2)$ or even $O(d)$ operations (matrix-vector products), making the method practical for large-scale optimization. The forcing sequence $\{\eta_k\}$ controls the trade-off between per-iteration cost and convergence rate: (1) If $\eta_k \leq \eta < 1$ (constant), the method converges linearly (Theorem 7.1). (2) If $\eta_k \to 0$, the method converges superlinearly (Theorem 7.2). (3) If $\eta_k = O(\|\nabla f_k\|)$, the method converges quadratically (Theorem 7.2). A common practical choice is $\eta_k = \min(0.5, \sqrt{\|\nabla f_k\|})$ which achieves superlinear convergence while keeping early iterations cheap. The method can even be implemented in a 'Hessian-free' manner using finite differences to approximate Hessian-vector products $\nabla^2 f_k d \approx (\nabla f(x_k + hd) - \nabla f(x_k))/h$ (page 190, equation 7.10), requiring only gradient evaluations.

## Internal Notes

Internal: Section 7.1 (pages 165-169) introduces inexact Newton methods and their motivation. Key points: (1) Direct factorization of Hessian costs $O(d^3)$, iterative methods reduce this to $O(d^2)$ per iteration or better depending on problem structure. (2) Methods solve Newton system approximately using CG or Lanczos with residual tolerance $\|r_k\| \leq \eta_k \|\nabla f_k\|$ where $\{\eta_k\}$ is the forcing sequence. (3) Theorem 7.1 (p. 186) proves local convergence when $\eta_k \leq \eta < 1$. (4) Theorem 7.2 (p. 188) proves superlinear convergence when $\eta_k \to 0$, and quadratic convergence when $\eta_k = O(\|\nabla f_k\|)$. (5) Common choice: $\eta_k = \min(0.5, \sqrt{\|\nabla f_k\|})$ for superlinear convergence. (6) Page 166 explicitly states this applies 'not just to Newton-CG procedures but to all inexact Newton methods whose steps satisfy (7.2) and (7.3).' Used in NewtonTab to justify using CG for large-scale problems.

## Verification

**Verified:** 2025-11-12

**Verified By:** verification-agent

**Verification Notes:** VERIFIED (Batch 5 - Adversarial): FIXED critical page numbering error. Original proofPages had PDF pages 0165-0169 which contained WRONG content (SR1 Method from Chapter 6). Corrected to PDF pages 0185-0189 (book pages 165-169) which contain Section 7.1 'Inexact Newton Methods'. The 20-page offset was not accounted for in original citation. Quote is word-for-word accurate. Theorem 7.1 (page 186) and Theorem 7.2 (page 188) verified. NOTE: The claim's 'O(d³) to O(d²)' complexity reduction is an INTERPRETATION - the source says 'spares us from concerns about the expense of a direct factorization' but doesn't explicitly state Big-O notation. This interpretation is correct and well-justified (direct factorization = O(d³), CG matrix-vector products = O(d²) for dense or O(d) for sparse).

## Used In

- NewtonTab

## Proof Pages

### Page 1

![Proof page 1](../extracted-pages/numericaloptimization2006_page_0185.png)

### Page 2

![Proof page 2](../extracted-pages/numericaloptimization2006_page_0186.png)

### Page 3

![Proof page 3](../extracted-pages/numericaloptimization2006_page_0187.png)

### Page 4

![Proof page 4](../extracted-pages/numericaloptimization2006_page_0188.png)

### Page 5

![Proof page 5](../extracted-pages/numericaloptimization2006_page_0189.png)

