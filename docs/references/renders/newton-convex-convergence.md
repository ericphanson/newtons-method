# newton-convex-convergence

## Reference

Yurii Nesterov. *Lectures on Convex Optimization* (2nd edition). Springer, 2018.

**File:** `Lectures on Convex Optimization.pdf`

## Claim

For convex functions with Lipschitz continuous Hessian, Newton's method (with cubic regularization) converges to the global minimum. Specifically, the method finds stationary points satisfying $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) \succeq 0$, which for convex functions are global minima.

## Quote

> Theorem 4.1.2: Let the sequence $\{x_i\}$ be generated by method (4.1.16). Let us assume that for some $i \geq 0$ the set $\mathscr{L}(f(x_i))$ is bounded. Then there exists a limit $\lim_{i\to\infty} f(x_i) = f^*$. The set $X^*$ of limit points of this sequence is non-empty. Moreover, this is a connected set such that for any $x^* \in X^*$ we have $f(x^*) = f^*$, $\nabla f(x^*) = 0$, $\nabla^2 f(x^*) \succeq 0$. [...] Example 4.1.1 (Convex Functions): Let $f$ be convex on $\mathbb{R}^n$. Assume it achieves its minimum at point $x^*$. Then, for any $x \in \mathbb{R}^n$ with $\|x - x^*\| < R$, we have $f(x) - f(x^*) \leq \langle\nabla f(x), x - x^*\rangle \leq \|\nabla f(x)\| \cdot R$. Thus, the function $f$ is a gradient dominated function of degree one. [...] Theorem 4.1.6: If $f(x_0) - f(x^*) \leq \gamma^2 \hat{\omega}$, then $f(x_k) - f(x^*) \leq \hat{\omega} \cdot \frac{\gamma^2}{(2 + k + \frac{3}{2\gamma})^2}$.

**Pages:** 242-257

**Theorem/Result:** Theorem 4.1.2, Theorem 4.1.6, and Example 4.1.1

## Reader Notes

Newton's method convergence on convex functions requires careful treatment because the Hessian may not be positive definite everywhere (only at the minimum). Nesterov's Cubic Regularization of Newton's Method (Algorithm 4.1.16) addresses this by using cubic regularization: at each iteration, minimize $\nabla f(x_k)^T(y - x_k) + \frac{1}{2}(y - x_k)^T\nabla^2 f(x_k)(y - x_k) + \frac{M}{6}\|y - x_k\|^3$ where $M \geq L$ is the Lipschitz constant of the Hessian. This regularization ensures the subproblem is always well-defined. For convex functions achieving their minimum at $x^*$, the convergence guarantee follows from two results: (1) Example 4.1.1 shows that convex functions are gradient-dominated of degree 1, meaning $f(x) - f(x^*) \leq \|\nabla f(x)\| \cdot R$ for $\|x - x^*\| < R$. (2) Theorem 4.1.6 establishes that for gradient-dominated functions of degree 1, the method achieves $f(x_k) - f(x^*) \leq O(1/k^2)$ convergence. (3) Theorem 4.1.2 guarantees that limit points satisfy $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) \succeq 0$, which for convex functions are precisely the global minima. The requirement of Lipschitz continuous Hessian (Assumption 4.1.1: $\|\nabla^2 f(x) - \nabla^2 f(y)\| \leq L\|x - y\|$) is essential for the analysis and is satisfied by many practical functions including twice-differentiable convex functions with bounded Hessians.

## Internal Notes

Internal: This citation addresses Newton's method convergence on convex functions with Lipschitz continuous Hessian. The key insight is that convex functions are gradient-dominated of degree 1 (Example 4.1.1), and the Cubic Regularization of Newton's Method converges for such functions. Assumption 4.1.1 (page 262) requires Lipschitz continuous Hessian: $\|\nabla^2 f(x) - \nabla^2 f(y)\| \leq L\|x - y\|$. For convex functions, stationary points with positive semidefinite Hessian are global minima (Theorem 1.2.2 on page 33, combined with convexity). The convergence rate for gradient-dominated functions of degree 1 is $O(1/k^2)$ (Theorem 4.1.6, equation 4.1.36). This is the cubic regularized Newton method (4.1.16), not the classical Newton method, but it handles the case where Hessian may not be positive definite everywhere. Used in NewtonTab.

## Verification

**Verified:** 2025-11-12

**Verified By:** verification-agent

**Verification Notes:** VERIFIED (Batch 5 - Adversarial): FIXED page numbering from PDF pages to book pages (262-277 â†’ 243-258). Added missing proof pages (268-270 for Theorem 4.1.2 continuation, 272-273 for connecting theorems). All 16 pages verified necessary: pages 262-267 (Assumption 4.1.1, Algorithm 4.1.16, supporting lemmas), pages 268-270 (Theorem 4.1.2 complete statement), pages 271-273 (connection to gradient dominated functions), pages 274-275 (Example 4.1.1 showing convex functions are gradient dominated), pages 276-277 (Theorem 4.1.6 convergence rate). Quote is accurate, claim correctly describes cubic regularized Newton's method converging to global minimum for convex functions with Lipschitz continuous Hessian. The 16-page range is justified.

## Used In

- NewtonTab

## Proof Pages

### Page 1

![Proof page 1](../extracted-pages/lectures_on_convex_optimization_page_0262.png)

### Page 2

![Proof page 2](../extracted-pages/lectures_on_convex_optimization_page_0263.png)

### Page 3

![Proof page 3](../extracted-pages/lectures_on_convex_optimization_page_0264.png)

### Page 4

![Proof page 4](../extracted-pages/lectures_on_convex_optimization_page_0265.png)

### Page 5

![Proof page 5](../extracted-pages/lectures_on_convex_optimization_page_0266.png)

### Page 6

![Proof page 6](../extracted-pages/lectures_on_convex_optimization_page_0267.png)

### Page 7

![Proof page 7](../extracted-pages/lectures_on_convex_optimization_page_0268.png)

### Page 8

![Proof page 8](../extracted-pages/lectures_on_convex_optimization_page_0269.png)

### Page 9

![Proof page 9](../extracted-pages/lectures_on_convex_optimization_page_0270.png)

### Page 10

![Proof page 10](../extracted-pages/lectures_on_convex_optimization_page_0271.png)

### Page 11

![Proof page 11](../extracted-pages/lectures_on_convex_optimization_page_0272.png)

### Page 12

![Proof page 12](../extracted-pages/lectures_on_convex_optimization_page_0273.png)

### Page 13

![Proof page 13](../extracted-pages/lectures_on_convex_optimization_page_0274.png)

### Page 14

![Proof page 14](../extracted-pages/lectures_on_convex_optimization_page_0275.png)

### Page 15

![Proof page 15](../extracted-pages/lectures_on_convex_optimization_page_0276.png)

### Page 16

![Proof page 16](../extracted-pages/lectures_on_convex_optimization_page_0277.png)

