# newton-convex-convergence

**Source:** [newton-convex-convergence.json](../../citations/newton-convex-convergence.json)

## Reference

Yurii Nesterov. *Lectures on Convex Optimization* (2nd edition). Springer, 2018.

**File:** `Lectures on Convex Optimization.pdf`

## Claim

For convex functions with Lipschitz continuous Hessian, Newton's method (with cubic regularization) converges to the global minimum. Specifically, the method finds stationary points where the gradient vanishes and the Hessian is positive semidefinite, which for convex functions are global minima.

## Quote

> Theorem 4.1.2: Let the sequence $\{x_i\}$ be generated by method (4.1.16). Let us assume that for some $i \geq 0$ the set $\mathscr{L}(f(x_i))$ is bounded. Then there exists a limit $\lim_{i\to\infty} f(x_i) = f^*$. The set $X^*$ of limit points of this sequence is non-empty. Moreover, this is a connected set such that for any $x^* \in X^*$ we have $f(x^*) = f^*$, $\nabla f(x^*) = 0$, $\nabla^2 f(x^*) \succeq 0$. [...] Example 4.1.2 (Strongly Convex Functions): Let $f$ be convex on $\mathbb{R}^n$. Assume it achieves its minimum at point $x^*$. Then, for any $x \in \mathbb{R}^n$ with $\|x - x^*\| < R$, we have $f(x) - f(x^*) \leq \langle\nabla f(x), x - x^*\rangle \leq \|\nabla f(x)\| \cdot R$. Thus, the function $f$ is a gradient dominated function of degree one. [...] Theorem 4.1.6: If $f(x_0) - f(x^*) \leq \gamma^2 \hat{\omega}$, then $f(x_k) - f(x^*) \leq \hat{\omega} \cdot \frac{\gamma^2}{(2 + k + \frac{3}{2\gamma})^2}$.

**Pages:** 243-258

**Theorem/Result:** Theorem 4.1.2, Theorem 4.1.6, and Example 4.1.2

## Extracted Formulas

*These formulas were extracted using the cropping workflow (see [agent-formula-extraction.md](../workflows/agent-formula-extraction.md)) for verification.*

### Formula 1 - Algorithm (4.1.5) (4.1.5)

**Cropped Formula Image:**

![lectures_on_convex_optimization_p262_algorithm_4_1_5](../extracted-pages/formulas/lectures_on_convex_optimization_p262_algorithm_4_1_5.png)

**Extracted LaTeX:**

$$
\min_y \left[ \langle \nabla f(x), y - x \rangle + \frac{1}{2}\langle \nabla^2 f(x)(y - x), y - x \rangle + \frac{M}{6}\|y - x\|^3 \right]
$$

<details>
<summary>LaTeX Source</summary>

```latex
\min_y \left[ \langle \nabla f(x), y - x \rangle + \frac{1}{2}\langle \nabla^2 f(x)(y - x), y - x \rangle + \frac{M}{6}\|y - x\|^3 \right]
```

</details>

**Verification:** ✅ Verified

**Metadata:** [lectures_on_convex_optimization_p262_algorithm_4_1_5.json](../extracted-pages/formulas/lectures_on_convex_optimization_p262_algorithm_4_1_5.json)

---

### Formula 2 - Theorem 4.1.2 Limit point characterization

**Cropped Formula Image:**

![lectures_on_convex_optimization_p267_theorem_4_1_2](../extracted-pages/formulas/lectures_on_convex_optimization_p267_theorem_4_1_2.png)

**Extracted LaTeX:**

$$
\lim_{i \to \infty} f(x_i) = f^*, \quad f(x^*) = f^*, \quad \nabla f(x^*) = 0, \quad \nabla^2 f(x^*) \succeq 0
$$

<details>
<summary>LaTeX Source</summary>

```latex
\lim_{i \to \infty} f(x_i) = f^*, \quad f(x^*) = f^*, \quad \nabla f(x^*) = 0, \quad \nabla^2 f(x^*) \succeq 0
```

</details>

**Verification:** ✅ Verified

**Metadata:** [lectures_on_convex_optimization_p267_theorem_4_1_2.json](../extracted-pages/formulas/lectures_on_convex_optimization_p267_theorem_4_1_2.json)

---

### Formula 3 - Definition 4.1.3 (4.1.31)

**Cropped Formula Image:**

![lectures_on_convex_optimization_p274_definition_4_1_3](../extracted-pages/formulas/lectures_on_convex_optimization_p274_definition_4_1_3.png)

**Extracted LaTeX:**

$$
f(x) - f(x^*) \leq \tau_f \|\nabla f(x)\|^p
$$

<details>
<summary>LaTeX Source</summary>

```latex
f(x) - f(x^*) \leq \tau_f \|\nabla f(x)\|^p
```

</details>

**Verification:** ✅ Verified

**Metadata:** [lectures_on_convex_optimization_p274_definition_4_1_3.json](../extracted-pages/formulas/lectures_on_convex_optimization_p274_definition_4_1_3.json)

---

### Formula 4 - Example 4.1.2 (4.1.32)

**Cropped Formula Image:**

![lectures_on_convex_optimization_p275_example_4_1_2](../extracted-pages/formulas/lectures_on_convex_optimization_p275_example_4_1_2.png)

**Extracted LaTeX:**

$$
f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2}\mu \|y - x\|^2
$$

<details>
<summary>LaTeX Source</summary>

```latex
f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle + \frac{1}{2}\mu \|y - x\|^2
```

</details>

**Verification:** ✅ Verified

**Metadata:** [lectures_on_convex_optimization_p275_example_4_1_2.json](../extracted-pages/formulas/lectures_on_convex_optimization_p275_example_4_1_2.json)

---

### Formula 5 - Theorem 4.1.6 (4.1.36)

**Cropped Formula Image:**

![lectures_on_convex_optimization_p276_theorem_4_1_6](../extracted-pages/formulas/lectures_on_convex_optimization_p276_theorem_4_1_6.png)

**Extracted LaTeX:**

$$
f(x_k) - f(x^*) \leq \hat{\omega} \cdot \frac{\gamma^2\left(2+\frac{3}{2}\gamma\right)^2}{\left(2+\left(k+\frac{3}{2}\right)\cdot\gamma\right)^2}
$$

<details>
<summary>LaTeX Source</summary>

```latex
f(x_k) - f(x^*) \leq \hat{\omega} \cdot \frac{\gamma^2\left(2+\frac{3}{2}\gamma\right)^2}{\left(2+\left(k+\frac{3}{2}\right)\cdot\gamma\right)^2}
```

</details>

**Verification:** ✅ Verified

**Metadata:** [lectures_on_convex_optimization_p276_theorem_4_1_6.json](../extracted-pages/formulas/lectures_on_convex_optimization_p276_theorem_4_1_6.json)

---

## Reader Notes

Newton's method convergence on convex functions requires careful treatment because the Hessian may not be positive definite everywhere (only at the minimum). Nesterov's Cubic Regularization of Newton's Method (Algorithm 4.1.16) addresses this by using cubic regularization: at each iteration, minimize $\nabla f(x_k)^T(y - x_k) + \frac{1}{2}(y - x_k)^T\nabla^2 f(x_k)(y - x_k) + \frac{M}{6}\|y - x_k\|^3$ where $M \geq L$ is the Lipschitz constant of the Hessian. This regularization ensures the subproblem is always well-defined. For convex functions achieving their minimum at $x^*$, the convergence guarantee follows from two results: (1) Example 4.1.2 shows that convex functions are gradient-dominated of degree 1, meaning $f(x) - f(x^*) \leq \|\nabla f(x)\| \cdot R$ for $\|x - x^*\| < R$. (2) Theorem 4.1.6 establishes that for gradient-dominated functions of degree 1, the method achieves $f(x_k) - f(x^*) \leq O(1/k^2)$ convergence. (3) Theorem 4.1.2 guarantees that limit points satisfy $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) \succeq 0$, which for convex functions are precisely the global minima. The requirement of Lipschitz continuous Hessian (Assumption 4.1.1: $\|\nabla^2 f(x) - \nabla^2 f(y)\| \leq L\|x - y\|$) is essential for the analysis and is satisfied by many practical functions including twice-differentiable convex functions with bounded Hessians.

## Internal Notes

Internal: This citation addresses Newton's method convergence on convex functions with Lipschitz continuous Hessian. The key insight is that convex functions are gradient-dominated of degree 1 (Example 4.1.2), and the Cubic Regularization of Newton's Method converges for such functions. Assumption 4.1.1 (page 262) requires Lipschitz continuous Hessian: $\|\nabla^2 f(x) - \nabla^2 f(y)\| \leq L\|x - y\|$. For convex functions, stationary points with positive semidefinite Hessian are global minima (Theorem 1.2.2 on page 33, combined with convexity). The convergence rate for gradient-dominated functions of degree 1 is $O(1/k^2)$ (Theorem 4.1.6, equation 4.1.36). This is the cubic regularized Newton method (4.1.16), not the classical Newton method, but it handles the case where Hessian may not be positive definite everywhere. Used in NewtonTab.

## Verification

**Verified:** 2025-11-12

**Verified By:** verification-agent

**Verification Notes:** COMPLETE (2025-11-13): All 5 formulas extracted and verified at 300 DPI. (1) Algorithm 4.1.5 page 262 - cubic regularization subproblem. (2) Theorem 4.1.2 page 267 - limit point characterization with stationary conditions. (3) Definition 4.1.3 page 274 - gradient-dominated function of degree p. (4) Example 4.1.2 page 275 - strongly convex inequality (equation 4.1.32). (5) Theorem 4.1.6 page 276 - convergence bound for gradient-dominated functions (CRITICAL FIX from Batch 5: corrected missing numerator factor). All formulas follow 3-checkpoint verification workflow with no cutoffs.

## Used In

- NewtonTab

## Proof Pages

### Page 1

![Proof page 1](../extracted-pages/lectures_on_convex_optimization_page_0262.png)

### Page 2

![Proof page 2](../extracted-pages/lectures_on_convex_optimization_page_0263.png)

### Page 3

![Proof page 3](../extracted-pages/lectures_on_convex_optimization_page_0264.png)

### Page 4

![Proof page 4](../extracted-pages/lectures_on_convex_optimization_page_0265.png)

### Page 5

![Proof page 5](../extracted-pages/lectures_on_convex_optimization_page_0266.png)

### Page 6

![Proof page 6](../extracted-pages/lectures_on_convex_optimization_page_0267.png)

### Page 7

![Proof page 7](../extracted-pages/lectures_on_convex_optimization_page_0268.png)

### Page 8

![Proof page 8](../extracted-pages/lectures_on_convex_optimization_page_0269.png)

### Page 9

![Proof page 9](../extracted-pages/lectures_on_convex_optimization_page_0270.png)

### Page 10

![Proof page 10](../extracted-pages/lectures_on_convex_optimization_page_0271.png)

### Page 11

![Proof page 11](../extracted-pages/lectures_on_convex_optimization_page_0272.png)

### Page 12

![Proof page 12](../extracted-pages/lectures_on_convex_optimization_page_0273.png)

### Page 13

![Proof page 13](../extracted-pages/lectures_on_convex_optimization_page_0274.png)

### Page 14

![Proof page 14](../extracted-pages/lectures_on_convex_optimization_page_0275.png)

### Page 15

![Proof page 15](../extracted-pages/lectures_on_convex_optimization_page_0276.png)

### Page 16

![Proof page 16](../extracted-pages/lectures_on_convex_optimization_page_0277.png)

