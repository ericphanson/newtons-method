# gd-linesearch-strongly-convex-linear-convergence-nocedal-wright-2006

## Reference

Jorge Nocedal and Stephen J. Wright. *Numerical Optimization* (2nd edition). Springer, 2006.

**File:** `NumericalOptimization2006.pdf`

## Claim

For strongly convex quadratic functions and general smooth strongly convex functions, steepest descent with exact line search achieves linear convergence with rate determined by the condition number: $\|x_{k+1} - x^*\|_Q^2 \leq \left(\frac{\lambda_n - \lambda_1}{\lambda_n + \lambda_1}\right)^2 \|x_k - x^*\|_Q^2$ where $\kappa(Q) = \lambda_n/\lambda_1$ is the condition number

## Quote

> Theorem 3.3. When the steepest descent method with exact line searches is applied to the strongly convex quadratic function $f(x) = \frac{1}{2}x^T Q x - b^T x$, the error norm satisfies $\|x_{k+1} - x^*\|_Q^2 \leq \left(\frac{\lambda_n - \lambda_1}{\lambda_n + \lambda_1}\right)^2 \|x_k - x^*\|_Q^2$, where $0 < \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$ are the eigenvalues of $Q$. [...] Theorem 3.4. Suppose that $f : \mathbb{R}^n \to \mathbb{R}$ is twice continuously differentiable, and that the iterates generated by the steepest-descent method with exact line searches converge to a point $x^*$ at which the Hessian matrix $\nabla^2 f(x^*)$ is positive definite. Let $r$ be any scalar satisfying $r \in [(\lambda_n - \lambda_1)/(\lambda_n + \lambda_1), 1]$, where $\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$ are the eigenvalues of $\nabla^2 f(x^*)$. Then for all $k$ sufficiently large, we have $f(x_{k+1}) - f(x^*) \leq r^2[f(x_k) - f(x^*)]$.

**Pages:** 42-44

**Theorem/Result:** Theorem 3.3 and Theorem 3.4

## Reader Notes

These results show that steepest descent with exact line search achieves linear convergence on strongly convex functions, with the convergence rate determined by the condition number $\kappa = \lambda_n/\lambda_1$. For quadratic functions $f(x) = \frac{1}{2}x^T Q x - b^T x$, Theorem 3.3 gives the exact rate $(\lambda_n - \lambda_1)/(\lambda_n + \lambda_1)$. For general smooth functions, Theorem 3.4 shows that near a solution $x^*$ where $\nabla^2 f(x^*)$ is positive definite, the method achieves the same asymptotic rate based on the eigenvalues of the Hessian at $x^*$. The exact line search finds the step length $\alpha_k$ that minimizes $f(x_k - \alpha \nabla f(x_k))$, given by $\alpha_k = \nabla f_k^T \nabla f_k / (\nabla f_k^T Q \nabla f_k)$ for quadratic functions (equation 3.25, book page 42). The convergence rate degrades as the condition number increases: when $\kappa$ is large, the rate approaches 1 and convergence becomes very slow. The weighted norm $\|x\|_Q^2 = x^T Q x$ measures optimality gap: $\frac{1}{2}\|x - x^*\|_Q^2 = f(x) - f(x^*)$ (equation 3.27, book page 43).

## Internal Notes

Internal: These theorems establish linear convergence for steepest descent (gradient descent) with exact line search on strongly convex functions. Theorem 3.3 gives the exact rate for quadratic functions, while Theorem 3.4 extends to general nonlinear functions (asymptotically, near the solution). The convergence rate depends on the condition number $\kappa = \lambda_n/\lambda_1 = L/\mu$. The rate $(\lambda_n - \lambda_1)/(\lambda_n + \lambda_1) = (L-\mu)/(L+\mu) = (Q-1)/(Q+1)$ where $Q = L/\mu$ is the condition number. This matches the result in Nesterov 2018 Theorem 2.1.15 for globally strongly convex functions. The exact line search minimizes $f(x_k - \alpha \nabla f(x_k))$ over $\alpha > 0$. Used in GdLineSearchTab.

## Verification

**Verified:** 2025-11-12

**Verified By:** adversarial-verification-agent-batch6-agent3

**Verification Notes:** ADVERSARIAL VERIFICATION: Found and corrected page numbering error. The citation originally listed pages as "62-64" which are PDF page numbers, not book page numbers. The actual book pages are 42-44. The proof pages (PDF pages 62-64) are correct and contain the right content. Theorem 3.3 appears on book page 43 (PDF page 63), and Theorem 3.4 spans book pages 43-44 (PDF pages 63-64). The context pages showing equation 3.24 (quadratic function definition), equation 3.25 (exact line search formula), and equation 3.26 (steepest descent iteration) are on book page 42 (PDF page 62). Quote accuracy verified word-for-word against source. Claim is consistent with theorems - correctly states the squared convergence rate. Updated readerNotes to use correct book page numbers for equation references.

## Used In

- GdLineSearchTab

## Proof Pages

### Page 1

![Proof page 1](../extracted-pages/numericaloptimization2006_page_0062.png)

### Page 2

![Proof page 2](../extracted-pages/numericaloptimization2006_page_0063.png)

### Page 3

![Proof page 3](../extracted-pages/numericaloptimization2006_page_0064.png)

