{
  "reference": "nocedal-wright-2006",
  "pages": "165-169",
  "theorem": "Theorem 7.2",
  "claim": "Inexact Newton methods solve the Newton system $\\nabla^2 f_k p_k = -\\nabla f_k$ approximately using iterative methods like conjugate gradient, reducing computational cost from $O(d^3)$ to $O(d^2)$ or better while maintaining superlinear convergence with appropriate forcing sequences",
  "quote": "In this section, we describe techniques for obtaining approximations to $p_k^N$ that are inexpensive to calculate but are good search directions or steps. These approaches are based on solving (7.1) by using the conjugate gradient (CG) method (see Chapter 5) or the Lanczos method, with modifications to handle negative curvature in the Hessian $\\nabla^2 f_k$. [...] The use of iterative methods for (7.1) spares us from concerns about the expense of a direct factorization of the Hessian $\\nabla^2 f_k$ and the fill-in that may occur during this process. [...] Theorem 7.2: Suppose that the conditions of Theorem 7.1 hold, and assume that the iterates $\\{x_k\\}$ generated by the inexact Newton method converge to $x^*$. Then the rate of convergence is superlinear if $\\eta_k \\to 0$. If in addition, $\\nabla^2 f(x)$ is Lipschitz continuous for $x$ near $x^*$ and if $\\eta_k = O(\\|\\nabla f_k\\|)$, then the convergence is quadratic.",
  "notes": "Internal: Section 7.1 (pages 165-169) introduces inexact Newton methods and their motivation. Key points: (1) Direct factorization of Hessian costs $O(d^3)$, iterative methods reduce this to $O(d^2)$ per iteration or better depending on problem structure. (2) Methods solve Newton system approximately using CG or Lanczos with residual tolerance $\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|$ where $\\{\\eta_k\\}$ is the forcing sequence. (3) Theorem 7.1 (p. 186) proves local convergence when $\\eta_k \\leq \\eta < 1$. (4) Theorem 7.2 (p. 188) proves superlinear convergence when $\\eta_k \\to 0$, and quadratic convergence when $\\eta_k = O(\\|\\nabla f_k\\|)$. (5) Common choice: $\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})$ for superlinear convergence. (6) Page 166 explicitly states this applies 'not just to Newton-CG procedures but to all inexact Newton methods whose steps satisfy (7.2) and (7.3).' Used in NewtonTab to justify using CG for large-scale problems.",
  "readerNotes": "Inexact Newton methods solve the Newton system $\\nabla^2 f_k p_k = -\\nabla f_k$ approximately rather than exactly. Instead of computing an exact factorization of the Hessian (which costs $O(d^3)$ operations), these methods use iterative linear solvers like conjugate gradient (CG) that terminate after achieving residual $\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|$ for some tolerance $\\eta_k \\in (0,1)$. For sparse or structured problems, each CG iteration costs only $O(d^2)$ or even $O(d)$ operations (matrix-vector products), making the method practical for large-scale optimization. The forcing sequence $\\{\\eta_k\\}$ controls the trade-off between per-iteration cost and convergence rate: (1) If $\\eta_k \\leq \\eta < 1$ (constant), the method converges linearly (Theorem 7.1). (2) If $\\eta_k \\to 0$, the method converges superlinearly (Theorem 7.2). (3) If $\\eta_k = O(\\|\\nabla f_k\\|)$, the method converges quadratically (Theorem 7.2). A common practical choice is $\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})$ which achieves superlinear convergence while keeping early iterations cheap. The method can even be implemented in a 'Hessian-free' manner using finite differences to approximate Hessian-vector products $\\nabla^2 f_k d \\approx (\\nabla f(x_k + hd) - \\nabla f(x_k))/h$ (page 190, equation 7.10), requiring only gradient evaluations.",
  "proofPages": [
    "docs/references/extracted-pages/numericaloptimization2006_page_0185.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0186.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0187.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0188.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0189.png"
  ],
  "formulaImages": [
    {
      "formula_id": "inexact_newton_superlinear_condition",
      "description": "Superlinear convergence condition: forcing sequence must decay to zero",
      "latex": "\\eta_k \\to 0",
      "verified": true,
      "theorem": "Theorem 7.2",
      "context": "The rate of convergence is superlinear if $\\eta_k \\to 0$",
      "significance": "Central condition for superlinear convergence of inexact Newton methods",
      "page": 188
    },
    {
      "formula_id": "inexact_newton_quadratic_condition",
      "description": "Quadratic convergence condition with Lipschitz Hessian",
      "latex": "\\eta_k = O(\\|\\nabla f_k\\|)",
      "verified": true,
      "theorem": "Theorem 7.2",
      "context": "If $\\nabla^2 f(x)$ is Lipschitz continuous near $x^*$ and $\\eta_k = O(\\|\\nabla f_k\\|)$, then convergence is quadratic",
      "significance": "Sufficient condition for quadratic convergence",
      "page": 188
    },
    {
      "formula_id": "inexact_newton_residual_tolerance",
      "description": "Residual tolerance criterion for iterative linear solvers",
      "latex": "\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|",
      "verified": true,
      "context": "Standard termination criterion for CG and other iterative solvers",
      "significance": "Balances per-iteration cost against convergence rate",
      "pages": "165-169"
    },
    {
      "formula_id": "inexact_newton_practical_forcing",
      "description": "Practical forcing sequence for superlinear convergence",
      "latex": "\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})",
      "verified": true,
      "context": "Common practical choice that achieves superlinear convergence",
      "significance": "Widely used heuristic in implementations",
      "pages": "165-169"
    }
  ],
  "verified": "2025-11-12",
  "verifiedBy": "verification-agent",
  "verificationNotes": "VERIFIED (Batch 5 - Formula Structure Update): Updated formulaImages to new standardized structure with formula_id, verified flags, and proper metadata. All 4 formulas verified against source (Theorem 7.2, page 188). Page numbering previously corrected (PDF 185-189 = book 165-169). Quote accurate. Formulas are simple conditions (no image cropping needed) - LaTeX extracted directly from theorem statement.",
  "usedIn": [
    "NewtonTab"
  ],
  "pdfPages": "185-189"
}