{
  "reference": "nocedal-wright-2006",
  "pages": "165-169",
  "theorem": "Theorem 7.2",
  "claim": "Inexact Newton methods solve the Newton system $\\nabla^2 f_k p_k = -\\nabla f_k$ approximately using iterative methods like conjugate gradient, reducing computational cost from $O(d^3)$ to $O(d^2)$ or better while maintaining superlinear convergence with appropriate forcing sequences",
  "quote": "In this section, we describe techniques for obtaining approximations to $p_k^N$ that are inexpensive to calculate but are good search directions or steps. These approaches are based on solving (7.1) by using the conjugate gradient (CG) method (see Chapter 5) or the Lanczos method, with modifications to handle negative curvature in the Hessian $\\nabla^2 f_k$... The use of iterative methods for (7.1) spares us from concerns about the expense of a direct factorization of the Hessian $\\nabla^2 f_k$ and the fill-in that may occur during this process... Theorem 7.2: Suppose that the conditions of Theorem 7.1 hold, and assume that the iterates $\\{x_k\\}$ generated by the inexact Newton method converge to $x^*$. Then the rate of convergence is superlinear if $\\eta_k \\to 0$. If in addition, $\\nabla^2 f(x)$ is Lipschitz continuous for $x$ near $x^*$ and if $\\eta_k = O(\\|\\nabla f_k\\|)$, then the convergence is quadratic.",
  "notes": "Internal: Section 7.1 (pages 165-169) introduces inexact Newton methods and their motivation. Key points: (1) Direct factorization of Hessian costs $O(d^3)$, iterative methods reduce this to $O(d^2)$ per iteration or better depending on problem structure. (2) Methods solve Newton system approximately using CG or Lanczos with residual tolerance $\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|$ where $\\{\\eta_k\\}$ is the forcing sequence. (3) Theorem 7.1 (p. 186) proves local convergence when $\\eta_k \\leq \\eta < 1$. (4) Theorem 7.2 (p. 188) proves superlinear convergence when $\\eta_k \\to 0$, and quadratic convergence when $\\eta_k = O(\\|\\nabla f_k\\|)$. (5) Common choice: $\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})$ for superlinear convergence. (6) Page 166 explicitly states this applies 'not just to Newton-CG procedures but to all inexact Newton methods whose steps satisfy (7.2) and (7.3).' Used in NewtonTab to justify using CG for large-scale problems.",
  "readerNotes": "Inexact Newton methods solve the Newton system $\\nabla^2 f_k p_k = -\\nabla f_k$ approximately rather than exactly. Instead of computing an exact factorization of the Hessian (which costs $O(d^3)$ operations), these methods use iterative linear solvers like conjugate gradient (CG) that terminate after achieving residual $\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|$ for some tolerance $\\eta_k \\in (0,1)$. For sparse or structured problems, each CG iteration costs only $O(d^2)$ or even $O(d)$ operations (matrix-vector products), making the method practical for large-scale optimization. The forcing sequence $\\{\\eta_k\\}$ controls the trade-off between per-iteration cost and convergence rate: (1) If $\\eta_k \\leq \\eta < 1$ (constant), the method converges linearly (Theorem 7.1). (2) If $\\eta_k \\to 0$, the method converges superlinearly (Theorem 7.2). (3) If $\\eta_k = O(\\|\\nabla f_k\\|)$, the method converges quadratically (Theorem 7.2). A common practical choice is $\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})$ which achieves superlinear convergence while keeping early iterations cheap. The method can even be implemented in a 'Hessian-free' manner using finite differences to approximate Hessian-vector products $\\nabla^2 f_k d \\approx (\\nabla f(x_k + hd) - \\nabla f(x_k))/h$ (page 190, equation 7.10), requiring only gradient evaluations.",
  "proofPages": [
    "docs/references/extracted-pages/numericaloptimization2006_page_0185.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0186.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0187.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0188.png",
    "docs/references/extracted-pages/numericaloptimization2006_page_0189.png"
  ],
  "formulaImages": [
    {
      "description": "Superlinear convergence condition for inexact Newton methods",
      "formula": "\\eta_k \\to 0",
      "context": "Theorem 7.2 - The rate of convergence is superlinear if $\\eta_k \\to 0$",
      "source": "Nocedal & Wright (2006), Theorem 7.2, page 188",
      "significance": "Central condition for superlinear convergence; forcing sequence must decay to zero where $\\eta_k$ is the residual tolerance parameter in the inexact Newton system solve"
    },
    {
      "description": "Quadratic convergence condition with Lipschitz continuous Hessian",
      "formula": "\\eta_k = O(\\|\\nabla f_k\\|)",
      "context": "Theorem 7.2 - If in addition $\\nabla^2 f(x)$ is Lipschitz continuous near $x^*$ and $\\eta_k = O(\\|\\nabla f_k\\|)$, then convergence is quadratic",
      "source": "Nocedal & Wright (2006), Theorem 7.2, page 188",
      "significance": "Sufficient condition for quadratic convergence; forcing sequence decay proportional to gradient norm enables rapid convergence"
    },
    {
      "description": "Residual tolerance in inexact Newton system",
      "formula": "\\|r_k\\| \\leq \\eta_k \\|\\nabla f_k\\|",
      "context": "The linear system is solved to residual tolerance proportional to gradient norm",
      "source": "Nocedal & Wright (2006), Section 7.1, pages 165-169",
      "significance": "Standard termination criterion for iterative solvers like CG; balances per-iteration cost against convergence rate"
    },
    {
      "description": "Practical forcing sequence for superlinear convergence",
      "formula": "\\eta_k = \\min(0.5, \\sqrt{\\|\\nabla f_k\\|})",
      "context": "Common practical choice that achieves superlinear convergence while keeping early iterations cheap",
      "source": "Nocedal & Wright (2006), Section 7.1",
      "significance": "Widely used heuristic combining constant threshold with gradient-norm decay for good practical performance"
    }
  ],
  "verified": "2025-11-12",
  "verifiedBy": "verification-agent",
  "verificationNotes": "VERIFIED (Batch 5 - Adversarial): FIXED critical page numbering error. Original proofPages had PDF pages 0165-0169 which contained WRONG content (SR1 Method from Chapter 6). Corrected to PDF pages 0185-0189 (book pages 165-169) which contain Section 7.1 'Inexact Newton Methods'. The 20-page offset was not accounted for in original citation. Quote is word-for-word accurate. Theorem 7.1 (page 186) and Theorem 7.2 (page 188) verified. NOTE: The claim's 'O(d³) to O(d²)' complexity reduction is an INTERPRETATION - the source says 'spares us from concerns about the expense of a direct factorization' but doesn't explicitly state Big-O notation. This interpretation is correct and well-justified (direct factorization = O(d³), CG matrix-vector products = O(d²) for dense or O(d) for sparse).",
  "usedIn": [
    "NewtonTab"
  ],
  "pdfPages": "185-189"
}