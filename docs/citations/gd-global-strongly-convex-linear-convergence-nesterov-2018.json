{
  "reference": "nesterov-2018",
  "pages": "101-102",
  "pdfPages": "101-102",
  "theorem": "Theorem 2.1.15",
  "claim": "For globally $\\mu$-strongly convex, $L$-smooth functions, gradient descent with step size $0 < h \\leq 2/(\\mu+L)$ achieves global linear convergence: $\\|x_k - x^*\\|^2 \\leq (1 - 2h\\mu L/(\\mu+L))^k \\|x_0 - x^*\\|^2$. With the optimal step size $h^* = 2/(\\mu+L)$, this gives the rate $\\|x_k - x^*\\| \\leq ((Q_f-1)/(Q_f+1))^k \\|x_0 - x^*\\|$ where $Q_f = L/\\mu$ is the condition number.",
  "quote": "Theorem 2.1.15 If $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ and $0 < h \\leq \\frac{2}{\\mu+L}$, then the Gradient Method generates a sequence $\\{x_k\\}$ such that $\\|x_k - x^*\\|^2 \\leq \\left(1 - \\frac{2h\\mu L}{\\mu+L}\\right)^k \\|x_0 - x^*\\|^2$. If $h = \\frac{2}{\\mu+L}$, then $\\|x_k - x^*\\| \\leq \\left(\\frac{Q_f-1}{Q_f+1}\\right)^k \\|x_0 - x^*\\|$, $f(x_k) - f^* \\leq \\frac{L}{2}\\left(\\frac{Q_f-1}{Q_f+1}\\right)^{2k} \\|x_0 - x^*\\|^2$, where $Q_f = L/\\mu$.",
  "notes": "Internal: This is a GLOBAL convergence result for gradient descent on globally strongly convex smooth functions, unlike Theorem 1.2.4 which is a LOCAL result requiring the starting point to be near a strict local minimum. The function class $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ denotes functions that are $\\mu$-strongly convex and have $L$-Lipschitz continuous gradient everywhere (Definition 2.1.3, page 94). The theorem shows global linear convergence for any starting point $x_0$, with optimal step size $h^* = 2/(\\mu+L)$ giving convergence rate $(Q_f-1)/(Q_f+1)$ where $Q_f = L/\\mu$ is the condition number. Note the contrast with local Theorem 1.2.4 which requires $f \\in C_M^{2,2}$ (twice differentiable with Lipschitz continuous Hessian) and starting point within radius $\\bar{r} = 2\\mu/M$ of the local minimum. Used in GdFixedTab for global strongly convex convergence guarantees.",
  "readerNotes": "This theorem establishes GLOBAL linear convergence for gradient descent on strongly convex smooth functions, valid from any starting point. The function class $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ (Definition 2.1.3, page 94) requires the function to be $\\mu$-strongly convex with parameter $\\mu > 0$ and have $L$-Lipschitz continuous gradient everywhere. The step size restriction $0 < h \\leq 2/(\\mu+L)$ ensures convergence, with the optimal choice $h^* = 2/(\\mu+L)$ giving the fastest rate. The convergence rate $(Q_f-1)/(Q_f+1)$ depends on the condition number $Q_f = L/\\mu$: better conditioned problems (smaller $Q_f$) converge faster. Note: Nesterov uses $h$ for step size; here we use $\\alpha$. This GLOBAL result contrasts with the LOCAL result in Theorem 1.2.4 (pages 53-55) which requires the starting point to be close to a strict local minimum and assumes a twice-differentiable function with Lipschitz continuous Hessian.",
  "proofPages": [
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0094.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0101.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0102.png"
  ],
  "verified": "2025-11-14",
  "verifiedBy": "claude-code-agent",
  "verificationNotes": "Created citation for Theorem 2.1.15 (global strongly convex linear convergence). Verified theorem statement on pages 101-102. Page 101 contains the theorem statement and main convergence bound. Page 102 contains the proof and the explicit formulas for optimal step size h = 2/(μ+L). Extracted and verified the convergence rate formula. Page 94 contains Definition 2.1.3 for the function class S^{1,1}_{μ,L}. This is the GLOBAL result that should be used for general strongly convex functions, as opposed to Theorem 1.2.4 which is a LOCAL result requiring proximity to a strict local minimum. The theorem applies to any starting point x_0 and guarantees global linear convergence.",
  "usedIn": [
    "GdFixedTab"
  ],
  "formulaImages": [
    {
      "formula_id": "lectures_on_convex_optimization_p101_theorem_2_1_15",
      "metadata_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p101_theorem_2_1_15.json",
      "image_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p101_theorem_2_1_15.png",
      "latex": "\\|x_k - x^*\\|^2 \\leq \\left(1 - \\frac{2h\\mu L}{\\mu+L}\\right)^k \\|x_0 - x^*\\|^2",
      "verified": true,
      "theorem": "Theorem 2.1.15",
      "equation": ""
    }
  ]
}
