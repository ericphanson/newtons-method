{
  "reference": "nesterov-2018",
  "pages": "243-258",
  "theorem": "Theorem 4.1.2, Theorem 4.1.6, and Example 4.1.2",
  "claim": "For convex functions with Lipschitz continuous Hessian, Newton's method (with cubic regularization) converges to the global minimum. Specifically, the method finds stationary points where the gradient vanishes and the Hessian is positive semidefinite, which for convex functions are global minima.",
  "quote": "Theorem 4.1.2: Let the sequence $\\{x_i\\}$ be generated by method (4.1.16). Let us assume that for some $i \\geq 0$ the set $\\mathscr{L}(f(x_i))$ is bounded. Then there exists a limit $\\lim_{i\\to\\infty} f(x_i) = f^*$. The set $X^*$ of limit points of this sequence is non-empty. Moreover, this is a connected set such that for any $x^* \\in X^*$ we have $f(x^*) = f^*$, $\\nabla f(x^*) = 0$, $\\nabla^2 f(x^*) \\succeq 0$. [...] Example 4.1.2 (Strongly Convex Functions): Let $f$ be convex on $\\mathbb{R}^n$. Assume it achieves its minimum at point $x^*$. Then, for any $x \\in \\mathbb{R}^n$ with $\\|x - x^*\\| < R$, we have $f(x) - f(x^*) \\leq \\langle\\nabla f(x), x - x^*\\rangle \\leq \\|\\nabla f(x)\\| \\cdot R$. Thus, the function $f$ is a gradient dominated function of degree one. [...] Theorem 4.1.6: If $f(x_0) - f(x^*) \\leq \\gamma^2 \\hat{\\omega}$, then $f(x_k) - f(x^*) \\leq \\hat{\\omega} \\cdot \\frac{\\gamma^2}{(2 + k + \\frac{3}{2\\gamma})^2}$.",
  "notes": "Internal: This citation addresses Newton's method convergence on convex functions with Lipschitz continuous Hessian. The key insight is that convex functions are gradient-dominated of degree 1 (Example 4.1.2), and the Cubic Regularization of Newton's Method converges for such functions. Assumption 4.1.1 (page 262) requires Lipschitz continuous Hessian: $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$. For convex functions, stationary points with positive semidefinite Hessian are global minima (Theorem 1.2.2 on page 33, combined with convexity). The convergence rate for gradient-dominated functions of degree 1 is $O(1/k^2)$ (Theorem 4.1.6, equation 4.1.36). This is the cubic regularized Newton method (4.1.16), not the classical Newton method, but it handles the case where Hessian may not be positive definite everywhere. Used in NewtonTab.",
  "readerNotes": "Newton's method convergence on convex functions requires careful treatment because the Hessian may not be positive definite everywhere (only at the minimum). Nesterov's Cubic Regularization of Newton's Method (Algorithm 4.1.16) addresses this by using cubic regularization: at each iteration, minimize $\\nabla f(x_k)^T(y - x_k) + \\frac{1}{2}(y - x_k)^T\\nabla^2 f(x_k)(y - x_k) + \\frac{M}{6}\\|y - x_k\\|^3$ where $M \\geq L$ is the Lipschitz constant of the Hessian. This regularization ensures the subproblem is always well-defined. For convex functions achieving their minimum at $x^*$, the convergence guarantee follows from two results: (1) Example 4.1.2 shows that convex functions are gradient-dominated of degree 1, meaning $f(x) - f(x^*) \\leq \\|\\nabla f(x)\\| \\cdot R$ for $\\|x - x^*\\| < R$. (2) Theorem 4.1.6 establishes that for gradient-dominated functions of degree 1, the method achieves $f(x_k) - f(x^*) \\leq O(1/k^2)$ convergence. (3) Theorem 4.1.2 guarantees that limit points satisfy $\\nabla f(x^*) = 0$ and $\\nabla^2 f(x^*) \\succeq 0$, which for convex functions are precisely the global minima. The requirement of Lipschitz continuous Hessian (Assumption 4.1.1: $\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\leq L\\|x - y\\|$) is essential for the analysis and is satisfied by many practical functions including twice-differentiable convex functions with bounded Hessians.",
  "proofPages": [
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0262.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0263.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0264.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0265.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0266.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0267.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0268.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0269.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0270.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0271.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0272.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0273.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0274.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0275.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0276.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0277.png"
  ],
  "verified": "2025-11-12",
  "verifiedBy": "verification-agent",
  "verificationNotes": "COMPLETE (2025-11-13): All 5 formulas extracted and verified at 300 DPI. (1) Algorithm 4.1.5 page 262 - cubic regularization subproblem. (2) Theorem 4.1.2 page 267 - limit point characterization with stationary conditions. (3) Definition 4.1.3 page 274 - gradient-dominated function of degree p. (4) Example 4.1.2 page 275 - strongly convex inequality (equation 4.1.32). (5) Theorem 4.1.6 page 276 - convergence bound for gradient-dominated functions (CRITICAL FIX from Batch 5: corrected missing numerator factor). All formulas follow 3-checkpoint verification workflow with no cutoffs.",
  "usedIn": [
    "NewtonTab"
  ],
  "pdfPages": "262-277",
  "formulaImages": [
    {
      "formula_id": "lectures_on_convex_optimization_p262_algorithm_4_1_5",
      "metadata_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p262_algorithm_4_1_5.json",
      "image_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p262_algorithm_4_1_5.png",
      "latex": "\\min_y \\left[ \\langle \\nabla f(x), y - x \\rangle + \\frac{1}{2}\\langle \\nabla^2 f(x)(y - x), y - x \\rangle + \\frac{M}{6}\\|y - x\\|^3 \\right]",
      "verified": true,
      "theorem": "Algorithm (4.1.5)",
      "equation": "(4.1.5)",
      "page": 262
    },
    {
      "formula_id": "lectures_on_convex_optimization_p267_theorem_4_1_2",
      "metadata_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p267_theorem_4_1_2.json",
      "image_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p267_theorem_4_1_2.png",
      "latex": "\\lim_{i \\to \\infty} f(x_i) = f^*, \\quad f(x^*) = f^*, \\quad \\nabla f(x^*) = 0, \\quad \\nabla^2 f(x^*) \\succeq 0",
      "verified": true,
      "theorem": "Theorem 4.1.2",
      "equation": "Limit point characterization",
      "page": 267
    },
    {
      "formula_id": "lectures_on_convex_optimization_p274_definition_4_1_3",
      "metadata_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p274_definition_4_1_3.json",
      "image_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p274_definition_4_1_3.png",
      "latex": "f(x) - f(x^*) \\leq \\tau_f \\|\\nabla f(x)\\|^p",
      "verified": true,
      "theorem": "Definition 4.1.3",
      "equation": "(4.1.31)",
      "page": 274
    },
    {
      "formula_id": "lectures_on_convex_optimization_p275_example_4_1_2",
      "metadata_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p275_example_4_1_2.json",
      "image_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p275_example_4_1_2.png",
      "latex": "f(y) \\geq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{1}{2}\\mu \\|y - x\\|^2",
      "verified": true,
      "theorem": "Example 4.1.2",
      "equation": "(4.1.32)",
      "page": 275
    },
    {
      "formula_id": "lectures_on_convex_optimization_p276_theorem_4_1_6",
      "metadata_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p276_theorem_4_1_6.json",
      "image_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p276_theorem_4_1_6.png",
      "latex": "f(x_k) - f(x^*) \\leq \\hat{\\omega} \\cdot \\frac{\\gamma^2\\left(2+\\frac{3}{2}\\gamma\\right)^2}{\\left(2+\\left(k+\\frac{3}{2}\\right)\\cdot\\gamma\\right)^2}",
      "verified": true,
      "theorem": "Theorem 4.1.6",
      "equation": "(4.1.36)",
      "page": 276
    }
  ]
}