{
  "reference": "nesterov-2018",
  "pages": "72-73, 83-95",
  "theorem": "Theorem 2.2.2, Theorem 2.1.7 (lower bound), Constant Step Scheme II (2.2.20) for μ=0, and Constant Step Scheme III (2.2.22) for μ>0",
  "claim": "Nesterov's accelerated gradient method achieves the optimal convergence rate of order one over k-squared for smooth convex functions, which is provably optimal among all first-order methods",
  "quote": "Let us take in (2.2.7) $\\gamma_0 = 3L + \\mu$. Then this scheme generates a sequence $\\{x_k\\}_{k=0}^{\\infty}$ such that $f(x_k) - f^* \\leq \\frac{2(4+q_f)L\\|x_0-x^*\\|^2}{3(k+1)^2}$. This means that method (2.2.7) is optimal for solving the unconstrained minimization problem (2.2.1) with $f \\in \\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ and $\\mu \\geq 0$. If $\\mu = 0$, then this method is optimal.",
  "notes": "This is the 2018 edition version with refined calligraphic notation. The general scheme (2.2.7) is developed in pages 83-92, with constant step variants on pages 93-95: Scheme II (equation 2.2.20) for smooth convex (μ=0), and Scheme III (equation 2.2.22) for strongly convex (μ>0). Optimality is proven in Theorem 2.2.2 by comparing with Theorem 2.1.7 lower bound (pages 72-73). For μ=0 (smooth convex), equation (2.2.18) on page 93 gives the explicit rate: $f(x_k) - f^* \\leq \\frac{8L\\|x_0-x^*\\|^2}{3(k+1)^2}$, which is O(1/k²) compared to gradient descent's O(1/k) rate.",
  "readerNotes": "The notation $\\mathscr{S}_{\\mu,L}^{1,1}(\\mathbb{R}^n)$ denotes functions with strong convexity parameter $\\mu \\geq 0$ and Lipschitz continuous gradient with constant $L$. When $\\mu = 0$, this reduces to $\\mathscr{F}_L^{1,1}(\\mathbb{R}^n)$, the class of smooth convex functions. The $O(1/k^2)$ convergence rate is provably optimal: Theorem 2.1.7 (pages 72-73) establishes a lower bound showing that no first-order method can achieve better than $O(1/k^2)$ convergence for this function class. Nesterov's accelerated method matches this lower bound up to constant factors, proving optimality. IMPORTANT: There are two different constant step schemes: Scheme II (page 94, equation 2.2.20) for smooth convex ($\\mu=0$) uses momentum coefficient $\\beta_k = \\frac{\\alpha_k(1-\\alpha_k)}{\\alpha_k^2+\\alpha_{k+1}}$ with $\\alpha_k$ from a recurrence relation. Scheme III (page 95, equation 2.2.22) for strongly convex ($\\mu>0$) uses $\\beta = \\frac{1-\\sqrt{q_f}}{1+\\sqrt{q_f}}$ where $q_f = \\mu/L$. The form $x_{k+1} = y_k - \\frac{1}{L}\\nabla f(y_k)$, $y_{k+1} = x_{k+1} + \\beta(x_{k+1} - x_k)$ is the same for both, but the momentum coefficient differs.",
  "proofPages": [
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0091.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0092.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0102.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0103.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0104.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0105.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0106.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0107.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0108.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0109.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0110.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0111.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0112.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0113.png",
    "docs/references/extracted-pages/lectures_on_convex_optimization_page_0114.png"
  ],
  "verified": "2025-11-12",
  "verifiedBy": "verification-agent",
  "verificationNotes": "ENHANCED VERIFICATION (adversarial testing): Independently verified all 15 proof pages. Quote is WORD-FOR-WORD accurate from Theorem 2.2.2 (page 91, book page). The O(1/k²) rate is PROVEN (not just stated): equation (2.2.18) on page 93 (book page) gives explicit bound. Optimality is PROVEN: Theorem 2.1.7 (pages 72-73, book pages) establishes information-theoretic lower bound, and Theorem 2.2.2 proves the method matches this bound. All 15 pages are NECESSARY: pages 72-73 (book pages) prove lower bound (required for optimality claim), pages 83-90 (book pages) develop estimating sequence framework (mathematical foundation), pages 91-92 (book pages) prove Theorem 2.2.2 (main optimality result), pages 93-95 (book pages) derive concrete algorithmic schemes (Schemes II and III). CORRECTED: Updated theorem field to include both Scheme II (μ=0) and Scheme III (μ>0), as the original only mentioned Scheme III which is incomplete. Updated pages field to use book page numbers (72-73, 83-95) instead of PDF page numbers. REMOVED 'GdFixedTab' from usedIn as grep verification showed the citation is not actually referenced in that file (this was an error in the original citation). CORRECTED (2025-11-13): Fixed page number inconsistency - pages field now correctly shows book pages 72-73, 83-95 (corresponding to PDF pages 91-92, 102-114 with pageOffset=19).",
  "usedIn": [],
  "formulaImages": [
    {
      "formula_id": "lectures_on_convex_optimization_p110_theorem_2_2_2",
      "metadata_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p110_theorem_2_2_2.json",
      "image_path": "docs/references/extracted-pages/formulas/lectures_on_convex_optimization_p110_theorem_2_2_2.png",
      "latex": "f(x_k) - f^* \\leq \\frac{2(4+q_f)L\\|x_0-x^*\\|^2}{3(k+1)^2}",
      "verified": true,
      "theorem": "Theorem 2.2.2",
      "equation": "(2.2.13)"
    }
  ],
  "pdfPages": "91-92, 102-114"
}