================================================================================
                    CODEBASE SURVEY - EXECUTIVE SUMMARY
================================================================================

PROJECT: Pedagogical Optimization Algorithm Visualizer
FOCUS: GD (Gradient Descent), GD+LS (Gradient Descent + Line Search), 
       and LR (Logistic Regression) test problem

CREATED: Survey generated Nov 7, 2025

================================================================================
                              WHAT YOU HAVE
================================================================================

1. FOUR OPTIMIZATION ALGORITHMS
   - Gradient Descent (fixed step size)
   - Gradient Descent with Armijo Line Search (adaptive step)
   - Newton's Method (second-order)
   - L-BFGS (memory-efficient quasi-Newton)
   - [Bonus] Diagonal Preconditioner (Hessian-diagonal based)

2. TWO FULLY PARALLEL IMPLEMENTATIONS
   - TypeScript/JavaScript (React visualization)
   - Python (scipy validation reference)

3. COMPLETE TEST PROBLEM SET
   - 6 pure optimization problems (2D, analytical)
   - 1 logistic regression problem (3D, dataset-based)
   - 3 SVM variants (soft-margin, perceptron, squared-hinge)
   
4. COMPREHENSIVE VALIDATION SUITE
   - 40+ test combinations (algorithms × problems)
   - Python/TypeScript comparison with tolerance checking
   - Iteration history tracking
   - Convergence criterion verification

5. INTERACTIVE VISUALIZATION
   - React/Canvas contour plots
   - Real-time algorithm trajectories
   - Line search trial visualization (for GD+LS)
   - Hessian eigenvalue display (for Newton)
   - L-BFGS memory pair visualization
   - Basin of attraction coloring

================================================================================
                         KEY FILES BY CATEGORY
================================================================================

GD IMPLEMENTATION
  TS: /src/algorithms/gradient-descent.ts
  PY: /python/scipy_runner.py (custom)
  Main function: runGradientDescent(problem, options)

GD+LS IMPLEMENTATION
  TS: /src/algorithms/gradient-descent-linesearch.ts
  PY: /python/scipy_runner.py (maps to scipy.optimize.CG)
  Main function: runGradientDescentLineSearch(problem, options)

LINE SEARCH (ARMIJO)
  TS: /src/line-search/armijo.ts
  Algorithm: Backtracking with c1=0.0001, rho=0.5, maxTrials=20

LOGISTIC REGRESSION PROBLEM
  TS: /src/utils/logisticRegression.ts
  PY: /python/data_problems.py (LogisticRegression class)
  Dataset: /python/datasets/crescent.json (140 points, 2D crescent)

VALIDATION SUITE
  Main: /python/validate_with_python.py
  Comparator: /python/comparator.py
  TS runner: /python/ts_runner.py
  Scipy wrapper: /python/scipy_runner.py

PROBLEM DEFINITIONS
  Pure 2D: /src/problems/ (6 problems + registry)
  Python equivalents: /python/problems.py

INTEGRATION
  Adapter: /src/utils/problemAdapter.ts
  Types: /src/algorithms/types.ts (ProblemFunctions interface)
  UI: /src/UnifiedVisualizer.tsx (main component)

================================================================================
                      LOGISTIC REGRESSION DEFINITION
================================================================================

WHAT IS LR?
  Binary classification problem using logistic regression
  Model: P(y=1|x) = sigmoid(w0*x1 + w1*x2 + w2)
  3D weights: [w0, w1, w2] where w2 is bias term

LOSS FUNCTION
  L = -mean[y*log(p) + (1-y)*log(1-p)] + (λ/2)*(w0² + w1²)
  
  Gradient: error = sigmoid(z) - y
  ∂L/∂w0 = mean[error * x1] + λ*w0
  ∂L/∂w1 = mean[error * x2] + λ*w1
  ∂L/∂w2 = mean[error]  (no regularization on bias)

HESSIAN
  H = (1/n) * X^T * D * X + λ*I_partial
  where D = diag(σ(z)*(1-σ(z))) for each data point
  
NUMERICAL STABILITY
  Sigmoid clipping: z ∈ [-500, 500]
  Loss clipping: p ∈ [1e-15, 1-1e-15]

DATASET
  140 points (70 per class)
  Interleaved crescent moons
  Features: 2D (x1, x2)
  Labels: binary (0, 1)

================================================================================
                       GD vs GD+LS COMPARISON
================================================================================

METRIC                    GD (Fixed)         GD+LS (Line Search)
Step size selection       Manual (α)         Automatic (Armijo)
Tuning difficulty         High               Low
Iterations on LR          100-200            20-50
Implementation            Simple             Complex
Visualization             Trajectory only    + Line search trials
Default params            α ∈ [0.01-0.1]     c1=0.0001, rho=0.5

PERFORMANCE ON LR
  GD with α=0.05:    ~100 iter ✓
  GD with α=0.10:    ~30-50 iter ✓✓
  GD with α=0.15:    ~25 iter ✓✓
  GD with α=0.30:    Diverges ✗
  
  GD+LS:             ~20-30 iter ✓✓ (always works)

KEY INSIGHT
  GD+LS adapts step size automatically, eliminating manual tuning.
  GD is simpler but requires careful α selection.
  On convex problems like LR, both should converge from any start.

================================================================================
                          HOW TO RUN THINGS
================================================================================

VIEW THE INTERACTIVE APP
  1. npm install
  2. npm run dev
  3. Open http://localhost:5173
  4. Select GD or GD+LS from algorithm tabs
  5. Adjust step size (α) or line search parameters
  6. Watch real-time visualization

RUN VALIDATION SUITE
  cd python
  python validate_with_python.py --verbose
  
  Expected output: Pass/Fail/Suspicious status for 40+ test combinations

RUN SINGLE EXPERIMENT
  python validate_with_python.py --problem logistic-regression --verbose
  
  Shows Python (scipy) vs TypeScript results side-by-side

RUN CUSTOM TESTS
  npx ts-node test-gradient-consistency.ts
  npx ts-node test-step-size-progression.ts

================================================================================
                        RECENT CHANGES AFFECTING GD/LS
================================================================================

Nov 7, 2025:
  - Added diagonal preconditioner (not GD/LS specific)
  - Linting fixes in algorithm code
  - UI enhancements for preconditioner controls

Oct 2025 (key historical changes):
  - Aligned defaults and convergence behavior with scipy
  - Added AlgorithmResult interface for uniform convergence reporting
  - Made Hessian damping configurable for Newton
  - Made convergence tolerance configurable

NO BREAKING CHANGES
  - GD implementation stable since Q3 2025
  - GD+LS Armijo implementation stable since Q3 2025
  - LR problem definition stable since initial implementation
  - Both TS and Python implementations match within 1-10% tolerance

================================================================================
                      INTEGRATION ARCHITECTURE
================================================================================

UNIVERSAL PROBLEM INTERFACE (ProblemFunctions)
  All algorithms work with this interface:
  {
    objective: (w) => number,
    gradient: (w) => number[],
    hessian?: (w) => number[][],
    dimensionality: number
  }

ALGORITHM SIGNATURE (all algorithms follow this)
  runAlgorithm(problem: ProblemFunctions, options) → AlgorithmResult<IterationType>
  
  Returns:
  {
    iterations: IterationType[],      // Full iteration history
    summary: {
      converged: boolean,
      finalLocation: number[],
      finalLoss: number,
      finalGradNorm: number,
      terminationMessage: string
    }
  }

ADAPTER PATTERN
  problemToProblemFunctions() - Pure problems
  logisticRegressionToProblemFunctions() - LR + data
  separatingHyperplaneToProblemFunctions() - SVM variants
  
This pattern allows algorithms to be problem-agnostic.

================================================================================
                      WHAT CHANGED (Current Status)
================================================================================

MODIFIED (UNCOMMITTED)
  - src/components/AlgorithmConfiguration.tsx (UI state)
  - src/components/BasinPicker.tsx (Basin visualization UI)
  
NOT AFFECTED
  - GD implementation (stable)
  - GD+LS implementation (stable)
  - LR problem definition (stable)
  - Validation suite (stable)

MAIN BRANCH
  Ready for use, no critical uncommitted changes

================================================================================
                       DOCUMENTATION PROVIDED
================================================================================

This survey includes:

1. CODEBASE_SURVEY_GD_GDLS_LR.md (720 lines)
   - Complete technical reference
   - All algorithm details with code samples
   - Python/TS comparison
   - Integration patterns
   - Known issues and workarounds

2. GD_GDLS_QUICK_REFERENCE.txt (this file structure)
   - Quick lookup guide
   - File locations
   - Key parameters
   - Performance characteristics
   - Troubleshooting tips

3. SURVEY_SUMMARY.txt (this file)
   - Executive overview
   - Key files by category
   - Quick start instructions
   - Architecture summary

================================================================================
                         TESTING & VALIDATION
================================================================================

Python Validation Suite: 40+ test cases
  • 6 pure problems × 4 algorithms = 24 cases
  • Logistic regression × 4 algorithms = 4 cases
  • 3 SVM variants × 4 algorithms = 12 cases

Comparison Criteria:
  • Convergence match: Both must agree on converged/diverged
  • Loss match: Within 10% → PASS, 1-10% → SUSPICIOUS, >10% → FAIL
  • Position match: Converged positions within 1.0 → PASS

Expected Results:
  GD on LR:       PASS (within scipy CG reference)
  GD+LS on LR:    PASS (matches scipy CG)
  Newton on LR:   PASS (matches scipy Newton-CG)
  L-BFGS on LR:   PASS (matches scipy L-BFGS-B)

================================================================================
                           KEY TAKEAWAYS
================================================================================

1. GD and GD+LS are fully implemented and stable on LR problem

2. GD requires step size tuning; GD+LS adapts automatically

3. LR problem is mathematically identical in TS and Python

4. Validation suite shows <10% difference across implementations

5. Line search visualization shows Armijo condition checking per trial

6. Both algorithms converge on logistic regression in reasonable iterations
   - GD: 30-50 iterations (with good α choice)
   - GD+LS: 20-30 iterations (no tuning needed)

7. Complete problem-algorithm integration via adapter pattern

8. 40+ test cases validate correctness vs scipy reference

9. Interactive visualizer allows pedagogical exploration

10. Code is production-ready for optimization algorithm learning

================================================================================
                           NEXT STEPS
================================================================================

TO UNDERSTAND GD/GD+LS BETTER
  1. Read section 1-2 of CODEBASE_SURVEY_GD_GDLS_LR.md
  2. Look at /src/algorithms/gradient-descent.ts (GD)
  3. Look at /src/algorithms/gradient-descent-linesearch.ts (GD+LS)
  4. Run validation suite to see performance
  5. Use interactive visualizer to watch algorithms converge

TO ANALYZE PERFORMANCE
  1. Run python/validate_with_python.py --verbose
  2. Compare iteration counts and losses
  3. Check convergence criteria messages
  4. Try different problems (quadratic, ill-conditioned, etc.)

TO EXTEND THE CODEBASE
  1. Read section 11 (Integration Architecture)
  2. Follow ProblemFunctions interface
  3. Add to adapter in problemAdapter.ts
  4. Update UI in ProblemConfiguration.tsx
  5. Add test cases to validate_with_python.py

================================================================================
